# 循环神经网络

## RNN模型

### 1.RNN作用

​	传统神经网络无法处理对时序数据高度敏感的任务，如股票、天气数据、经典古诗等，如果顺序不同，内容也会大大改变。

​	因此，在处理具有时间或逻辑顺序的数据时，我们必须考虑到顺序的重要性，并设计相应的模型来充分利用数据的前后关系，以获得更准确和有意义的结果。**循环神经网络（RNN）等模型就是由此产生**，因为它们能够捕捉到数据的顺序信息，并根据前面的输入来预测后续的输出。

### 2.RNN原理

#### 2.1.RNN概述

- 循环神经网络（Recurrent Neural Network，RNN）是一种神经网络结构，专门用于处理序列数据。与传统的前馈神经网络不同，RNN 在内部具有反馈连接，允许信息在网络内部传递。这种结构使得 RNN 能够对序列数据的历史信息进行建模，并在一定程度上具有记忆能力。
- RNN 被广泛应用于语言建模、机器翻译、情感分析等任务。通过捕捉单词之间的上下文信息，RNN 能够更好地理解语言的含义和结构。
- 传统的 RNN 存在一些问题，例如难以处理长期依赖关系、梯度消失或梯度爆炸等。为了解决这些问题，出现了一些改进的 RNN 变种，如长短期记忆网络（LSTM）和门控循环单元（GRU）。

#### 2.2.RNN模型架构

​		常见的RNN架构如下图两种：

<img src="https://github.com/ljgit1316/Picture_resource/blob/main/RNN_Pic\\image-20240320141450146.png" alt="image-20240320141450146" style="zoom:50%;" />

​		**在RNN的经典架构中，网络通过一个特殊的循环结构将信息从一个处理步骤传递到下一个。这个循环结构通常被称为“隐藏层状态”或简单地称为“隐藏状态”。隐藏状态是RNN的记忆部分，它能够捕获并存储关于已处理序列元素的信息。**

- 原理流程

  <img src="https://github.com/ljgit1316/Picture_resource/blob/main/RNN_Pic\\image-20240320142904019.png" alt="image-20240320142904019" style="zoom: 67%;" />

- 参数解析

  假设为一个包含三个单词的句子，将模型展开，即为一个三层的网络结构，可以理解为，${x}_{t-1}$为第一个词，${x}_{t}$为第二个词，${x}_{t+1}$为第三个词

  - ${x}_{t}$表示第t步的输入。比如${x}_{1}$为第二个词的词向量(${x}_{0}$为第一个词)；

  - ${H}_{t}$为隐藏层的第t步的状态，它是网络的记忆单元。

    - ${H}_{t}$根据当前输入层的输出与上一时刻隐藏层的状态${H}_{t-1}$进行计算，如下所示。
      $$
      H_t=f(\mathbf{U}·x_t+\mathbf{W}·H_{t-1})
      $$

    - 其中，**U**是输入层的连接矩阵，**W**是上一时刻隐含层到下一时刻隐含层的权重矩阵，f()一般是非线性的激活函数，如tanh或ReLU。

  - ${O}_{t}$是第t步的输出。输出层是全连接层，即它的每个节点和隐含层的每个节点都互相连接，**V**是输出层的连接矩阵，g(·)一是激活函数。

    - $$
      o_t=g(\mathbf{V}·s_t)
      $$

    - 带入可以得到
      $$
      \begin{align}
      H_t&=f(\mathbf{W}_{in}X_t+\mathbf{W}_{s}H_{t-1}+b_t)\\
      &=f(\mathbf{W}_{in}X_t+\mathbf{W}_{s}f(\mathbf{W}_{in}X_{t-1}+\mathbf{W}_{s}H_{t-2}+b_{t-1})+b_{t})\\
      &=f(\mathbf{W}_{in}X_t+\mathbf{W}_{s}f(\mathbf{W}_{in}X_{t-1}+\mathbf{W}_{s}f(\mathbf{W}_{in}X_{t-2}+\mathbf{W}_{s}H_{t-3}+b_{t-2})+b_{t-1})+b_t)\\
      &=f(\mathbf{W}_{in}X_t+\mathbf{W}_{s}f(\mathbf{W}_{in}X_{t-1}+\mathbf{W}_{s}f(\mathbf{W}_{in}X_{t-2}+\mathbf{W}_{s}(...))+b_{t-2})+b_{t-1})+b_t)
      \end{align}
      $$

#### 2.3.RNN内部结构

- 内部模型

  <img src="https://github.com/ljgit1316/Picture_resource/blob/main/RNN_Pic\\图片11.png" style="zoom: 67%;" />

- 激活函数

  <img src="https://github.com/ljgit1316/Picture_resource/blob/main/RNN_Pic\\图片22.png" style="zoom:80%;" />

#### 2.4.RNN模型输入输出关系

- 图示

  <img src="https://github.com/ljgit1316/Picture_resource/blob/main/RNN_Pic\\图片20240610184427.png" style="zoom: 50%;" />

- 一对多

  这种结构的RNN接受单个输入并产生一系列输出。**这种模式常用于“看图说话”的任务**，即给定一张图片（单个输入），RNN生成一段描述该图片的文本（一系列输出）。在这种情况下，RNN的结构被调整为首先对输入图片进行编码，然后根据这个编码连续生成文本序列中的词语。

- 多对一

  与一对多相反，多对一的RNN结构接受一系列输入并产生单个输出。**这种结构适用于如文本分类和情感分析等任务**，其中模型需要阅读和理解整个文本（一系列输入），然后决定文本属于哪个类别（单个输出）。在图片生成的上下文中，这种结构可以通过分析一系列的特征或指令来生成单个图片输出。

- 多对多

  这种结构的RNN既接受一系列输入，也产生一系列输出。**这在需要输入和输出均为序列的任务中非常有用，例如机器翻译**，其中模型需要读取一个语言的文本（一系列输入），然后生成另一种语言的对应文本（一系列输出）。另一个例子是小说生成，其中RNN可以基于给定的开头或主题（一系列输入），连续生成故事的后续内容.

### 3.RNN实现

#### 3.1.RNN参数解析

1. **Batch Size (批量大小)**:

   - Batch size指的是在一次前向传播或反向传播过程中同时处理的样本数量。
   - 例如，在文本处理中，如果一批数据包含100个句子，那么batch size就是100。

2. **Sequence Length (序列长度)**:

   - Sequence length是指输入数据中每个样本的连续时间步（或词、字符）的数量。
   - 例如，在一个句子级别的任务中，一个句子可能包含10个单词，那么序列长度就是10。

3. **Input Size (输入大小)**:

   - Input size是指每个时间步输入向量的特征维度。
   - 在处理文本时，如果每个词都被表示为一个固定维度的向量，那么input size就是这个词向量的维度。
   - 如在情感分析任务中，每个词可能被嵌入为一个100维的向量，那么input size就是100。

4. **Hidden Size (隐藏层大小)**:

   - Hidden size是指RNN单元内部隐藏状态（Hidden State）的维度。
   - 在每个时间步，RNN都会根据当前输入和上一时间步的隐藏状态来计算新的隐藏状态，新隐藏状态的维度就是hidden size。
   - 例如，如果我们设置hidden size为256，那么每个时间步产生的隐藏状态就是一个256维的向量。
   - 根据实验和模型复杂度的要求自由选择隐藏层大小，它并不是通过特定计算得出的数值。
   - 隐藏层大小的选择会影响到模型的学习能力和表示能力，同时也影响到模型的计算资源消耗。
   - 实践中，较小的隐藏层大小可能会限制模型的表达能力，而过大的隐藏层大小则可能导致过拟合、训练时间增加等问题。
   - 在决定隐藏层大小时，通常需要结合具体任务的特点、数据集规模、计算资源等因素进行合理选择，并通过交叉验证、网格搜索等方式进行超参数调优，以找到最优的隐藏层大小以及其他超参数组合。

5. **Output Size (输出大小)**:

   - Output size通常与特定任务相关。

   - 对于一般的RNN，每个时间步的输出大小与hidden size相同，即输出也是一个隐藏状态维度的向量。

   - 在分类任务中，最后一层可能通过一个全连接层映射到类别数目，这时最后一个时间步的输出大小可能是类别数目的维度。

   - 如果是多层或双向RNN，输出也可能经过额外的处理（如拼接、池化等），最终的输出大小会根据具体应用需求来确定。

   - 在最简单的单向单层循环神经网络（RNN）中，输出大小（output size）的计算通常比较直接：

     - 如果目的是为了获取每个时间步（time step）的隐藏状态表示，并且不进行额外的转换操作，那么每个时间步的输出大小（output size）就等于您设定的隐藏层大小（hidden size）。

     例如，如果设置的隐藏层大小（hidden size）是256，那么在每个时间步，RNN的输出也将是一个256维的向量。

     - 如果在RNN之后添加了其他层（如全连接层或分类层）来进行进一步的处理，比如进行分类任务，那么输出大小取决于这些后续层的设计。例如，如果您接下来是一个Softmax层用于做多分类，且类别数是10，则输出大小将会是10，表示每个样本的概率分布。

     - 如果是在做序列到序列（Sequence-to-Sequence）的任务，比如机器翻译，最后的时间步的隐藏状态通常会通过一个线性层映射到目标词汇表大小，这样输出大小就会是目标词汇表的大小。

具体的单层单向RNN示例来说明维度变换过程：

假设正在处理一个文本分类任务，每个单词已经被嵌入为一个100维的向量，我们的序列长度（sequence length）是50（即最长句子有50个单词），批量大小（batch size）是32（一次处理32个句子），我们设定的隐藏层大小（hidden size）是128。

1. 输入维度（input size）: 每个时间步（每个单词）的输入向量维度是100，所以整个输入张量的维度是 `(batch size, sequence length, input size)`，即 `(32, 50, 100)`。

2. 隐藏层计算: RNN会对每个时间步的输入进行处理，并基于上一时间步的隐藏状态生成当前时间步的隐藏状态。隐藏状态的维度由我们设定，这里是128维，所以每个时间步的隐藏状态和输出的维度都是 `(batch size, hidden size)`，即 `(32, 128)`。

3. 输出维度（output size）: 因为这里我们假设没有在RNN后添加额外的层（例如分类层），所以每个时间步的输出大小就等于隐藏层大小，也就是128维。但是，由于输出是针对每一个时间步的，所以整个输出序列的维度为 `(batch size, sequence length, hidden size)`，即 `(32, 50, 128)`。

​		如果后续需要进行分类，比如这是一个二分类问题，我们会把最后一个时间步的隐藏状态（`128`维）通过一个全连接层（Dense Layer）映射到类别数目的维度，如2维，此时输出大小将变为 `(32, 2)`，表示32个样本的二维概率分布。

#### 3.2.RNN底层原理实现

```python
import numpy as np

# 假设输入的数据有三个时间步，每个时间步有两个特征
x = np.random.rand(3, 2)
print(x)

# 定义RNN的参数
input_size = 2
hidden_size = 3
output_size = 4

# 初始化权重和偏置
# 输入到隐藏层
w_xh = np.random.rand(input_size, hidden_size)
# 隐藏层到隐藏层
w_hh = np.random.rand(hidden_size, hidden_size)
# 隐藏层到输出层
w_hy = np.random.rand(hidden_size, output_size)
# 隐藏层的偏置
b_h = np.zeros((hidden_size,))
# 输出层的偏置
b_y = np.zeros((output_size,))


# 激活函数
def tanh(x):
    return np.tanh(x)


# 初始化隐藏层状态，也就是第一层隐藏层所接受的隐藏层输入（为0）
h_prev = np.zeros((hidden_size,))

# 前向传播
# 时间步1
x1 = x[0, :]  # 输入数据
h1 = tanh(np.dot(x1, w_xh) + np.dot(h_prev, w_hh) + b_h)  # 隐藏层输入
o1 = np.dot(h1, w_hy) + b_y  # 输出层输入
# 时间步2
x2 = x[1, :]
h2 = tanh(np.dot(x2, w_xh) + np.dot(h1, w_hh) + b_h)
o2 = np.dot(h2, w_hy) + b_y
# 时间步3
x3 = x[2, :]
h3 = tanh(np.dot(x3, w_xh) + np.dot(h2, w_hh) + b_h)
o3 = np.dot(h3, w_hy) + b_y

#测试输出
print('时间步1的隐藏状态h1', h1)
print('时间步1的输出o1', o1)
print('时间步2的隐藏状态h2', h2)
print('时间步2的输出o2', o2)
print('时间步3的隐藏状态h3', h3)
print('时间步3的输出o3', o3)
```

#### 3.3.RNN_Cell实现

`nn.RNNCell` 本质上只返回隐藏状态，它没有单独的输出结果。一般在 `RNN` 中，隐藏状态既可以被视为输出，也可以通过一个线性层将隐藏状态转化为实际的输出。

```python
import torch
import torch.nn as nn

# 创建数据
# 创建一个形状为(2,3,4)的输入张量。2为批次，3为次数(时间步)，4为每个词向量大小
x = torch.randn(2, 3, 4)
print(x)

# 定义一个RNN类
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, batch_first):
        super(RNN, self).__init__()
        self.rnn_cell = nn.RNNCell(input_size, hidden_size)
        self.hidden_size = hidden_size
        self.batch_first = batch_first

    def _initialize_hidden(self, batch_size):
        # 初始化隐藏状态
        return torch.zeros(batch_size, self.hidden_size)

    def forward(self, x, init_hidden=None):
        # 如果batch_first=True,输入x的形状为(batch_size, seq_len, input_size)
        if self.batch_first:
            # 获取输入数据的尺寸，size()会返回一个元组
            batch_size, seq_len, input_size = x.size()
            # rnn中需要的数据维度是(seq_len, batch_size, input_size)
            x = x.permute(1, 0, 2)
        else:
            seq_len, batch_size, input_size = x.size()

        hidden_list = []  # 用于存储每一个时间步的隐藏状态
        # 提供初始化全为0的隐藏状态
        if init_hidden is None:
            init_hidden = self._initialize_hidden(batch_size)
            init_hidden = init_hidden.to(x.device)  # 将初始化隐藏状态移动到与输入张量相同的设备上
        hidden_t = init_hidden

        # 遍历每个时间步
        for t in range(seq_len):
            hidden_t = self.rnn_cell(x[t], hidden_t)
            hidden_list.append(hidden_t)
        print(hidden_list)
        # 将所有时间步的隐藏状态拼接成一个张量
        hidden_list = torch.stack(hidden_list)
        print(hidden_list)
        # 如果batch_first=True，则将输出的维度重新调整回(batch_size, seq_len, hidden_size)
        if self.batch_first:
            hidden_list = hidden_list.permute(1, 0, 2)

        print(hidden_list)
        return hidden_list


model=RNN(4,8,True)
output=model(x)
print(x)
```

#### 3.4.API实现RNN

##### 3.4.1单向、单层RNN

- 举例说明

  1. 定义一个单层循环神经网络（RNN）实例：

     ```python
     signle_rnn = nn.RNN(4, 3, 1, batch_first=True)
     ```

     这行代码创建了一个RNN层，其参数含义如下：

     - `4` 表示输入序列的特征维度（feature size），即每个时间步的输入向量长度为4。
     - `3` 表示隐藏状态（hidden state）的维度，即RNN单元内部记忆的向量长度为3。
     - `1` 表示RNN层的数量，这里仅为单层。
     - `batch_first=True` 指定输入张量的第一个维度代表批次(batch)，第二个维度代表时间步(sequence length)，这对于处理批次数据时更容易理解。

  2. 创建输入数据张量：

     ```python
     input = torch.randn(1, 2, 4)
     ```

     这行代码生成了一个随机张量作为RNN的输入，它的形状为 `(batch_size, sequence_length, feature_size)`，具体到这里的值是：

     - `1` 表示批大小（batch size），即本次输入的数据样本数量。
     - `2` 表示序列长度（sequence length），即每个样本的输入序列包含两个时间步。
     - `4` 是每个时间步输入向量的特征维度，与RNN层设置一致。

  3. 对输入数据进行前向传播：

     ```python
     output, h_n = signle_rnn(input)
     ```

     这行代码将之前创建的随机输入数据送入RNN层进行前向计算。执行后得到两个输出：

     - `output` 是经过RNN处理后的输出序列，其形状通常为 `(batch_size, sequence_length, num_directions * hidden_size)`。在这个例子中，因为没有指定双向RNN，所以 `num_directions=1`。因此，`output` 的尺寸将是 `(1, 2, 3)`，对应每个批次中的每个时间步输出一个维度为3的向量。
     - `h_n` 是最后一个时间步的隐藏状态（hidden state），它通常是最终时间步的隐藏状态或者是所有时间步隐藏状态的某种聚合（取决于RNN类型）。在这里，`h_n` 的形状是 `(num_layers * num_directions, batch_size, hidden_size)`，但由于只有一层并且是无方向的RNN，所以形状会简化为 `(1, 1, 3)`，即单一隐藏状态向量。这个隐藏状态可以用于下个时间步的预测或者作为整个序列的编码。

- 实例代码

  ```python
  import torch
  import torch.nn as nn
  
  # 设置超参数
  batch_size, seq_len, input_size, hidden_size = 2, 3, 4, 8
  
  # 生成数据
  input = torch.randn(batch_size, seq_len, input_size)
  
  # 初始化隐藏状态
  h_prev = torch.zeros(batch_size, hidden_size)
  
  # 创建RNN模型
  # input_size: 输入向量维度, hidden_size: 隐藏层维度,batch_first: 是否使用batch_size作为第一维(batch_size,seq_len,input_size)
  rnn = nn.RNN(input_size, hidden_size, batch_first=True)
  # output：每个时间步输出的隐藏状态，state_final：最后一个时间步输出的隐藏状态
  output, state_final = rnn(x, h_prev.unsqueeze(0))
  print(output.shape)
  print(state_final.shape)
  
  """
  torch.Size([2, 3, 8])
  torch.Size([1, 2, 8])
  
  """
  ```

  - `input` 是一个形状为 `(batch_size, sequence_length, input_size)` 的张量，表示一批包含 `T` 个时间步长的序列，每个时间步长的输入特征维度为 `input_size`。
  - `h_prev` 是所有序列共享的初始隐含状态，形状为 `(batch_size, hidden_size)`。
  - `h_prev.unsqueeze(0)` 将 `h_prev` 的批量维度增加一层，因为PyTorch RNN期望隐含状态作为一个元组 `(num_layers, batch_size, hidden_size)`，在这里我们只有一个隐藏层，所以增加了一维使得形状变为 `(1, batch_size, hidden_size)`。
  - `rnn(input, h_prev.unsqueeze(0))` 执行RNN的前向传播，得到的 `rnn_output` 是整个序列的输出结果，形状为 `(batch_size, sequence_length, hidden_size)`，而 `state_final` 是最后一个时间步的隐含状态，形状为 `(num_layers, batch_size, hidden_size)`。
  - 两个返回值 `rnn_output` 和 `state_final` 代表着循环神经网络在当前时间步的输出和最终的隐藏状态。
    - `rnn_output`：代表当前时间步的 RNN 输出。对于很多序列模型而言，每个时间步都会有一个输出。这个输出可能会被用于下一时间步的计算，或者作为模型的最终输出。
    - `state_final`：代表 RNN 模型在最后一个时间步的隐藏状态。这个隐藏状态通常被认为是对整个序列的编码或总结，它可能会被用于某些任务的最终预测或输出。

##### 3.4.2双向、单层RNN

- 定义

  双向单层RNN（Recurrent Neural Network）是一种特殊类型的循环神经网络，它能够在两个方向上处理序列数据，即正向和反向。

  双向单层RNN由两个独立的单层RNN组成，一个负责处理正向序列（从开始到结束），另一个负责处理反向序列（从结束到开始）。

- 特点

  1. **双向处理：** 最显著的特点是双向结构，使得模型能够同时学习到序列中某一点前后的上下文信息，这对于很多序列任务来说是非常有价值的，比如自然语言处理中的文本理解、语音识别等。

  2. **单层结构：** “单层”指的是在每个方向上，网络结构只有一层RNN，即每个方向上只有一层循环单元（如LSTM单元或GRU单元）。虽然是单层的，但由于其双向特性，实际上每个时间点都有两个循环单元对信息进行处理。

     ![](https://github.com/ljgit1316/Picture_resource/blob/main/RNN_Pic\\图片20240610195407.png)

- 举例说明

  1. 定义一个双向循环神经网络（Bi-RNN）实例：

     ```python
     bi_rnn = nn.RNN(4, 3, 1, batch_first=True, bidirectional=True)
     ```

     这行代码创建了一个具有双向连接的RNN层，参数含义如下：

     - `4` 依然是输入序列的特征维度（每个时间步长的输入向量有4个元素）。
     - `3` 表示的是单向隐藏状态（hidden state）的维度；由于设置了 `bidirectional=True`，实际上模型会同时维护正向和反向两个隐藏状态，因此总的隐藏状态维度将是 `2 * 3`。
     - `1` 表示RNN层的数量，这里也是单层。
     - `batch_first=True` 保持输入张量的批量维度在最前面。
     - `bidirectional=True` 指定该RNN为双向的，这意味着对于每个时间步，除了向前传递的信息外，还会考虑向后传递的信息，从而能够捕捉序列中前后依赖关系。

  2. 创建输入数据张量：

     ```python
     input = torch.randn(1, 2, 4)
     ```

     这行代码生成了一个随机张量作为双向RNN的输入，其形状仍为 `(batch_size, sequence_length, feature_size)`，即 `(1, 2, 4)`。这表示有一个样本（batch_size=1），序列长度为2，每个时间步有4个特征。

  3. 对输入数据进行前向传播：

     ```python
     output, h_n = bi_rnn(input)
     ```

     将随机输入数据传入双向RNN进行前向计算。执行后获取的结果与单向RNN有所不同：

     - `output` 现在包含了正向和反向两个方向的输出，其形状为 `(batch_size, sequence_length, num_directions * hidden_size)`，在本例中为 `(1, 2, 2 * 3)`，即每个时间步有两个方向上的隐藏状态输出拼接而成的向量。

     - `h_n` 包含了最后时间步的正向和反向隐藏状态，形状为 `(num_layers * num_directions, batch_size, hidden_size)`，在本例中实际为 `(2, 1, 3)`，分别对应正向和反向隐藏状态各一个。每个隐藏状态向量都是相应方向上整个序列信息的汇总。

- 实例代码

  ```python
  import torch
  import torch.nn as nn
  
  # 输入数据
  # 输入数据为2个batch，每个batch有3个时间步，每个时间步有4个维度
  inputs = torch.rand(5, 3, 4)
  
  # 双向RNN,bidirectional=True,表示为双向RNN
  # 输入数据为4个维度，隐藏层为6，输出为1个维度，
  bi_rnn = nn.RNN(4, 6, 1, batch_first=True, bidirectional=True)
  
  # output: [batch_size, seq_len, num_directions * hidden_size]
  # h_n: [num_layers * num_directions, batch_size, hidden_size]
  out, h_n = bi_rnn(inputs)
  print(out.shape)
  print(h_n.shape)
  
  """
  torch.Size([5, 3, 12])
  torch.Size([2, 5, 6])
  """
  ```

  

