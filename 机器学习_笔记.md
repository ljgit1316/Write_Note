
# 机器学习

## 机器学习分类

### 1.监督学习

监督学习（Supervised Learning）是**从有标签的训练数据中学习模型**，**然后对某个给定的新数据利用模型预测它的标签**。如果分类标签精确度越高，则学习模型准确度越高，预测结果越精确。

监督学习主要用于**回归**和**分类**。

常见的监督学习的**回归算法有线性回归、回归树、K邻近、Adaboost、神经网络等。**

常见的监督学习的**分类算法有朴素贝叶斯、决策树、SVM、逻辑回归、K邻近、Adaboost、神经网络等。**

### 2.半监督学习

半监督学习（Semi-Supervised Learning）是**利用少量标注数据和大量无标注数据进行学习的模式。**

半监督学习**侧重于在有监督的分类算法中加入无标记样本来实现半监督分类。**

常见的半监督学习算法有Pseudo-Label、Π-Model、Temporal Ensembling、Mean Teacher、VAT、UDA、MixMatch、ReMixMatch、FixMatch等。

### 3.无监督学习

无监督学习（Unsupervised Learning）是**从未标注数据中寻找隐含结构的过程。**

无监督学习**主要用于关联分析、聚类和降维。**

常见的无监督学习算法有稀疏自编码（Sparse Auto-Encoder）、主成分分析（Principal Component Analysis, PCA）、K-Means算法（K均值算法）、DBSCAN算法（Density-Based Spatial Clustering of Applications with Noise）、最大期望算法（Expectation-Maximization algorithm, EM）等。

### 4.强化学习

强化学习（Reinforcement Learning）**类似于监督学习，但未使用样本数据进行训练，是是通过不断试错进行学习的模式。**

在强化学习中，有两个可以进行交互的对象：智能体（Agnet）和环境（Environment），还有四个核心要素：策略（Policy）、回报函数（收益信号，Reward Function）、价值函数（Value Function）和环境模型（Environment Model），其中环境模型是可选的。

## 机器学习应用场合

### 1.自然语言处理（NLP）

自然语言处理是人工智能中的重要领域之一，涉及计算机与人类自然语言的交互。NLP技术可以实现语音识别、文本分析、情感分析等任务，为智能客服、聊天机器人、语音助手等提供支持。

### 2.医疗诊断与影像分析

机器学习在医疗领域有着广泛的应用，包括医疗图像分析、疾病预测、药物发现等。深度学习模型在医疗影像诊断中的表现引人注目。

### 3.金融风险管理

机器学习在金融领域的应用越来越重要，尤其是在风险管理方面。模型可以分析大量的金融数据，预测市场波动性、信用风险等。

### 4.预测与推荐系统

机器学习在预测和推荐系统中也有广泛的应用，如销售预测、个性化推荐等。协同过滤和基于内容的推荐是常用的技术。

### 5.制造业和物联网

物联网（IoT）在制造业中的应用越来越广泛，机器学习可用于处理和分析传感器数据，实现设备预测性维护和质量控制。

### 6.能源管理与环境保护

机器学习可以帮助优化能源管理，减少能源浪费，提高能源利用效率。通过分析大量的能源数据，识别优化的机会。

### 7.决策支持与智能分析

机器学习在决策支持系统中的应用也十分重要，可以帮助分析大量数据，辅助决策制定。基于数据的决策可以更加准确和有据可依。

### 8.图像识别与计算机视觉

图像识别和计算机视觉是另一个重要的机器学习应用领域，它使计算机能够理解和解释图像。深度学习模型如卷积神经网络（CNN）在图像分类、目标检测等任务中取得了突破性进展。

## 机器学习项目开发步骤

5个基本步骤用于执行机器学习任务：

1. **收集数据**：无论是来自excel，access，文本文件等的原始数据，这一步（收集过去的数据）构成了未来学习的基础。相关数据的种类，密度和数量越多，机器的学习前景就越好。
2. **准备数据**：任何分析过程都会依赖于使用的数据质量如何。人们需要花时间确定数据质量，然后采取措施解决诸如缺失的数据和异常值的处理等问题。探索性分析可能是一种详细研究数据细微差别的方法，从而使数据的质量迅速提高。
3. **练模型**：此步骤涉及以模型的形式选择适当的算法和数据表示。清理后的数据分为两部分 - 训练和测试（比例视前提确定）; 第一部分（训练数据）用于开发模型。第二部分（测试数据）用作参考依据。
4. **评估模型**：为了测试准确性，使用数据的第二部分（保持/测试数据）。此步骤根据结果确定算法选择的精度。检查模型准确性的更好测试是查看其在模型构建期间根本未使用的数据的性能。
5. **提高性能**：此步骤可能涉及选择完全不同的模型或引入更多变量来提高效率。这就是为什么需要花费大量时间进行数据收集和准备的原因。

无论是任何模型，这5个步骤都可用于构建技术，当我们讨论算法时，您将找到这五个步骤如何出现在每个模型中！

## 数据集

### 1.sklearn玩具数据集

- 定义

  数据量小，数据在sklearn库的本地，只要安装了sklearn，不用上网就可以获取

- 图示

  ![real_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/real_1.png)

### 2.sklearn现实世界数据集

- 定义

  数据量大，数据只能通过网络获取

- 图示

  ![toal_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/toal_1.png)

### 3.sklearn加载玩具数据集

#### 3.1 鸢尾花数据

- 语法

  ```python
  from sklearn.datasets import load_iris
  iris = load_iris()#鸢尾花数据,返回一个Bunch对象
  ```

- 特征

  - ​	花萼长 sepal length  
  - ​	花萼宽 sepal width 
  - ​	花瓣长 petal length  
  - ​	花瓣宽 petal width

- 分类

  - ​	0-Setosa山鸢尾    
  - ​	1-Versicolour变色鸢尾   
  - ​	2-Virginica维吉尼亚鸢尾

- 实例

  ```python
  import pandas as pd
  import numpy as np
  from sklearn.datasets import load_iris
  iris=load_iris()
  data=iris.data
  target=iris.target
  target=target.reshape(len(target),1)
  iris_con=np.hstack([data,target])
  col=iris.feature_names
  col.append('target')
  iris_show=pd.DataFrame(iris_con,columns=col)
  ```

  ![bird](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/bird.png)

#### 3.2 糖尿病数据

- 语法

  ```python
  from sklearn.datasets import  load_diabetes
  diabetes=load_diabetes()
  ```

- 特征

  - ‌**年龄（‌age）‌**‌
  - ‌**性别（‌sex）‌**‌
  - ‌**身体质量指数（‌bmi）‌**‌
  - ‌**平均血压（‌bp）‌**‌
  - ‌**六种血清的化验数据（‌s1~s6）‌**‌，‌包括T细胞（‌tc）‌、‌低密度脂蛋白（‌ldl）‌、‌高密度脂蛋白（‌hdl）‌、‌促甲状腺激素（‌tch）‌、‌拉莫三嗪（‌ltg）‌和血糖水平（‌glu）‌。‌

- 实例

  ```python
  from sklearn.datasets import  load_diabetes
  diabetes=load_diabetes()
  data=diabetes.data
  target=diabetes.target
  target=target.reshape(len(target),1)
  diabetes_con=np.hstack([data,target])
  col=diabetes.feature_names
  col.append('target')
  diabetes_show=pd.DataFrame(diabetes_con,columns=col)
  
  print(diabetes_show)
  ```

  ![tang](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tang.png)



#### 3.3 数字数据集

- 语法

  ```python
  from sklearn.datasets import load_digits
  num=load_digits()
  ```

- 特征

  - 每个样本由64个特征组成，‌这些特征实际上是8x8像素图像中的每个像素的灰度值。‌灰度值通常是一个介于0到某个最大值（‌在这个数据集中是16）‌之间的整数，‌但在加载到sklearn中时，‌它们被转换为float64类型的值，‌并且通常会被归一化到[0, 1]范围内，。
  - 64个特征以线性方式排列，‌对应于8x8图像矩阵的展平版本。‌具体来说，‌第一个特征对应于图像左上角的像素，‌接下来的8个特征对应于第一行的其余像素，‌依此类推，‌直到遍历整个图像。‌

- 分类

  **含0-9共10种标签，‌各类样本均衡，‌特征为离散数值0-16之间**‌。‌

- 实例

  ```python
  from sklearn.datasets import load_digits
  num=load_digits()
  
  data=num.data
  target=num.target
  target=target.reshape(len(target),1)
  num_con=np.hstack([data,target])
  col=num.feature_names
  col.append('target')
  num_show=pd.DataFrame(num_con,columns=col)
  
  print(num_show)
  ```

  ![num](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/num.png)

#### 3.4 linnerud物理锻炼数据

- 语法

  ```python
  from sklearn.datasets import load_linnerud
  phy=load_linnerud()
  ```

- 特征

  Chins：引体向上

  Situps：仰卧起坐  

  Jumps：跳高  

- 目标参数

  Weight：体重  

  Waist：腰围  

  Pulse：脉搏

- 实例

  ```python
  from sklearn.datasets import load_linnerud
  phy=load_linnerud()
  
  data=phy.data
  target=phy.target
  phy_con=np.hstack([data,target])
  col=phy.feature_names
  col.append('Weight')
  col.append('Waist')
  col.append('Pulse')
  phy_show=pd.DataFrame(phy_con,columns=col)
  
  print(phy_show)
  ```

  ![phy](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/phy.png)

#### 3.5 葡萄酒数据集

- 语法

  ```python
  from sklearn.datasets import load_wine
  achlo=load_wine()
  ```

- 特征

  1.‌**酒精（‌alcohol）‌**‌：‌葡萄酒中的酒精含量。‌

  2.‌**苹果酸（‌malic_acid）‌**‌：‌葡萄酒中的苹果酸含量，‌苹果酸是一种有机酸，‌对葡萄酒的风味和口感有重要影响。‌

  3.‌**灰分（‌ash）‌**‌：‌葡萄酒中的灰分含量，‌灰分是葡萄酒中无机物质的总和，‌包括矿物质等。‌

  4.‌**灰的碱性（‌alcalinity_of_ash）‌**‌：‌灰分的碱性程度，‌反映了灰分中矿物质的种类和含量。‌

  5.‌**镁（‌magnesium）‌**‌：‌葡萄酒中的镁含量，‌镁是一种重要的矿物质，‌对葡萄酒的风味和稳定性有一定影响。‌

  6.‌**总酚（‌total_phenols）‌**‌：‌葡萄酒中的总酚含量，‌酚类物质是葡萄酒中的重要成分，‌对葡萄酒的颜色、‌风味和抗氧化性有贡献。‌

  7.‌**类黄酮（‌flavanoids）‌**‌：‌葡萄酒中的类黄酮含量，‌类黄酮是一类具有抗氧化性质的植物化学物质。‌

  8.‌**非黄烷类酚类（‌nonflavanoid_phenols）‌**‌：‌葡萄酒中的非黄烷类酚类物质含量，‌这些物质同样对葡萄酒的风味和抗氧化性有贡献。‌

  9.‌**花青素（‌proanthocyanins）‌**‌：‌葡萄酒中的花青素含量，‌花青素是一种强效的天然色素和抗氧化剂，‌对葡萄酒的颜色和稳定性有重要作用。‌

  10.‌**颜色强度（‌color_intensity）‌**‌：‌葡萄酒的颜色强度，‌反映了葡萄酒中色素的含量和分布。‌

  11.‌**色调（‌hue）‌**‌：‌葡萄酒的色调，‌即颜色的具体表现，‌如红葡萄酒可能呈现不同的红色调。‌

  12.‌**od280/od315稀释葡萄酒（‌od280/od315_of_diluted_wines）‌**‌：‌这是一个光学密度的比值，‌用于描述葡萄酒在特定波长下的吸收特性，‌与葡萄酒中的某些化学成分含量有关。‌

  13.‌**脯氨酸（‌proline）‌**‌：‌葡萄酒中的脯氨酸含量，‌脯氨酸是一种氨基酸，‌对葡萄酒的风味和口感有一定影响。‌

- 分类

  'class_0'

  'class_1'

  'class_2'

- 实例

  ```python
  from sklearn.datasets import load_wine
  achlo=load_wine()
  
  data=achlo.data
  target=achlo.target
  target=target.reshape(len(target),1)
  achlo_con=np.hstack([data,target])
  col=achlo.feature_names
  col.append('target')
  achlo_show=pd.DataFrame(achlo_con,columns=col)
  
  
  print(achlo_show)
  ```

  ![achol](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/achol.png)

#### 3.6 乳腺癌数据集

- 语法

  ```python
  from sklearn.datasets import load_breast_cancer
  cancer=load_breast_cancer()
  ```

- 特征

  1.‌**肿瘤的大小相关特征**‌：‌如半径、‌周长、‌面积等，‌这些特征直接反映了肿瘤的物理尺寸1。‌

  2.‌**肿瘤的形状和纹理特征**‌：‌如纹理（‌灰度值的标准偏差）‌、‌平滑度（‌半径的变化幅度）‌、‌密实度（‌周长的平方除以面积的商再减1）‌、‌凹度（‌凹陷部分轮廓的严重程度）‌、‌凹点（‌凹陷轮廓的数量）‌等，‌这些特征描述了肿瘤的表面形态和内部结构12。‌

  3.‌**其他生物学特征**‌：‌如对称性、‌分形维度等，‌这些特征提供了关于肿瘤生长方式和复杂性的额外信息

- 分类

  0表示良性肿瘤

  1表示恶性肿瘤

- 实例

  ```python
  from sklearn.datasets import load_breast_cancer
  cancer=load_breast_cancer()
  
  data=cancer.data
  target=cancer.target
  target=target.reshape(len(target),1)
  cancer_con=np.hstack([data,target])
  col=cancer.feature_names
  col=list(col)
  col.append('target')
  cancer_show=pd.DataFrame(cancer_con,columns=col)
  
  print(cancer_show)
  ```

  ![cancer](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/cancer.png)

### 4.sklearn获取现实世界数据集

#### 4.1获取20分类新闻数据

- 文件保存目录

  ```python
  """
  (1)所有现实世界数据，通过网络才能下载后，默认保存的目录可以使用下面api获取。实际上就是保存到家目录
  
  (2)下载时，有可能回为网络问题而出题，要“小心”的解决网络问题，不可言…..
  
  (3)第一次下载会保存的硬盘中，如果第二次下载，因为硬盘中已经保存有了，所以不会再次下载就直接加载成功了。
  
  """
  from sklearn import datasets
  ret=datasets.get_data_home()
  print(ret)
  #C:\Users\13167\scikit_learn_data
  ```

- 语法

  ```python
  sklearn.datasets.fetch_20newsgroups(data_home,subset)
  ```

- 参数

  data_home

  ```python
  =None
  这是默认值，下载的文件路径为 “C:/Users/ADMIN/scikit_learn_data/20news-bydate_py3.pkz”
  
  =自定义路径
  	例如 “./src”, 下载的文件路径为“./20news-bydate_py3.pkz”
  ```

  subset

  ```python
  “train”，只下载训练集
  “test”，只下载测试集
  “all”， 下载的数据包含了训练集和测试集
  ```

   return_X_y，决定着返回值的情况

  ```python
  当参数return_X_y值为False时， 函数返回Bunch对象,Bunch对象中有以下属性
      *data:特征数据集, 长度为18846的列表list, 每一个元素就是一篇新闻内容， 共有18846篇
      *target:目标数据集，长度为18846的数组ndarray, 第一个元素是一个整数，整数值为[0,20]
      *target_names:目标描述，长度为20的list
      *filenames：长度为18846的ndarray, 元素为字符串,代表新闻的数据位置的路径
      
  当参数return_X_y值为True时，函数返回值为元组，元组长度为2， 第一个元素值为特征数据集，第二个元素值为目标数据集
  ```

- 实例

  ```python
  from sklearn import datasets
  ret=sklearn.datasets.fetch_20newsgroups(data_home='./',subset='all')
  print(len(ret.data))
  print(ret.target.shape) #(18846,)
  print(ret.target_names) #20
  print(ret.filenames) #18846
  ```

  ![news](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/news.png)

#### 4.2加载加利福尼亚住房数据

- 语法

  ```python
  fetch_california_housing( *, data_home=None, download_if_missing=True, return_X_y=False, as_frame=False
  )
  ```

- 实例

  ```python
  from sklearn import datasets
  ret=datasets.fetch_california_housing(data_home='./',as_frame=True)
  print(ret.data)
  ```

  ![house](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/house.png)

### 5.本地csv数据创建和使用

#### 1.创建csv文件

方式1：打开计事本，写出如下数据，数据之间使用英文下的逗号, 保存文件后把后缀名改为csv

```python
milage,Liters,Consumtime,target
40920,8.326976,0.953952,3
14488,7.153469,1.673904,2
26052,1.441871,0.805124,1
75136,13.147394,0.428964,1
```

方式2：创建excel 文件,  填写数据，以csv为后缀保存文件

#### 2.pandas加载csv

```py
test=pd.read_csv('test1.csv')
print(test)
```

![csv](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/csv.png)

### 6.数据集划分

#### 1.函数

```python
sklearn.model_selection.train_test_split(*arrays，**options)

"""
(1) *array 
	这里用于接收1到多个"列表、numpy数组、稀疏矩阵或padas中的DataFrame"。	
	
(2) **options， 重要的关键字参数有：
 	    test_size 值为0.0到1.0的小数，表示划分后测试集占的比例
        random_state 值为任意整数，表示随机种子，使用相同的随机种子对相同的数据集多次划分结果是相同的。否则多半不同
        
(3)返回值说明
	返回值为列表list, 列表长度与形参array接收到的参数数量相关联, 形参array接收到的是什么类型，list中对应被划分出来的两部分就是什么类型
"""
```

#### 2.列表数据集划分

```python
from sklearn.model_selection import train_test_split
data1 = [1,    2,    3,    4,    5]
data2 = ["1a", "2a","3a", "4a",  "5a"]
#随机种子都使用了相同的整数(11)，所以划分的划分的情况是相同的。
a, b = train_test_split(data1, test_size=0.1, random_state=11)
print(a, b)
#[5, 1, 4, 2] [3]
a, b = train_test_split(data2, test_size=0.1, random_state=11)
print(a, b)
#['5a', '1a', '4a', '2a'] ['3a']
a, b, c, d  = train_test_split(data1, data2,  test_size=0.1, random_state=11)
print(a,b,c,d)
#[5, 1, 4, 2] [3] ['5a', '1a', '4a', '2a'] ['3a']
```

#### 3.ndarray数据集划分

```python
#划分前和划分后的数据类型是相同的
data1 = [1,    2,    3,    4,    5]
data2 = np.array(["1a", "2a","3a", "4a",  "5a"]) 
a, b, c, d  = train_test_split(data1, data2,  test_size=0.4, random_state=22)
print(a, b, c, d)  
#[4, 1, 5] [2, 3] ['4a' '1a' '5a'] ['2a' '3a']
print(type(a), type(b), type(c), type(d)) 
#<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>
```

#### 4.二维数组数据集划分

```python
from sklearn.model_selection import train_test_split
import numpy as np
data1 = np.arange(1, 16, 1)
data1.shape=(5,3)
print(data1)
a, b = train_test_split(data1,  test_size=0.4, random_state=22)
print(a)
print(b)

"""
[[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]
 [13 14 15]]
 
 [[10 11 12]
 [ 1  2  3]
 [13 14 15]]

 [[4 5 6]
 [7 8 9]]

"""
```

#### 5.DataFrame数据集划分

```python
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
#可以划分DataFrame, 划分后的两部分还是DataFrame
data1 = np.arange(1, 16, 1)
data1.shape=(5,3)
data1 = pd.DataFrame(data1, index=[1,2,3,4,5], columns=["one","two","three"])
print(data1)

a, b = train_test_split(data1,  test_size=0.4, random_state=22)
print(a)
print(b)

"""
 one  two  three
1    1    2      3
2    4    5      6
3    7    8      9
4   10   11     12
5   13   14     15

    one  two  three
4   10   11     12
1    1    2      3
5   13   14     15

    one  two  three
2    4    5      6
3    7    8      9

"""
```

#### 6.字典数据集划分

```python
"""
可以划分非稀疏矩阵

用于将字典列表转换为特征向量。这个转换器主要用于处理类别数据和数值数据的混合型数据集

1.对于类别特征`DictVectorizer` 会为每个不同的类别创建一个新的二进制特征，如果原始数据中的某个样本具有该类别，则对应的二进制特征值为1，否则为0。

2.对于数值特征保持不变，直接作为特征的一部分
"""
```

```python
from sklearn.feature_extraction import DictVectorizer
data = [{'city':'成都', 'age':30, 'temperature':20}, 
        {'city':'重庆','age':33, 'temperature':60}, 
        {'city':'北京', 'age':42, 'temperature':80},
        {'city':'上海', 'age':22, 'temperature':70},
        {'city':'成都', 'age':72, 'temperature':40},
       ]
transfer = DictVectorizer(sparse=True)
data_new = transfer.fit_transform(data)
print("data_new:\n", data_new)
x = data_new.toarray()
print(type(x))
print(x)
"""
(0,0)是矩阵的行列下标  30是值
data_new:
  (0, 0)	30.0
  (0, 3)	1.0
  (0, 5)	20.0
  (1, 0)	33.0
  (1, 4)	1.0
  (1, 5)	60.0
  (2, 0)	42.0
  (2, 2)	1.0
  (2, 5)	80.0
  (3, 0)	22.0
  (3, 1)	1.0
  (3, 5)	70.0
  (4, 0)	72.0
  (4, 3)	1.0
  (4, 5)	40.0
<class 'numpy.ndarray'>
# 第一行中:30表示age的值  0表示上海 0表示北京 1表示成都 0表示重庆 20表示temperature
[[30.  0.  0.  1.  0. 20.]
 [33.  0.  0.  0.  1. 60.]
 [42.  0.  1.  0.  0. 80.]
 [22.  1.  0.  0.  0. 70.]
 [72.  0.  0.  1.  0. 40.]]

"""
```

```python
a, b = train_test_split(data_new,  test_size=0.4, random_state=22)
print(a)
print("\n", b)

"""
  (0, 0)	22.0
  (0, 1)	1.0
  (0, 5)	70.0
  (1, 0)	30.0
  (1, 3)	1.0
  (1, 5)	20.0
  (2, 0)	72.0
  (2, 3)	1.0
  (2, 5)	40.0

  (0, 0)	33.0
  (0, 4)	1.0
  (0, 5)	60.0
  (1, 0)	42.0
  (1, 2)	1.0
  (1, 5)	80.0


"""
```

```python
#data_new.toarray()是ndarray
a, b = train_test_split(data_new.toarray(),  test_size=0.4, random_state=22)
print(a)
print("\n", b)
"""
[[22.  1.  0.  0.  0. 70.]
 [30.  0.  0.  1.  0. 20.]
 [72.  0.  0.  1.  0. 40.]]

 [[33.  0.  0.  0.  1. 60.]
 [42.  0.  1.  0.  0. 80.]]

"""
```

#### 7.鸢尾花数据集划分

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
iris = load_iris()
list = train_test_split(iris.data,iris.target,  test_size=0.2, random_state=22)
#x_train训练特征数据集,x_test测试特征数据集, y_train训练目标数据集,y_test测试目标数据集,
x_train, x_test, y_train, y_test = list   
#(120, 4) (30, 4) (120,) (30,)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
```

#### 8.现实世界数据集划分

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
import numpy as np
news = fetch_20newsgroups(data_home='./', subset='all')
list = train_test_split(news.data, news.target,test_size=0.2, random_state=22)
# """
# 返回值是一个list:其中有4个值，分别为训练集特征、测试集特征、训练集目标、测试集目标
x_train, x_test, y_train, y_test = list
# 15076 3770 (15076,) (3770,)
print(len(x_train), len(x_test), y_train.shape, y_test.shape) 
```

```python
#“加载加利福尼亚住房数据集”,并进行数据集划分
from sklearn import datasets
ret=datasets.fetch_california_housing(data_home='./',as_frame=True)
list=train_test_split(ret.data,ret.target,test_size=0.2,random_state=22)
x_train,x_target,y_train,y_target=list
print(x_train,x_target)
print(y_train,y_target)

```
## 特征工程

### 1.特征工程概念

- 定义

  特征工程是将任意数据(如文本或图像)转换为可用于机器学习的**数字特征**,比如:字典特征提取(特征离散化)、文本特征提取、图像特征提取。

- 步骤

  - 特征提取， 如果不是表格数据，要进行特征提取，比如字典特征提取，文本特征提取

  - 无量刚化(预处理)

    - 归一化
    - 标准化

  - 降维

    - 底方差过滤特征选择

    - 主成分分析-PCA降维

### 2.特征工程相关API

- 实例化转换器类对象

  ```python
  DictVectorizer  	字典特征提取
  CountVectorizer 	文本特征提取
  TfidfVectorizer 	TF-IDF文本特征词的重要程度特征提取 
  MinMaxScaler 		归一化
  StandardScaler 		标准化
  VarianceThreshold 	底方差过滤降维
  PCA  				主成分分析降维
  ```

- 转换器对象调用

  ```python
  """
  fit_transform()进行转换, 其中fit用于计算数据，transform进行最终转换
  fit_transform()可以使用fit()和transform()代替
  """
  data_new = transfer.fit_transform(data)
  可写成
  transfer.fit(data)
  data_new = transfer.transform(data)
  ```



### 3.特征工程存储形式

- 稀疏矩阵定义

  稀疏矩阵是指一个矩阵中大部分元素为零，只有少数元素是非零的矩阵，且非零元素分布没有明显的规律。

- 三元组表

  三元组表就是一种稀疏矩阵类型数据,存储非零元素的行索引、列索引和值。除了列出的有值, 其余全是0。

  ```python
  (0,0) 10
  
  (0,1) 20
  
  (2,0) 90
  
  (2,20) 8
  
  (8,0) 70
  ```

- 稠密矩阵

  矩阵中非零元素的数量与总元素数量相比接近或相等，也就是说矩阵中的大部分元素都是非零的。

- 总结

  - **存储**：稀疏矩阵使用特定的存储格式来节省空间，而稠密矩阵使用常规的数组存储所有元素，无论其是否为零。
  - **计算**：稀疏矩阵在进行计算时可以利用零元素的特性跳过不必要的计算，从而提高效率。而稠密矩阵在计算时需要处理所有元素，包括零元素。
  - **应用领域**：稀疏矩阵常见于大规模数据分析、图形学、自然语言处理、机器学习等领域，而稠密矩阵在数学计算、线性代数等通用计算领域更为常见。

### 4.DictVectorizer 字典列表特征提取

- 语法

  ```python
  #创建转换器对象
  sklearn.feature_extraction.DictVectorizer(sparse=True)
  """
  sparse=True返回类型为csr_matrix的稀疏矩阵
  
  sparse=False表示返回的是数组,数组可以调用.toarray()方法将稀疏矩阵转换为数组
  """
  ```

  ```python
  #转换器对象
  """
  转换器对象调用fit_transform(data)函数，参数data为一维字典数组或一维字典列表,返回转化后的矩阵或数组
  
  转换器对象get_feature_names_out()方法获取特征名
  """
  ```

- 实例:提取稀疏矩阵对应的数组

  ```python
  from sklearn.feature_extraction import DictVectorizer
  data=[
      {"name":"jack","age":23,"money":1000,"com":"小米"},
       {"name":"rose","age":33,"money":2000,"com":"腾讯"},
        {"name":"marry","age":83,"money":25,"com":"苹果"},
         {"name":"joe","age":123,"money":899,"com":"小米"},
          {"name":"小蒋","age":33,"money":1,"com":"华清"},
  ]
  dict=DictVectorizer(sparse=False)
  """
  sparse=True返回类型为csr_matrix的稀疏矩阵
  
  sparse=False表示返回的是数组,数组可以调用.toarray()方法将稀疏矩阵转换为数组
  """
  data=dict.fit_transform(data)
  """
  fit_transform()进行转换, 其中fit用于计算数据，transform进行最终转换
  fit_transform()可以使用fit()和transform()代替
  """
  print(dict.get_feature_names_out())
  #获取特征标签
  print(data)
  ```

  ![dict_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/dict_1.png)

- 实例：提取稀疏矩阵

  ```python
  from sklearn.feature_extraction import DictVectorizer
  data=[
      {"name":"jack","age":23,"money":1000,"com":"小米"},
       {"name":"rose","age":33,"money":2000,"com":"腾讯"},
        {"name":"marry","age":83,"money":25,"com":"苹果"},
         {"name":"joe","age":123,"money":899,"com":"小米"},
          {"name":"小蒋","age":33,"money":1,"com":"华清"},
  ]
  dict=DictVectorizer(sparse=True)
  """
  sparse=True返回类型为csr_matrix的稀疏矩阵
  
  sparse=False表示返回的是数组,数组可以调用.toarray()方法将稀疏矩阵转换为数组
  """
  data=dict.fit_transform(data)
  """
  fit_transform()进行转换, 其中fit用于计算数据，transform进行最终转换
  fit_transform()可以使用fit()和transform()代替
  """
  print(dict.get_feature_names_out())
  #获取特征标签
  print(data)
  ```

  ![dict_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/dict_2.png)

### 5.CountVectorizer 文本特征提取

- 语法

  ```python
  sklearn.feature_extraction.text.CountVectorizer
  """
  CountVectorizer()构造函数关键字参数stop_words，值为list，表示词的黑名单
  
  fit_transform函数的返回值为稀疏矩阵
  
  """
  ```

- 实例：英文文本提取

  ```python
  from sklearn.feature_extraction.text import CountVectorizer
  data=['man','what can i say','manba out','say man i am']
  count=CountVectorizer(stop_words=['man'])#过滤掉man单词
  data=count.fit_transform(data).toarray()#转化为数组
  data_show=pd.DataFrame(data,index=['第一','第二','第三','第四'],columns=count.get_feature_names_out())
  print(data_show)
  ```

  ![count_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/count_1.png)

- 实例：中文文本提取

  ```python
  #a.中文文本不像英文文本，中文文本文字之间没有空格，所以要先分词，一般使用jieba分词.
  
  data = "在如今的互联网世界，正能量正成为澎湃时代的大流量"
  data = jieba.cut(data)
  data = list(data)
  print(data) #['在', '如今', '的', '互联网', '世界', '，', '正', '能量', '正', '成为', '澎湃', '时代', '的', '大', '流量']
  data = "".join(data)
  print(data) #"在如今的互联网世界 ，正能量正成为澎湃时代的大流量"
  ```

  ```python
  def china_txt(txt):
      return ' '.join(list(jieba.cut(txt)))
  
  data=['纽约联储调查显示失业预期创历史新高','德克萨斯电网面临夏季最大考验','德克萨斯和纽约']
  data_new=[china_txt(x) for x in data]
  count=CountVectorizer()
  count_data=count.fit_transform(data_new).toarray()
  data_show=pd.DataFrame(count_data,columns=count.get_feature_names_out())
  
  print(data_show)
  ```

  ![count_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/count_2.png)

### 6.TfidfVectorizer TF-IDF 文本特征词的重要程度特征提取 

- 算法

  ![compute](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/compute.png)

- 语法

  ```python
  sklearn.feature_extraction.text.TfidfVectorizer()
  """
  构造函数关键字参数stop_words，表示词特征黑名单
  
  fit_transform函数的返回值为稀疏矩阵
  """
  ```

- 实例

  ```python
  from sklearn.feature_extraction.text import TfidfVectorizer
  
  def china_txt(txt):
      return ' '.join(list(jieba.cut(txt)))
  
  data=['纽约联储调查显示失业预期创历史新高','德克萨斯电网面临夏季最大考验','德克萨斯和纽约']
  data_new=[china_txt(x) for x in data]
  count=TfidfVectorizer(stop_words=['预期','考验'])
  count_data=count.fit_transform(data_new).toarray()
  data_show=pd.DataFrame(count_data,columns=count.get_feature_names_out())
  print(count.get_feature_names_out())
  print(count_data)
  ```

  ![tf](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tf.png)

### 7.无量纲化-预处理

- 定义

  无量纲，即没有单位的数据

- 举例说明

  | 编号id | 身高 h   | 收入 s    | 体重 w  |
  | ------ | -------- | --------- | ------- |
  | 1      | 1.75(米) | 15000(元) | 120(斤) |
  | 2      | 1.5(米)  | 16000(元) | 140(斤) |
  | 3      | 1.6(米)  | 20000(元) | 100(斤) |

  假设算法中需要求它们之间的欧式距离, 这里以编号1和编号2为示例:

  $L = \sqrt{(1.75-1.5)^2+(15000-16000)^2+(120-140)^2}$

  从计算上来看, 发现身高对计算结果没有什么影响, 基本主要由收入来决定了,但是现实生活中,身高是比较重要的判断标准.  所以需要无量纲化.

#### 1.MinMaxScaler 归一化

- 定义

  通过对原始数据进行变换把数据映射到指定区间**(默认为0-1)**

- 公式

  ![G_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/G_1.png)

  这里的 𝑥min 和 𝑥max 分别是每种特征中的最小值和最大值，而 𝑥是当前特征值，𝑥scaled 是归一化后的特征值。

  若要缩放到其他区间，可以使用公式：**x=x*(max-min)+min**

  比如 [-1, 1]的公式为:

  ![G_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/G_2.png)

  手算过程:

  ![G_3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/G_3.png)

- 语法

  ```python
  sklearn.preprocessing.MinMaxScaler(feature_range)
  """
  参数:feature_range=(0,1) 归一化后的值域,可以自己设定
  
  fit_transform函数归一化的原始数据类型可以是list、DataFrame和ndarray, 不可以是稀疏矩阵
  
  fit_transform函数的返回值为ndarray
  
  """
  ```

- 实例：list数据归一化

  ```python
  from sklearn.preprocessing import MinMaxScaler
  data=[[110,1243,23255,238423,42314],
        [103,1223,23535,234423,42335],
        [103,1223,23535,234423,42332],
        [103,1223,23535,234423,42339],
        [103,1223,23535,234423,42330],
        [103,1223,23535,234423,42331],
        [170,1253,23755,234223,10]]
  scale=MinMaxScaler(feature_range=(0,1))
  data=scale.fit_transform(data)
  print(data)
  ```

  ![min_max_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/min_max_1.png)

- 实例：dataFrame数据归一化

  ```python
  from sklearn.preprocessing import MinMaxScaler
  scale=MinMaxScaler(feature_range=(0,1))
  data=[[12,22,4],[22,23,1],[11,23,9]]
  data = pd.DataFrame(data=data, index=["一","二","三"], columns=["一列","二列","三列"])
  data_show=scale.fit_transform(data)
  print(data_show)
  ```

  ![min_max_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/min_max_2.png)

- 实例：ndarray数据归一化

  ```python
  from sklearn.preprocessing import MinMaxScaler
  data=[
      {"name":"jack","age":23,"money":1000,"com":"小米"},
       {"name":"rose","age":33,"money":2000,"com":"腾讯"},
        {"name":"marry","age":83,"money":25,"com":"苹果"},
         {"name":"joe","age":123,"money":899,"com":"小米"},
          {"name":"小蒋","age":33,"money":1,"com":"华清"},
  ]
  scale=MinMaxScaler()
  dict=DictVectorizer()
  data_dict=dict.fit_transform(data).toarray()
  data_scale=scale.fit_transform(data_dict)
  print(data_scale)
  ```

  ![min_max_3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/min_max_3.png)

- 缺点

  最大值和最小值容易受到异常点影响，所以**鲁捧性(健壮性)**较差。所以常使用标准化的无量钢化

#### 2.StandardScaler 标准化

- 定义

  标准化是一种数据预处理技术，也称为数据归一化或特征缩放。它的目的是将不同特征的数值范围缩放到统一的标准范围，以便更好地适应一些机器学习算法，特别是那些对输入数据的尺度敏感的算法。

- 公式

  最常见的标准化方法是Z-score标准化，也称为零均值标准化。它通过对每个特征的值减去其均值，再除以其标准差，将数据转换为均值为0，标准差为1的分布。

  ![标准化](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/标准化.png)

  其中，z是转换后的数值，x是原始数据的值，μ是该特征的均值，σ是该特征的标准差

- 语法

  ```python
  sklearn.preprocessing.StandardScale
  """
  与MinMaxScaler一样，原始数据类型可以是list、DataFrame和ndarray
  
  fit_transform函数的返回值为ndarray,   归一化后得到的数据类型都是ndarray
  """
  ```

- 实例

  ```python
  from sklearn.preprocessing import StandardScaler
  data=[
      {"name":"jack","age":23,"money":1000,"com":"小米"},
       {"name":"rose","age":33,"money":2000,"com":"腾讯"},
        {"name":"marry","age":83,"money":25,"com":"苹果"},
         {"name":"joe","age":123,"money":899,"com":"小米"},
          {"name":"小蒋","age":33,"money":1,"com":"华清"},
  ]
  scale=StandardScaler()
  dict=DictVectorizer()
  data_dict=dict.fit_transform(data).toarray()
  data_scale=scale.fit_transform(data_dict)
  print('ndarry标准化归一：\n',data_scale)
  
  
  scale=StandardScaler()
  data=[[12,22,4],[22,23,1],[11,23,9]]
  data = pd.DataFrame(data=data, index=["一","二","三"], columns=["一列","二列","三列"])
  data_show=scale.fit_transform(data)
  print('DataFrame标准化归一：\n',data_show)
  
  
  data=[[110,1243,23255,238423,42314],
        [103,1223,23535,234423,42335],
        [103,1223,23535,234423,42332],
        [103,1223,23535,234423,42339],
        [103,1223,23535,234423,42330],
        [103,1223,23535,234423,42331],
        [170,1253,23755,234223,10]]
  scale=StandardScaler()
  data=scale.fit_transform(data)
  print('list标准化归一：\n',data)
  ```

  ![standard](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/standard.png)

- 重点

  使用如`StandardScaler`这样的数据转换器时，`fit`、`fit_transform`和`transform`这三个方法的使用是至关重要的，它们各自有不同的作用：

  1. **fit**:
     - 这个方法用来计算数据的统计信息，比如均值和标准差（在`StandardScaler`的情况下）。这些统计信息随后会被用于数据的标准化。
     - 你应当仅在训练集上使用`fit`方法。
  2. **fit_transform**:
     - 这个方法相当于先调用`fit`再调用`transform`，但是它在内部执行得更高效。
     - 它同样应当仅在训练集上使用，它会计算训练集的统计信息并立即应用到该训练集上。
  3. **transform**:
     - 这个方法使用已经通过`fit`方法计算出的统计信息来转换数据。
     - 它可以应用于任何数据集，包括训练集、验证集或测试集，但是应用时使用的统计信息必须来自于训练集。

  当你在预处理数据时，首先需要在训练集`X_train`上使用`fit_transform`，这样做可以一次性完成统计信息的计算和数据的标准化。这是因为我们需要确保模型是基于训练数据的统计信息进行学习的，而不是整个数据集的统计信息。

  **一旦`scaler`对象在`X_train`上被`fit`，它就已经知道了如何将数据标准化。**这时，对于测试集`X_test`，我们只需要使用`transform`方法，因为我们不希望在测试集上重新计算任何统计信息，也不希望测试集的信息影响到训练过程。如果我们对`X_test`也使用`fit_transform`，测试集的信息就可能会影响到训练过程。

  **总结来说:我们常常是先fit_transform(x_train)然后再transform(x_text)**

### 8.特征降维

- 定义

  降维就是去掉一些特征,或者转化多个特征为少量个特征

- 作用

  **特征降维其目的**:是减少数据集的维度，同时尽可能保留数据的重要信息。

  **特征降维的好处**:

  减少计算成本：在高维空间中处理数据可能非常耗时且计算密集。降维可以简化模型，降低训练时间和资源需求。

  去除噪声：高维数据可能包含许多无关或冗余特征，这些特征可能引入噪声并导致过拟合。降维可以帮助去除这些不必要的特征。

- 方法

  - 特征选择
    - 从原始特征集中挑选出最相关的特征
  - 主成份分析(PCA)
    - 主成分分析就是把之前的特征通过一系列数学计算，形成新的特征，新的特征数量会小于之前特征数量

#### 1.特征选择

##### 1.1 VarianceThreshold 低方差过滤特征选择

- 定义

  **Filter(过滤式):** 主要探究特征本身特点， 特征与特征、特征与目标 值之间关联

  - 方差选择法: 低方差特征过滤

    如果**一个特征的方差很小，说明这个特征的值在样本中几乎相同或变化不大**，包含的信息量很少，模型很难通过该特征区分不同的对象,**比如区分甜瓜子和咸瓜子还是蒜香瓜子,如果有一个特征是长度,这个特征相差不大可以去掉。**

    1. **计算方差**：对于每个特征，计算其在训练集中的方差(**每个样本值与均值之差的平方,在求平均**)。
    2. **设定阈值**：选择一个方差阈值，任何低于这个阈值的特征都将被视为低方差特征。
    3. **过滤特征**：移除所有方差低于设定阈值的特征

- 语法

  ```python
  #创建对象，准备把方差为等于小于2的去掉，threshold的缺省值为2.0
  sklearn.feature_selection.VarianceThreshold(threshold=2.0)
  
  #把x中低方差特征去掉, x的类型可以是DataFrame、ndarray和list
  VananceThreshold.fit_transform(x)
  
  fit_transform函数的返回值为ndarray
  ```

- 实例

  ```python
  from sklearn.feature_selection import VarianceThreshold
  
  data=[[110,1243,23255,238423,42314],
        [103,1223,23535,234423,42335],
        [103,1223,23535,234423,42332],
        [103,1223,23535,234423,42339],
        [103,1223,23535,234423,42330],
        [103,1223,23535,234423,42331],
        [170,1253,23755,234223,10]]
  data=pd.DataFrame(data)
  threshold=VarianceThreshold(threshold=1000)
  data_hold=threshold.fit_transform(data)
  print('原始数据：\n',data)
  print('设定阈值后的数据：\n',data_hold)
  ```

  ![hold_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/hold_1.png)

##### 1.2 根据相关系数的特征选择

- 定义

  **正相关性**（Positive Correlation）是指两个变量之间的一种统计关系，其中一个变量的增加通常伴随着另一个变量的增加，反之亦然。正相关性通常用正值的相关系数来表示，**这个值介于0和1之间。当相关系数等于1时，表示两个变量之间存在完美的正相关关系，**即一个变量的值可以完全由另一个变量的值预测。

  - 如果第一个变量增加，第二个变量也有很大的概率会增加。
  - 同样，如果第一个变量减少，第二个变量也很可能会减少。

  **负相关性**（Negative Correlation）与正相关性刚好相反,但是也说明相关,比如运动频率和BIM体重指数程负相关。

  **不相关**：指两者的相关性很小,一个变量变化不会引起另外的变量变化,只是没有线性关系. 比如饭量和智商。

  **皮尔逊相关系数**（Pearson correlation coefficient)是一种度量两个变量之间线性相关性的统计量。它提供了两个变量间关系的方向（正相关或负相关）和强度的信息。皮尔逊相关系数的取值范围是 \[−1,1]。

  - $\rho=1$ 表示完全正相关，即随着一个变量的增加，另一个变量也线性增加。
  - $\rho=-1$  表示完全负相关，即随着一个变量的增加，另一个变量线性减少。
  - $\rho=0$ 表示两个变量之间不存在线性关系。

  相关系数$\rho$的绝对值为0-1之间，绝对值越大，表示越相关，当两特征完全相关时，两特征的值表示的向量是在同一条直线上，当两特征的相关系数绝对值很小时，两特征值表示的向量接近在同一条直线上。

- 公式

  对于两组数据 𝑋={𝑥1,𝑥2,...,𝑥𝑛} 和 𝑌={𝑦1,𝑦2,...,𝑦𝑛}，皮尔逊相关系数可以用以下公式计算：

	$\rho=\frac{\text{Cos}(x, y)}{\sqrt{D x} \cdot \sqrt{D y}}=\frac{E[(x-E x)(y-E y)]}{\sqrt{D x} \cdot \sqrt{D y}}=\frac{\sum_{i=1}^{n}(x-\tilde{x})(y-\bar{y}) /(n-1)}{\sqrt{\sum_{i=1}^{n}(x-\bar{x})^{2} /(n-1)} \cdot \sqrt{\sum_{i=1}^{n}(y-\bar{y})^{2} /(n-1)}}$

  $\bar{x}$和 $\bar{y}$ 分别是𝑋和𝑌的平均值

  |ρ|<0.4为低度相关;    0.4<=|ρ|<0.7为显著相关；  0.7<=|ρ|<1为高度相关

- 语法

  ```python
  scipy.stats.personr(x, y) #计算两特征之间的相关性
  """
  返回对象有两个属性:
  
     statistic皮尔逊相关系数[-1,1]   
  
     pvalue零假设(了解),统计上评估两个变量之间的相关性,越小越相关
     
  """
  ```

- 实例

  ```python
  from scipy.stats import pearsonr
  x=np.array([1,2,3,4, 5, 6, 7, 8,9])
  y_1=np.array([10,20,30,40,50,60,70,80,90])
  y_2=np.array([100,200,300,10, 20, 0, 70, 1000,200])
  print("x:\n",x)
  print("y_1:\n",y_1)
  print("y_2:\n",y_2)
  ret_1=pearsonr(x,y_1)
  ret_2=pearsonr(x,y_2)
  print(ret_1)
  print(ret_2)
  ```

  ![pearsonr](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pearsonr.png)

#### 2.主成分分析(PCA)

- 定义

  PCA的核心目标是从原始特征空间中找到一个新的坐标系统，使得数据在新坐标轴上的投影能够最大程度地保留数据的方差，同时减少数据的维度。

- 原理

  ![pca_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_1.png)

  $x_0$投影到L的大小为$x_0*cos  \alpha$

  $y_0$投影到L的大小为$y_0*sin\alpha$ 

  使用$(x_0,y_0)$表示一个点，  表明该点有两个特征， 而映射到L上有一个特征就可以表示这个点了。这就达到了降维的功能 。

  投影到L上的值就是降维后保留的信息，投影到与L垂直的轴上的值就是丢失的信息。保留信息/丢失信息=信息保留的比例

  下图中红线上点与点的距离是最大的，所以在红色线上点的方差最大，粉红线上的刚好相反. 

  所以红色线上点来表示之前点的信息损失是最小的。

  ![pca_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_2.png)

- 步骤

  - 得到矩阵

  - 用矩阵P对原始数据进行线性变换，得到新的数据矩阵Z,每一列就是一个主成分, 如下图就是把10维降成了2维,得到了两个主成分

    ![pca_4](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_4.png)

  - 根据主成分的方差等，确定最终保留的主成分个数， 方差大的要留下。**一个特征的多个样本的值如果都相同，则方差为0， 则说明该特征值不能区别样本，所以该特征没有用。**

  比如下图的二维数据要降为一维数据，图形法是把所在数据在二维坐标中以点的形式标出，然后给出一条直线，让所有点垂直映射到直线上，该直线有很多，只有点到线的距离之和最小的线才能让之前信息损失最小。

  这样之前所有的二维表示的点就全部变成一条直线上的点，从二维降成了一维。

  ![pca_3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_3.png)

  上图是一个从二维降到一维的示例：的原始数据为

  | 特征1-X1 | 特征2-X2 |
  | -------- | -------- |
  | -1       | -2       |
  | -1       | 0        |
  | 0        | 0        |
  | 2        | 1        |
  | 0        | 1        |

  降维后新的数据为

  | 特征3-X0 |
  | -------- |
  | -3/√2    |
  | -1/√2    |
  | 0        |
  | 3/√2     |
  | -1/√2    |

- 语法

  ```python
  from sklearn.decomposition import PCA
  PCA(n_components=None)
  """
  - 主成分分析
  - n_components:
    - 实参为小数时：表示降维后保留百分之多少的信息
    - 实参为整数时：表示减少到多少特征
  """
  ```

- 实例

  ```python
  from sklearn.decomposition import PCA
  data=[[110,1243,23255,238423,42314],
        [103,1223,23535,234423,42335],
        [103,1223,23535,234423,42332],
        [103,1223,23535,234423,42339],
        [103,1223,23535,234423,42330],
        [103,1223,23535,234423,42331],
        [170,1253,23755,234223,10]]
  
  pca_intger=PCA(n_components=3)#降维至三维度
  pca_float=PCA(n_components=0.4)#保留百分之40的数据
  data_integer=pca_intger.fit_transform(data)
  data_float=pca_float.fit_transform(data)
  print('降维整数数据：\n',data_integer)
  print('降维小数数据：\n',data_float)
  ```

  ![pca_end](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_end.png)

## Sklearn机器学习概述

```python
1.实例化预估器(估计器)对象(estimator)， 预估器对象很多,都是estimator的子类
	（1）用于分类的预估器
		sklearn.neighbors.KNeighborsClassifier k-近邻
		sklearn.naive_bayes.MultinomialNB 贝叶斯
		sklearn.linear_model.LogisticRegressioon 逻辑回归
		sklearn.tree.DecisionTreeClassifier 决策树
		sklearn.ensemble.RandomForestClassifier 随机森林
	(2)用于回归的预估器
		sklearn.linear_model.LinearRegression线性回归
		sklearn.linear_model.Ridge岭回归
	(3)用于无监督学习的预估器
		sklearn.cluster.KMeans 聚类
2.进行训练，训练结束后生成模型
	estimator.fit(x_train, y_train)
3.模型评估
	(1)方式1，直接对比
		y_predict = estimator.predict(x_test)
		y_test == y_predict
  	(2)方式2, 计算准确率
  		accuracy = estimator.score(x_test, y_test)
```

## KNN分类算法

### 1.两种常用样本距离测算

![ou_ha](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ou_ha.png)

#### 1.1欧式距离

- 定义：欧式距离是最常见的距离度量方法，也称为直线距离。在二维空间中，欧式距离计算两点之间的直线距离。

- 缺点：在使用此距离度量之前，需要对数据进行标准化。随着数据维数的增加，欧氏距离的用处也就越小。

  ![ou_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ou_1.png)

  ![ou_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ou_2.png)

#### 1.2曼哈顿距离

- 定义：曼哈顿距离是计算两点之间水平或垂直线段的距离之和，也称为城市街区距离或L1距离。

- 缺点:由于它不是可能的最短路径，它比欧几里得距离更有可能给出一个更高的距离值。随着数据维数的增加，曼哈顿距离的用处也就越小。

  ![ha_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ha_1.png)

  ![ha_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ha_2.png)

### 2.KNN算法原理

- 定义

  K-近邻算法（K-Nearest Neighbors，简称KNN）,根据K个邻居样本的类别来判断当前样本的类别。

  如果一个样本在特征空间中的k个最相似(最邻近)样本中的大多数属于某个类别，则该类本也属于这个类别

- 例子说明

  ![exam](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/exam.png)

  根据KNN算法唐人街探案》电影属于**喜剧类型**：

  ![exam_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/exam_2.png)

### 3.KNN缺点

- 对于大规模数据集，计算量大，因为需要计算测试样本与所有训练样本的距离。

- 对于高维数据，距离度量可能变得不那么有意义，这就是所谓的“维度灾难”

- 需要选择合适的k值和距离度量，这可能需要一些实验和调整

### 4.KNN-API

- 语法

  ```python
  from sklearn.neighbors import KNeighborsClassifier
  
  knn=KNeighborsClassifier(n_neighbors=5, algorithm='auto')
  ```

- 参数

  **n_neighbors: int, default=5,** 默认情况下用于kneighbors查询的近邻数，就是K
  **algorithm:{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’。**找到近邻的方式，注意不是计算距离的方式，开发中默认值'auto'

- 训练与预测

  (1) fit(x,y) 使用X作为训练数据和y作为目标数据  
  (2) predict(X) 预测提供的数据，得到预测数据

### 5.KNN实例

- KNN算法—鸢尾花

  ```python
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  #1.获取数据
  iris=load_iris()
  data=np.array(iris.data)
  target=np.array(iris.target)
  #2.分割数据（划分训练数据，测试数据）
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,random_state=44,shuffle=True)
  #3.特征工程（标准化）
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)#保持测试集拥有与训练集相同的标准化
  #4.模型训练(传入训练数据和标签)
  knn=KNeighborsClassifier(n_neighbors=5)
  knn.fit(x_train,y_train)
  #5.模型评估（预测数据）
  y_predict=knn.predict(x_test)
  #方法1:对比预测值和真实值
  print('模型预测结果：',y_predict==y_test)
  #方法2：计算准确率
  score=knn.score(x_test,y_test)
  print('模型预测率：',score)
  print('模型预测率：',score)
  ```

  ![hua](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/hua.png)

- KNN算法—数字数据

  ```python
  from sklearn.datasets import load_digits
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  #1.加载数据
  digit=load_digits()
  data=np.array(digit.data)
  target=np.array(digit.target)
  #2.划分数据
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,random_state=44,shuffle=True)
  #3.标准化
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)#保持测试集拥有与训练集相同的标准化
  #4.训练模型(传入训练数据和标签)
  knn=KNeighborsClassifier(n_neighbors=5)
  knn.fit(x_train,y_train)
  #5.模型评估(预测测试集)
  y_predict=knn.predict(x_test)
  #方法1:对比预测值和真实值
  print('模型预测结果：',y_predict==y_test)
  #方法2：计算准确率
  score=knn.score(x_test,y_test)
  print('模型预测率：',score)
  ```

  ![digit](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/digit.png)

- KNN算法—葡萄酒

  ```python
  from sklearn.datasets import load_wine
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  
  #1.加载数据
  acho=load_wine()
  data=acho.data
  target=acho.target
  #2.划分数据
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,random_state=44,shuffle=True)
  #3.标准化
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)
  #4.训练模型(传入训练数据和标签)
  knn=KNeighborsClassifier(n_neighbors=7)
  knn.fit(x_train,y_train)
  #5.模型评估(预测测试集)
  y_predict=knn.predict(x_test)
  #方法1:对比预测值和真实值
  print('模型预测结果：',y_predict==y_test)
  #方法2：计算准确率
  score=knn.score(x_test,y_test)
  print('模型预测率：',score)
  ```

  ![acho](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/acho.png)

- KNN算法—乳腺癌

  ```python
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  
  # 1.加载数据
  cancer = load_breast_cancer()
  data = cancer.data
  target = cancer.target
  # 2.划分数据
  x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.3, shuffle=True, random_state=44)
  # 3.标准化
  stand = StandardScaler()
  x_train = stand.fit_transform(x_train)
  x_test = stand.transform(x_test)
  # 4.训练模型(传入训练数据和标签)
  knn = KNeighborsClassifier(n_neighbors=7)
  knn.fit(x_train, y_train)
  # 5.模型评估(预测测试集)
  y_predict = knn.predict(x_test)
  # 方法1:对比预测值和真实值
  print('模型预测结果：', y_predict == y_test)
  # 方法2：计算准确率
  score = knn.score(x_test, y_test)
  print('模型预测率：', score)
  ```

  ![cancer_knn](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/cancer_knn.png)

- KNN算法—森林覆盖类型数据

  ```python
  from sklearn.datasets import fetch_covtype
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  
  # 1.加载数据
  """"
  数据集中的特征可以分为两大类：
  土壤类型特征 (10 个特征)：
      每个土壤类型是一个二进制特征，指示样本是否属于该土壤类型。
      特征编号为 Soil_Type1 到 Soil_Type40，但并非所有土壤类型都出现。
  野生动植物测量值 (44 个特征)：
      这些特征包括海拔、坡度、水平距离到水体、垂直距离到水体等。
      特征编号为 Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology,
                Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, 
               Horizontal_Distance_To_Fire_Points 等。
  类别数量: 7 类不同的森林覆盖类型。
  """
  ret=datasets.fetch_covtype(data_home='./')
  data=ret.data
  target=ret.target
  #2.划分数据
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3,shuffle=True,random_state=44)
  # 3.标准化
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)
  # 4.训练模型
  knn=KNeighborsClassifier(n_neighbors=7)
  knn.fit(x_train,y_train)
  # 5.模型评估
  y_predict=knn.predict(x_test)
  # 方法1:对比预测值和真实值
  print('模型预测结果：', y_predict == y_test)
  # 方法2：计算准确率
  score = knn.score(x_test, y_test)
  print('模型预测率：', score)
  ```

  ![forest](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/forest.png)

### 6.模型保存与加载

```python
import joblib
# 保存模型
joblib.dump(estimator, "my_ridge.pkl")
# 加载模型
estimator = joblib.load("my_ridge.pkl")
#使用模型预测
y_test=estimator.predict([[0.4,0.2,0.4,0.7]])
print(y_test)
```

## 模型选择与调优

### 1.交叉验证

#### 1.1 保留交叉验证(HoldOut）

- 定义

  **HoldOut Cross-validation（Train-Test Split）**

- 作用

  整个数据集被**随机**地划分为训练集和验证集。根据经验法则，整个数据集的近70%被用作训练集，其余30%被用作验证集。**也就是我们最常使用的，直接划分数据集的方法。**

- 优点

  很简单很容易执行。

- 缺点

  1.不适用于不平衡的数据集。

  ​		假设80%的数据属于 “0 “类，其余20%的数据属于 “1 “类。这种情况下，训练集的大小为80%，测试数据的大小为数据集的20%。可能发生的情况是，所有80%的 “0 “类数据都在训练集中，而所有 “1 “类数据都在测试集中。因此，我们的模型将不能很好地概括我们的测试数据，因为它之前没有见过 “1 “类的数据。

  2.一大块数据被剥夺了训练模型的机会。

  ​		在小数据集的情况下，有一部分数据将被保留下来用于测试模型，这些数据可能具有重要的特征，而我们的模型可能会因为没有在这些数据上进行训练而错过。

#### 1.2 K-折交叉验证(K-fold)

- 定义

  **（K-fold Cross Validation，记为K-CV或K-fold）**

- 作用

  K-Fold交叉验证技术中，整个数据集被划分为**K个大小相同的部分**。每个分区被称为 一个”Fold”。所以我们有K个部分，我们称之为K-Fold。**一个Fold被用作验证集，其余的K-1个Fold被用作训练集。**

  该技术重复K次，**直到每个Fold都被用作验证集，其余的作为训练集。**

  **模型的最终准确度是通过取k个模型验证数据的平均准确度来计算的。**

- 图示

  ![k_z](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/k_z.png)

#### 1.3分层k-折交叉验证(Stratified k-fold)

- 定义

  **Stratified k-fold cross validation**

- 作用

  K-折交叉验证的变种， 分层的意思是说在**每一折中都保持着原始数据中各个类别的比例关系**，比如说：原始数据有3类，比例为1:2:1，采用3折分层交叉验证，那么划分的3折中，每一折中的数据类别保持着1:2:1的比例，这样的验证结果更加可信。

- 图示

  ![k_z_z](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/k_z_z.png)

#### 1.4其他验证

- 去除p交叉验证)
- 留一交叉验证）
- 蒙特卡罗交叉验证
- 时间序列交叉验证

#### 1.5API

```python
from sklearn.model_selection import StratifiedKFold
kf=StratifiedKFold(n_splits=5,shuffle=True,random_state=44)
"""
    n_splits划分为几个折叠 
    shuffle是否在拆分之前被打乱(随机化),False则按照顺序拆分
    random_state随机因子

"""
indexs=kf.split(X,y) 
"""
	返回一个可迭代对象,一共有5个折叠,每个折叠对应的是训练集和测试集的下标

	然后可以用for循环取出每一个折叠对应的X和y下标来访问到对应的测试数据集和训练数据集 以及测试目标集和训练目标集
"""
for train_index, test_index in indexs:
X[train_index],y[train_index],X[test_index ],y[test_index]


#普通K折交叉验证和分层K折交叉验证的使用是一样的  只是引入的类不同
#使用时只是KFold这个类名不一样其他代码完全一样
from sklearn.model_selection import KFold
```

#### 1.6实例

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

#1.加载数据
iris=load_iris()
data=iris.data
target=iris.target
#2.初始化分层K-折交叉验证器
kf=StratifiedKFold(n_splits=5,shuffle=True,random_state=44)
#3.初始化KNN容器
knn=KNeighborsClassifier(n_neighbors=7)
#4.交叉验证
score=[]
for train_index,test_index in kf.split(data,target):
    x_train,x_test=data[train_index],data[test_index]
    y_train,y_test=target[train_index],target[test_index]
    #标准化
    stand=StandardScaler()
    x_train=stand.fit_transform(x_train)
    x_test=stand.transform(x_test)
    #模型训练
    knn.fit(x_train,y_train)
    #得到每次折叠的准确率
    score_num=knn.score(x_test,y_test)
    score.append(score_num)
print('平均化得分：',sum(score)/len(score))
"""
平均化得分： 0.9600000000000002
"""
```

### 2.超参数搜索

- 定义

  超参数搜索也叫网格搜索(Grid Search)

  比如在KNN算法中，k是一个可以人为设置的参数，所以就是一个超参数。网格搜索能自动的帮助我们找到最好的超参数值。

- 图示

  ![super](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/super.png)

### 3.超参数搜索API

```python
from sklearn.model_selection import GridSearchCV
gc=GridSearchCV(estimator，param_grid，cv)

说明：
同时进行交叉验证(CV)、和网格搜索(GridSearch)，GridSearchCV实计上也是一个估计器(estimator)，同时它有几个重要属性：
      best_params_  最佳参数
      best_score_ 在训练集中的准确率
      best_estimator_ 最佳估计器
      cv_results_ 交叉验证过程描述
      best_index_最佳k在列表中的下标
参数：
	estimator： scikit-learn估计器实例
	param_grid:以参数名称（str）作为键，将参数设置列表尝试作为值的字典
		示例： {"n_neighbors": [1, 3, 5, 7, 9, 11]}
    cv: 确定交叉验证切分策略,值为:
        (1)None  默认5折
        (2)integer  设置多少折
        如果估计器是分类器，使用"分层k-折交叉验证(StratifiedKFold)"。在所有其他情况下，使用KFold。
```

### 4.超参数搜索实例

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
#1.加载数据
iris=load_iris()
data=iris.data
target=iris.target
#2.划分数据
x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3,random_state=44,shuffle=True)
#3.标准化（数据预处理）
stand=StandardScaler()
x_train=stand.fit_transform(x_train)
x_test=stand.transform(x_test)
#4.初始化KNN容器
knn=KNeighborsClassifier()
#5.超参数搜索
gc=GridSearchCV(knn,{'n_neighbors':[1,2,3,4,5,6,7,8,9,10]},cv=10)
gc.fit(x_train,y_train)
#模型评估
y_predict=gc.predict(x_test)
# 方法1:对比预测值和真实值
print('模型预测结果：', y_predict == y_test)
# 方法2：计算准确率
score = gc.score(x_test, y_test)
print('模型预测率：', score)
# 最佳参数：best_params_
print("最佳参数：\n", gc.best_params_)
# 最佳结果：best_score_
print("在训练集中的准确率：\n", gc.best_score_)
# 最佳估计器：best_estimator_
print("最佳估计器:\n", gc.best_estimator_)
# 交叉验证结果：cv_results_
print("交叉验证过程描述:\n", gc.cv_results_)
# 最佳参数组合的索引:最佳k在列表中的下标
print("最佳参数组合的索引:\n", gc.best_index_)
```

![spuer_serach](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/spuer_serach.png)

## 朴素贝叶斯分类

### 1.贝叶斯分类理论

有一个数据集，它由两类数据组成，数据分布如下图所示：

![byes1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/byes1.png)

用p1(x,y)表示数据点(x,y)属于类别1(图中红色圆点表示的类别)的概率，用p2(x,y)表示数据点(x,y)属于类别2(图中蓝色三角形表示的类别)的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：

- 如果p1(x,y)>p2(x,y)，那么类别为1
- 如果p1(x,y)<p2(x,y)，那么类别为2

也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。

### 2.条件概率

- 定义

  P(A|B)：在事件B发生的情况下，事件A发生的概率

- 图示

  ![bayes5](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/bayes5.png)

  在事件B发生的情况下，事件A发生的概率就是P(A∩B)除以P(B)。

  𝑃(𝐴|𝐵)=𝑃(𝐴∩𝐵)/𝑃(𝐵)

- 推理

  已知：𝑃(𝐴|𝐵)=𝑃(𝐴∩𝐵)/𝑃(𝐵)

  则：𝑃(𝐴∩𝐵)=𝑃(𝐴|𝐵)𝑃(𝐵) 

  可推出：𝑃(𝐴∩𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)

  即：𝑃(𝐴|𝐵)=𝑃(B|A)𝑃(𝐴)/𝑃(𝐵)

### 3.全概率公式

除了条件概率以外，在计算p1和p2的时候，还要用到全概率公式

- 图示

  假定样本空间S，是两个事件A与A'的和：

  ![bayes2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/bayes2.png)

  在这种情况下，事件B可以划分成两个部分：

  ![bayes3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/bayes3.png)

  即：

  𝑃(𝐵)=𝑃(𝐵∩𝐴)+𝑃(𝐵∩𝐴′)

- 推理

  已知：𝑃(𝐵)=𝑃(𝐵∩𝐴)+𝑃(𝐵∩𝐴′)

  所以：𝑃(𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)+𝑃(𝐵|𝐴′)𝑃(𝐴′)

  将这个公式代入上一节的**条件概率公式**，就得到了条件概率的另一种写法：

  $P(A|B)=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A^,)P(A^,)}$

### 4.贝叶斯推断

条件概率公式进行变形：

![bye](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/bye.png)

**我们把P(A)称为"先验概率"（Prior probability）**，即在B事件发生之前，我们对A事件概率的一个判断。

**P(A|B)称为"后验概率"（Posterior probability）**，即在B事件发生之后，我们对A事件概率的重新评估。

**P(B|A)/P(B)称为"可能性函数"（Likelyhood）**，这是一个调整因子，使得预估概率更接近真实概率。

条件概率可以转换成已下形式：

**后验概率　＝　先验概率ｘ调整因子**

### 5.朴素贝叶斯推断

- 定义

  朴素贝叶斯相对于贝叶斯，对条件概率分布做了**条件独立**性的假设。

- 解释

  假设有n个特征，根据贝叶斯定理，后验概率 P(a|X)  可以表示为：

  $P(a|X) = \frac{P(X|a)P(a)}{P(X)}$

  其中：

  - P(X|a)  是给定类别 \( a \) 下观测到特征向量  $X=(x_1, x_2, ..., x_n) $的概率；
  - P(a) 是类别  a  的先验概率；
  - P(X) 是观测到特征向量 X 的边缘概率，通常作为归一化常数处理。

  朴素贝叶斯分类器的关键假设是特征之间的条件独立性，即给定类别 a ，特征  $x_i$  和 $x_j$  (其中 $i \neq j$ 相互独立。)

  因此，我们可以将联合概率 P(X|a)  分解为各个特征的概率乘积：


  $P(X|a) = P(x_1, x_2, ..., x_n|a) = P(x_1|a)P(x_2|a)...P(x_n|a)$


  将这个条件独立性假设应用于贝叶斯公式，我们得到：


  $P(a|X) = \frac{P(x_1|a)P(x_2|a)...P(x_n|a)P(a)}{P(X)}$


  这样，朴素贝叶斯分类器就可以通过计算每种可能类别的条件概率和先验概率，然后选择具有最高概率的类别作为预测结果。

- 实例

  |      | 纹理 | 色泽 | 鼔声 | 类别 |
  | ---- | ---- | ---- | ---- | ---- |
  | 1    | 清晰 | 清绿 | 清脆 | 好瓜 |
  | 2    | 模糊 | 乌黑 | 浊响 | 坏瓜 |
  | 3    | 模糊 | 清绿 | 浊响 | 坏瓜 |
  | 4    | 清晰 | 乌黑 | 沉闷 | 好瓜 |
  | 5    | 清晰 | 清绿 | 浊响 | 好瓜 |
  | 6    | 模糊 | 乌黑 | 沉闷 | 坏瓜 |
  | 7    | 清晰 | 乌黑 | 清脆 | 好瓜 |
  | 8    | 模糊 | 清绿 | 沉闷 | 好瓜 |
  | 9    | 清晰 | 乌黑 | 浊响 | 坏瓜 |
  | 10   | 模糊 | 清绿 | 清脆 | 好瓜 |
  | 11   | 清晰 | 清绿 | 沉闷 | ？   |
  | 12   | 模糊 | 乌黑 | 浊响 | ?    |

  ```python
  公式：
      p(a|X) = p(X|a)* p(a)/p(X)
      p(X|a) = p(x1,x2,x3...xn|a) = p(x1|a)*p(x2|a)*p(x3|a)...p(xn|a)
      p(X) = p(x1,x2,x3...xn) = p(x1)*p(x2)*p(x3)...p(xn)
      p(a|X) = p(x1|a)*p(x2|a)*p(x3|a)...p(xn|a) * p(a) / p(x1)*p(x2)*p(x3)...p(xn)
  11号瓜示例：
      p(好瓜|纹理清晰，色泽清绿，鼓声沉闷)
          =p(纹理清晰，色泽清绿，鼓声沉闷|好瓜)*p(好瓜)/p(纹理清晰，色泽清绿，鼓声沉闷)
          =p(好瓜)】*p(纹理清晰|好瓜)*p(色泽清绿|好瓜)*p(鼓声沉闷|好瓜)/p(纹理清晰)*p(色泽清绿)*p(鼓声沉闷)
  
      p(坏瓜|纹理清晰，色泽清绿，鼓声沉闷)
          =p(坏瓜)*p(纹理清晰|坏瓜)*p(色泽清绿|坏瓜)*p(鼓声沉闷|坏瓜)/p(纹理清晰)*p(色泽清绿)*p(鼓声沉闷)
  
      从公式中判断"p(好瓜|纹理清晰，色泽清绿，鼓声沉闷)"和"p(坏瓜|纹理清晰，色泽清绿，鼓声沉闷)"时，因为它们的分母值是相同的，值都是p(纹理清晰)*p(色泽清绿)*p(鼓声沉闷)，所以只要计算它们的分子就可以判断是"好瓜"还是"坏瓜"之间谁大谁小了，所以没有必要计算分母
      p(好瓜) = 6/10
      p(坏瓜)=4/10
      p（纹理清晰|好瓜） = 4/6 
      p（色泽清绿|好瓜） = 4/6
      p（鼓声沉闷|好瓜） = 2/6
      p（纹理清晰|坏瓜） = 1/4 
      p（色泽清绿|坏瓜） = 1/4
      p（鼓声沉闷|坏瓜） = 1/4
      把以上计算代入公式的分子
      p(好瓜)*p（纹理清晰|好瓜）*p（色泽清绿|好瓜）*p（鼓声沉闷|好瓜) = 4/45
      p(坏瓜)*p（纹理清晰|坏瓜）*p（色泽清绿|坏瓜）*p（鼓声沉闷|坏瓜) = 1/160
      所以
      p(好瓜|纹理清晰，色泽清绿，鼓声沉闷) >  p(坏瓜|纹理清晰，色泽清绿，鼓声沉闷)，
      所以把(纹理清晰，色泽清绿，鼓声沉闷)的样本归类为好瓜
      
      
      
  
  12号瓜测算：
  	#分母相同不算
  	p(好瓜/(纹理模糊、色泽乌黑、鼓声浊响))
      	=p((纹理模糊、色泽乌黑、鼓声浊响)/好瓜)*p(好瓜)/p(纹理模糊、色泽乌黑、鼓声浊响)
          =p(纹理模糊/好瓜)*p(色泽乌黑/好瓜)*p(鼓声浊响/好瓜)*p(好瓜)/p(纹理模糊)*p(色泽乌黑)*p(鼓声浊响)
          =(2/6 * 2/6 * 1/6 * 6/10)
          =(1/90)
          
          
          
  	p(坏瓜/(纹理模糊、色泽乌黑、鼓声浊响))
      	=p((纹理模糊、色泽乌黑、鼓声浊响)/坏瓜)*p(好瓜)/p(纹理模糊、色泽乌黑、鼓声浊响)
  		=p(纹理模糊/坏瓜)*p(色泽乌黑/坏瓜)*p(鼓声浊响/坏瓜)*p(坏瓜)/p(纹理模糊)*p(色泽乌黑)*p(鼓声浊响)
  		=(3/4 * 3/4 * 3/4 * 4/10)
          =27/160
  	
      p(坏瓜/(纹理模糊、色泽乌黑、鼓声浊响))>p(好瓜/(纹理模糊、色泽乌黑、鼓声浊响))
      
      所以12号瓜归于坏瓜
  
  
  ```

### 6.拉普拉斯平滑系数

- 定义

  某些事件或特征可能从未出现过，这会导致它们的概率被估计为零。然而，在实际应用中，即使某个事件或特征没有出现在训练集中，也不能完全排除它在未来样本中出现的可能性。拉普拉斯平滑技术可以避免这种“零概率陷阱”。

- 公式

  ![lapulas](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/lapulas.png)

  **一般α取值1，m的值为总特征数量**

- 作用

  通过这种方法，**即使某个特征在训练集中从未出现过**，它的概率也不会被估计为零，**而是会被赋予一个很小但非零的值**，从而避免了模型在面对新数据时可能出现的过拟合或预测错误

- 示例

  ```python
  """
  比如计算判断新瓜(纹理清晰，色泽淡白，鼓声沉闷)是好和坏时,因为在样本中色泽淡白没有出现,导致出现0值,会影响计算结果,要采用拉普拉斯平滑系数
  """
  p(好瓜|纹理清晰，色泽淡白，鼓声沉闷)
  	=【p(好瓜)】*【p（纹理清晰|好瓜）*p（色泽淡白|好瓜）*p（鼓声沉闷|好瓜)】/【p（纹理清晰）*p（色泽淡白）*p（鼓声沉闷)】
  p(坏瓜|纹理清晰，色泽淡白，鼓声沉闷)
  	=【p(坏瓜)】*【p（纹理清晰|坏瓜）*p（色泽淡白|坏瓜）*p（鼓声沉闷|坏瓜)】/【p（纹理清晰）*p（色泽淡白）*p（鼓声沉闷)】   
  p（纹理清晰|好瓜）= (4+1)/(6+3)  # +1是因为防止零概率 +3是因为有3个特征(纹理,色泽,鼓声)
  p（色泽淡白|好瓜）= (0+1)/(6+3)  
  p（鼓声沉闷|好瓜) = (2+1)/(6+3) 
  p（纹理清晰|坏瓜）= (1+1)/(4+3)   
  p（色泽淡白|坏瓜）= (0+1)/(4+3)  
  p（鼓声沉闷|坏瓜) = (1+1)/(4+3)  
  ```

### 7.API

```python
from sklearn.naive_bayes import MultinomialNB
by=MultinomialNB()
by.fit(x_train, y_train)
y_predict = by.predict(x_test)
```

### 8.实例

- 朴素贝叶斯分类—鸢尾花

  ```python
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import MinMaxScaler
  from sklearn.naive_bayes import MultinomialNB
  
  #加载数据
  iris=load_iris()
  data=iris.data
  target=iris.target
  #划分数据
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,shuffle=True,random_state=44)
  #预处理数据
  minmax=MinMaxScaler()
  x_train=minmax.fit_transform(x_train)
  x_test=minmax.transform(x_test)
  #初始化朴素贝叶斯容器
  biys=MultinomialNB()
  biys.fit(x_train,y_train)
  #模型评估
  score=biys.score(x_test,y_test)
  print('模型准确率：',score)
  #数据预测
  y_predict=biys.predict([[1,4,5,1]])
  print('预测结果为：',iris.target_names[y_predict])
  ```

  ![biys_isir](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/biys_isir.png)

- 朴素贝叶斯分类—数字

  ```python
  from sklearn.datasets import load_digits
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import MinMaxScaler
  from sklearn.naive_bayes import MultinomialNB
  
  #加载数据
  digit=load_digits()
  data=digit.data
  target=digit.target
  #划分数据
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,shuffle=True,random_state=44)
  #预处理数据
  minmax=MinMaxScaler()
  x_train=minmax.fit_transform(x_train)
  x_test=minmax.transform(x_test)
  #初始化朴素贝叶斯容器
  biys=MultinomialNB()
  biys.fit(x_train,y_train)
  #模型评估
  score=biys.score(x_test,y_test)
  print('模型准确率：',score)
  #数据预测
  y_predict=biys.predict([[x for x in range(64)]])
  print('预测结果为：',digit.target_names[y_predict])
  ```

  ![biys_digit](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/biys_digit.png)

- 朴素贝叶斯分类—葡萄酒

  ```python
  from sklearn.datasets import load_wine
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import MinMaxScaler
  from sklearn.naive_bayes import MultinomialNB
  
  #加载数据
  wine=load_wine()
  data=wine.data
  target=wine.target
  #划分数据
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,shuffle=True,random_state=44)
  #预处理数据
  minmax=MinMaxScaler()
  x_train=minmax.fit_transform(x_train)
  x_test=minmax.transform(x_test)
  #初始化朴素贝叶斯容器
  biys=MultinomialNB()
  biys.fit(x_train,y_train)
  #模型评估
  score=biys.score(x_test,y_test)
  print('模型准确率：',score)
  #数据预测
  y_predict=biys.predict([[x for x in range(13)]])
  print('预测结果为：',wine.target_names[y_predict])
  ```

  ![biys_wine](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/biys_wine.png)

## 决策树分类

### 1.决策树概念

- 定义

  1、决策节点
  通过条件判断而进行分支选择的节点。如：将某个样本中的属性值(特征值)与决策节点上的值进行比较，从而判断它的流向。

  2、叶子节点
  没有子节点的节点，表示最终的决策结果。

  3、决策树的深度
  所有节点的最大层次数。

  决策树具有一定的层次结构，根节点的层次数定为0，从下面开始每一层子节点层次数增加

  4、决策树优点：

  ​      可视化 - 可解释能力-对算力要求低

   5、 决策树缺点：

  ​      容易产生过拟合，所以不要把深度调整太大了。

- 示例图示

  ![tree_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_1.png)

  ![tree_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_2.png)

  |       | 是动物 | 会飞 | 有羽毛 |
  | ----- | ------ | ---- | ------ |
  | 1麻雀 | 1      | 1    | 1      |
  | 2蝙蝠 | 1      | 1    | 0      |
  | 3飞机 | 0      | 1    | 0      |
  | 4熊猫 | 1      | 0    | 0      |

   是否为动物

  |       | 是动物 | 会飞 | 有羽毛 |
  | ----- | ------ | ---- | ------ |
  | 1麻雀 | 1      | 1    | 1      |
  | 2蝙蝠 | 1      | 1    | 0      |
  | 4熊猫 | 1      | 0    | 0      |

  是否会飞

  |       | 是动物 | 会飞 | 有羽毛 |
  | ----- | ------ | ---- | ------ |
  | 1麻雀 | 1      | 1    | 1      |
  | 2蝙蝠 | 1      | 1    | 0      |

  是否有羽毛

  |       | 是动物 | 会飞 | 有羽毛 |
  | ----- | ------ | ---- | ------ |
  | 1麻雀 | 1      | 1    | 1      |

### 2.基于信息增益决策树的建立

#### 2.1信息熵

信息熵描述的是不确定性。**信息熵越大，不确定性越大**。**信息熵的值越小，则D的纯度越高。**

假设样本集合D共有N类，第k类样本所占比例为$P_k$，则D的信息熵为:![tree_num](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_num.png)

#### 2.2信息增益

信息增益是一个统计量，用来描述一个属性区分数据样本的能力。**信息增益越大，那么决策树就会越简洁。**

信息增益公式：![tree_num2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_num2.png)

#### 2.3通过信息增益建立决策树

- 示例

  |      | 职业 | 年龄 | 收入  | 学历 | 是否贷款 |
  | ---- | ---- | ---- | ----- | ---- | -------- |
  | 1    | 工人 | 36   | 5500  | 高中 | 否       |
  | 2    | 工人 | 42   | 2800  | 初中 | 是       |
  | 3    | 白领 | 45   | 3300  | 小学 | 是       |
  | 4    | 白领 | 25   | 10000 | 本科 | 是       |
  | 5    | 白领 | 32   | 8000  | 硕士 | 否       |
  | 6    | 白领 | 28   | 13000 | 博士 | 是       |

- 建立流程

  1.**计算根节点的信息熵**

  上表根据是否贷款把样本分成2类样本，"是"占4/6=2/3, "否"占2/6=1/3

  ![tree_bulid_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_bulid_1.png)

  2.**计算属性的信息增益**

  <1> "职业"属性的信息增益:IG(D,"职业")

  ​		在职业中，工人占1/3,  工人中，是否代款各占1/2

  ![tree_bulid_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_bulid_2.png)

  ​		在职业中，白领占2/3,  白领中，是贷款占3/4, 不贷款占1/4

  ![tree_bulid_3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_bulid_3.png)

  ​		所以职业信息增益：

  ![tree_bulid_4](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_bulid_4.png)

  ![tree_bulid_5](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_bulid_5.png)

  <2>" 年龄"属性的信息增益（以35岁为界）

  ![tree_bulid_6](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_bulid_6.png)

  <3> "收入"属性的信息增益（以10000为界,大于等于10000为一类）

  ![tree_bulid_7](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_bulid_7.png)

  <4> "学历"属性的信息增益（以高中为界, 大于等于高中的为一类）

  ![tree_bulid_8](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_bulid_8.png)

  3.**划分属性**

  对比属性信息增益发现，"收入"和"学历"相等，并且是最高的，所以我们就可以选择"学历"或"收入"作为第一个

  ![tree_show](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tree_show.png)

### 3.基于基尼指数决策树的建立（了解）

- 定义

  **基尼指数**（Gini Index）是决策树算法中用于评估数据集纯度的一种度量，**基尼指数衡量的是数据集的不纯度，或者说分类的不确定性。**在构建决策树时，基尼指数被用来决定如何对数据集进行最优划分，以减少不纯度。

- 计算

  对于一个二分类问题，如果一个节点包含的样本属于正类的概率是 \(p\)，则属于负类的概率是 \(1-p\)。那么，这个节点的基尼指数 \(Gini(p)\) 定义为：

   $Gini(p) = 1 - p^2 - (1-p)^2 = 2p(1-p) $

  对于多分类问题，如果一个节点包含的样本属于第 k 类的概率是 $p_k$，则节点的基尼指数定义为：

  ![](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/Gini_num1.png)

- 作用

  - 当一个节点的所有样本都属于同一类别时，**基尼指数为 0，表示纯度最高。**
  - 当一个节点的样本均匀分布在所有类别时，**基尼指数最大，表示纯度最低。**

- 流程

  一个数据集 \(D\)，其中包含 \(N\) 个样本，特征 \(A\) 将数据集分割为 $|D_1|$和 $|D_2|$ ，则特征 \(A\) 的基尼指数为：

 ![](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/Gini_num2.png)

  其中 $|D_1|$和 $|D_2|$ 分别是子集 $D_1$ 和 $D_2$ 中的样本数量。

  通过这样的方式，决策树算法逐步构建一棵树，每一层的节点都尽可能地减少基尼指数，最终达到对数据集的有效分类。

- 示例

  ![gini_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_1.png)

  ![gini_1_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_1_1.png)

  工资有两个取值，分别是0和1。当工资=1时，有3个样本：

  ![gini_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_2.png)

  在这三个样本中，工作都是好：

  ![gini_3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_3.png)

  所以工资=1，最终结果：

  ![gini_4](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_4.png)

  同理，当工资=0时，有5个样本，在这五个样本中，工作有3个是不好，2个是好：

  ![gini_5](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_5.png)

  同理，可得压力的基尼指数如下：

  ![gini_6](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_6.png)

  平台的基尼指数如下：

  ![gini_7](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_7.png)

  **平台有三个取值0,1,2。所以在计算时，需要将平台的每一个取值都单独进行计算。**比如：当平台=0时，将数据集分为两部分，**第一部分是平台=0，第二部分是平台!=0(分母是5的原因)。**

  **根据基尼指数最小准则，我们优先选择工资或者平台=0作为D的第一特征。**

  我们选择工资作为第一特征，**那么当工资=1时，工作=好，无需继续划分。当工资=0时，需要继续划分。**

  当工资=0时，继续计算基尼指数：

  ![gini_8](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/gini_8.png)

  当平台=0时，基尼指数=0，可以优先选择。

  同时，当平台=0时，工作都是好，无需继续划分，当平台=1,2时，工作都是不好，也无需继续划分。直接把1,2放到树的一个结点就可以。


### 4.API

```python
from sklearn.tree import DecisionTreeClassifier
tree=DecisionTreeClassifier(criterion,max_depth)
"""
criterion="gini"或"entropy” 默认为="gini" 
	当criterion取值为"gini"时采用 基尼不纯度（Gini impurity）算法构造决策树，
	当criterion取值为"entropy”时采用信息增益（ information gain）算法构造决策树.
	
max_depth=int, 默认为=None  树的最大深度

"""

#可视化决策树
from sklearn.tree import export_graphviz
graph=export_graphviz(decision_tree,out_file,feature_names)
"""
	estimator决策树预估器
	out_file生成的文档
    feature_names节点特征属性名
 	把生成的文档打开，复制出内容粘到"http://webgraphviz.com/"中，点击"generate Graph"会生成一个树型的决策树图
"""
```

### 5.实例

- 决策树-鸢尾花

  ```python
  #加载数据
  iris=load_iris()
  data=iris.data
  target=iris.target
  #划分数据
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,shuffle=True,random_state=44)
  #预处理数据
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)
  #初始化决策树容器
  tree_gini=DecisionTreeClassifier(criterion='gini')
  tree_info=DecisionTreeClassifier(criterion='entropy')
  tree_gini.fit(x_train,y_train)#基尼指数决策树
  tree_info.fit(x_train,y_train)#信息增益决策树
  #模型评估
  score_gini=tree_gini.score(x_test,y_test)
  score_info=tree_info.score(x_test,y_test)
  print('基尼指数决策树准确率：',score_gini)
  print('信息增益决策树准确率：',score_info)
  #预测数据
  y_pre_gini=tree_gini.predict([[1,2,3,4]])
  y_pre_info=tree_info.predict([[1,2,3,4]])
  print('基尼指数决策树预测类型为：',iris.target_names[y_pre_gini])
  print('信息增益决策树预测类型为：',iris.target_names[y_pre_info])
  
  graph_gini=export_graphviz(tree_gini,'./iris_tree_gini',feature_names=iris.feature_names)
  graph_info=export_graphviz(tree_info,'./iris_tree_info',feature_names=iris.feature_names)
  ```

  ![decision_tree](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/decision_tree.png)

  ![iris_show](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/iris_show.png)

- 决策树-葡萄酒

  ```python
  from sklearn.datasets import load_wine
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.tree import export_graphviz
  
  
  #加载数据
  wine=load_wine()
  data=wine.data
  target=wine.target
  #划分数据
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,shuffle=True,random_state=24)
  #预处理数据
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)
  #初始化决策树容器
  tree_gini=DecisionTreeClassifier(criterion='gini')
  tree_info=DecisionTreeClassifier(criterion='entropy')
  tree_gini.fit(x_train,y_train)#基尼指数决策树
  tree_info.fit(x_train,y_train)#信息增益决策树
  #模型评估
  score_gini=tree_gini.score(x_test,y_test)
  score_info=tree_info.score(x_test,y_test)
  print('基尼指数决策树准确率：',score_gini)
  print('信息增益决策树准确率：',score_info)
  #预测数据
  y_pre_gini=tree_gini.predict([[x for x in range(13)]])
  y_pre_info=tree_info.predict([[x for x in range(13)]])
  print('基尼指数决策树预测类型为：',wine.target_names[y_pre_gini])
  print('信息增益决策树预测类型为：',wine.target_names[y_pre_info])
  
  graph_gini=export_graphviz(tree_gini,'./iris_tree_gini',feature_names=wine.feature_names)
  graph_info=export_graphviz(tree_info,'./iris_tree_info',feature_names=wine.feature_names)
  ```

  ![decision_tree_wine](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/decision_tree_wine.png)

  ![wine_show](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/wine_show.png)

- 坦尼克号乘客生存

  ```python
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.feature_extraction import DictVectorizer
  from sklearn.tree import DecisionTreeClassifier, export_graphviz
  # 1、获取数据
  titanic = pd.read_csv("src/titanic/titanic.csv")
  titanic.head()
  # 筛选特征值和目标值
  x = titanic[["pclass", "age", "sex"]]
  y = titanic["survived"]
  
  #2、数据处理
  # 1）缺失值处理， 因为其中age有缺失值。
  x["age"].fillna(x["age"].mean(), inplace=True)
  
  # 2) 转换成字典， 因为其中数据必须为数字才能进行决策树，所在先转成字典，后面又字典特征抽取，这样之后的数据就会是数字了, 鸢尾花的数据本来就全部是数字，所以不需要这一步。
  """
  x.to_dict(orient="records") 这个方法通常用于 Pandas DataFrame 对象，用来将 DataFrame 转换为一个列表，其中列表的每一个元素是一个字典，对应于 DataFrame 中的一行记录。字典的键是 DataFrame 的列名，值则是该行中对应的列值。
  假设你有一个如下所示的 DataFrame x:
     A  B  C
  0  1  4  7
  1  2  5  8
  2  3  6  9
  执行 x.to_dict(orient="records")，你会得到这样的输出：
  [
      {'A': 1, 'B': 4, 'C': 7},
      {'A': 2, 'B': 5, 'C': 8},
      {'A': 3, 'B': 6, 'C': 9}
  ]
  """
  x = x.to_dict(orient="records")
  # 3)、数据集划分
  x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=22)
  # 4)、字典特征抽取
  transfer = DictVectorizer()
  x_train = transfer.fit_transform(x_train) #稀疏矩阵
  x_test = transfer.transform(x_test)
  
  # 3）决策树预估器
  estimator = DecisionTreeClassifier(criterion="entropy", max_depth=8)
  estimator.fit(x_train, y_train)
  
  # 4）模型评估
  # 方法1：直接比对真实值和预测值
  y_predict = estimator.predict(x_test)
  print("y_predict:\n", y_predict)
  print("直接比对真实值和预测值:\n", y_test == y_predict)
  
  # 方法2：计算准确率
  score = estimator.score(x_test, y_test)
  print("准确率为：\n", score)
  
  
  # 6）预测
  x_test = transfer.transform([{'pclass': '1rd', 'age': 22.0, 'sex': 'female'}])
  index=estimator.predict(x_test)
  print("预测1:\n",index)#[1]  头等舱的就可以活下来
  x_test = transfer.transform([{'pclass': '3rd', 'age': 22.0, 'sex': 'female'}])
  index=estimator.predict(x_test)
  print("预测2:\n",index)#[0]  3等舱的活不下来
  
  # 可视化决策树
  export_graphviz(estimator, out_file="titanic_tree.dot", feature_names=transfer.get_feature_names_out())
  ```

  
