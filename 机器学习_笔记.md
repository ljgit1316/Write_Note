
# æœºå™¨å­¦ä¹ 

## æœºå™¨å­¦ä¹ åˆ†ç±»

### 1.ç›‘ç£å­¦ä¹ 

ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰æ˜¯**ä»æœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ æ¨¡å‹**ï¼Œ**ç„¶åå¯¹æŸä¸ªç»™å®šçš„æ–°æ•°æ®åˆ©ç”¨æ¨¡å‹é¢„æµ‹å®ƒçš„æ ‡ç­¾**ã€‚å¦‚æœåˆ†ç±»æ ‡ç­¾ç²¾ç¡®åº¦è¶Šé«˜ï¼Œåˆ™å­¦ä¹ æ¨¡å‹å‡†ç¡®åº¦è¶Šé«˜ï¼Œé¢„æµ‹ç»“æœè¶Šç²¾ç¡®ã€‚

ç›‘ç£å­¦ä¹ ä¸»è¦ç”¨äº**å›å½’**å’Œ**åˆ†ç±»**ã€‚

å¸¸è§çš„ç›‘ç£å­¦ä¹ çš„**å›å½’ç®—æ³•æœ‰çº¿æ€§å›å½’ã€å›å½’æ ‘ã€Ké‚»è¿‘ã€Adaboostã€ç¥ç»ç½‘ç»œç­‰ã€‚**

å¸¸è§çš„ç›‘ç£å­¦ä¹ çš„**åˆ†ç±»ç®—æ³•æœ‰æœ´ç´ è´å¶æ–¯ã€å†³ç­–æ ‘ã€SVMã€é€»è¾‘å›å½’ã€Ké‚»è¿‘ã€Adaboostã€ç¥ç»ç½‘ç»œç­‰ã€‚**

### 2.åŠç›‘ç£å­¦ä¹ 

åŠç›‘ç£å­¦ä¹ ï¼ˆSemi-Supervised Learningï¼‰æ˜¯**åˆ©ç”¨å°‘é‡æ ‡æ³¨æ•°æ®å’Œå¤§é‡æ— æ ‡æ³¨æ•°æ®è¿›è¡Œå­¦ä¹ çš„æ¨¡å¼ã€‚**

åŠç›‘ç£å­¦ä¹ **ä¾§é‡äºåœ¨æœ‰ç›‘ç£çš„åˆ†ç±»ç®—æ³•ä¸­åŠ å…¥æ— æ ‡è®°æ ·æœ¬æ¥å®ç°åŠç›‘ç£åˆ†ç±»ã€‚**

å¸¸è§çš„åŠç›‘ç£å­¦ä¹ ç®—æ³•æœ‰Pseudo-Labelã€Î -Modelã€Temporal Ensemblingã€Mean Teacherã€VATã€UDAã€MixMatchã€ReMixMatchã€FixMatchç­‰ã€‚

### 3.æ— ç›‘ç£å­¦ä¹ 

æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰æ˜¯**ä»æœªæ ‡æ³¨æ•°æ®ä¸­å¯»æ‰¾éšå«ç»“æ„çš„è¿‡ç¨‹ã€‚**

æ— ç›‘ç£å­¦ä¹ **ä¸»è¦ç”¨äºå…³è”åˆ†æã€èšç±»å’Œé™ç»´ã€‚**

å¸¸è§çš„æ— ç›‘ç£å­¦ä¹ ç®—æ³•æœ‰ç¨€ç–è‡ªç¼–ç ï¼ˆSparse Auto-Encoderï¼‰ã€ä¸»æˆåˆ†åˆ†æï¼ˆPrincipal Component Analysis, PCAï¼‰ã€K-Meansç®—æ³•ï¼ˆKå‡å€¼ç®—æ³•ï¼‰ã€DBSCANç®—æ³•ï¼ˆDensity-Based Spatial Clustering of Applications with Noiseï¼‰ã€æœ€å¤§æœŸæœ›ç®—æ³•ï¼ˆExpectation-Maximization algorithm, EMï¼‰ç­‰ã€‚

### 4.å¼ºåŒ–å­¦ä¹ 

å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰**ç±»ä¼¼äºç›‘ç£å­¦ä¹ ï¼Œä½†æœªä½¿ç”¨æ ·æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ˜¯æ˜¯é€šè¿‡ä¸æ–­è¯•é”™è¿›è¡Œå­¦ä¹ çš„æ¨¡å¼ã€‚**

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæœ‰ä¸¤ä¸ªå¯ä»¥è¿›è¡Œäº¤äº’çš„å¯¹è±¡ï¼šæ™ºèƒ½ä½“ï¼ˆAgnetï¼‰å’Œç¯å¢ƒï¼ˆEnvironmentï¼‰ï¼Œè¿˜æœ‰å››ä¸ªæ ¸å¿ƒè¦ç´ ï¼šç­–ç•¥ï¼ˆPolicyï¼‰ã€å›æŠ¥å‡½æ•°ï¼ˆæ”¶ç›Šä¿¡å·ï¼ŒReward Functionï¼‰ã€ä»·å€¼å‡½æ•°ï¼ˆValue Functionï¼‰å’Œç¯å¢ƒæ¨¡å‹ï¼ˆEnvironment Modelï¼‰ï¼Œå…¶ä¸­ç¯å¢ƒæ¨¡å‹æ˜¯å¯é€‰çš„ã€‚

## æœºå™¨å­¦ä¹ åº”ç”¨åœºåˆ

### 1.è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰

è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦é¢†åŸŸä¹‹ä¸€ï¼Œæ¶‰åŠè®¡ç®—æœºä¸äººç±»è‡ªç„¶è¯­è¨€çš„äº¤äº’ã€‚NLPæŠ€æœ¯å¯ä»¥å®ç°è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ†æã€æƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ï¼Œä¸ºæ™ºèƒ½å®¢æœã€èŠå¤©æœºå™¨äººã€è¯­éŸ³åŠ©æ‰‹ç­‰æä¾›æ”¯æŒã€‚

### 2.åŒ»ç–—è¯Šæ–­ä¸å½±åƒåˆ†æ

æœºå™¨å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—å›¾åƒåˆ†æã€ç–¾ç—…é¢„æµ‹ã€è¯ç‰©å‘ç°ç­‰ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—å½±åƒè¯Šæ–­ä¸­çš„è¡¨ç°å¼•äººæ³¨ç›®ã€‚

### 3.é‡‘èé£é™©ç®¡ç†

æœºå™¨å­¦ä¹ åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šé‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨é£é™©ç®¡ç†æ–¹é¢ã€‚æ¨¡å‹å¯ä»¥åˆ†æå¤§é‡çš„é‡‘èæ•°æ®ï¼Œé¢„æµ‹å¸‚åœºæ³¢åŠ¨æ€§ã€ä¿¡ç”¨é£é™©ç­‰ã€‚

### 4.é¢„æµ‹ä¸æ¨èç³»ç»Ÿ

æœºå™¨å­¦ä¹ åœ¨é¢„æµ‹å’Œæ¨èç³»ç»Ÿä¸­ä¹Ÿæœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå¦‚é”€å”®é¢„æµ‹ã€ä¸ªæ€§åŒ–æ¨èç­‰ã€‚ååŒè¿‡æ»¤å’ŒåŸºäºå†…å®¹çš„æ¨èæ˜¯å¸¸ç”¨çš„æŠ€æœ¯ã€‚

### 5.åˆ¶é€ ä¸šå’Œç‰©è”ç½‘

ç‰©è”ç½‘ï¼ˆIoTï¼‰åœ¨åˆ¶é€ ä¸šä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œæœºå™¨å­¦ä¹ å¯ç”¨äºå¤„ç†å’Œåˆ†æä¼ æ„Ÿå™¨æ•°æ®ï¼Œå®ç°è®¾å¤‡é¢„æµ‹æ€§ç»´æŠ¤å’Œè´¨é‡æ§åˆ¶ã€‚

### 6.èƒ½æºç®¡ç†ä¸ç¯å¢ƒä¿æŠ¤

æœºå™¨å­¦ä¹ å¯ä»¥å¸®åŠ©ä¼˜åŒ–èƒ½æºç®¡ç†ï¼Œå‡å°‘èƒ½æºæµªè´¹ï¼Œæé«˜èƒ½æºåˆ©ç”¨æ•ˆç‡ã€‚é€šè¿‡åˆ†æå¤§é‡çš„èƒ½æºæ•°æ®ï¼Œè¯†åˆ«ä¼˜åŒ–çš„æœºä¼šã€‚

### 7.å†³ç­–æ”¯æŒä¸æ™ºèƒ½åˆ†æ

æœºå™¨å­¦ä¹ åœ¨å†³ç­–æ”¯æŒç³»ç»Ÿä¸­çš„åº”ç”¨ä¹Ÿååˆ†é‡è¦ï¼Œå¯ä»¥å¸®åŠ©åˆ†æå¤§é‡æ•°æ®ï¼Œè¾…åŠ©å†³ç­–åˆ¶å®šã€‚åŸºäºæ•°æ®çš„å†³ç­–å¯ä»¥æ›´åŠ å‡†ç¡®å’Œæœ‰æ®å¯ä¾ã€‚

### 8.å›¾åƒè¯†åˆ«ä¸è®¡ç®—æœºè§†è§‰

å›¾åƒè¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰æ˜¯å¦ä¸€ä¸ªé‡è¦çš„æœºå™¨å­¦ä¹ åº”ç”¨é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œè§£é‡Šå›¾åƒã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

## æœºå™¨å­¦ä¹ é¡¹ç›®å¼€å‘æ­¥éª¤

5ä¸ªåŸºæœ¬æ­¥éª¤ç”¨äºæ‰§è¡Œæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼š

1. **æ”¶é›†æ•°æ®**ï¼šæ— è®ºæ˜¯æ¥è‡ªexcelï¼Œaccessï¼Œæ–‡æœ¬æ–‡ä»¶ç­‰çš„åŸå§‹æ•°æ®ï¼Œè¿™ä¸€æ­¥ï¼ˆæ”¶é›†è¿‡å»çš„æ•°æ®ï¼‰æ„æˆäº†æœªæ¥å­¦ä¹ çš„åŸºç¡€ã€‚ç›¸å…³æ•°æ®çš„ç§ç±»ï¼Œå¯†åº¦å’Œæ•°é‡è¶Šå¤šï¼Œæœºå™¨çš„å­¦ä¹ å‰æ™¯å°±è¶Šå¥½ã€‚
2. **å‡†å¤‡æ•°æ®**ï¼šä»»ä½•åˆ†æè¿‡ç¨‹éƒ½ä¼šä¾èµ–äºä½¿ç”¨çš„æ•°æ®è´¨é‡å¦‚ä½•ã€‚äººä»¬éœ€è¦èŠ±æ—¶é—´ç¡®å®šæ•°æ®è´¨é‡ï¼Œç„¶åé‡‡å–æªæ–½è§£å†³è¯¸å¦‚ç¼ºå¤±çš„æ•°æ®å’Œå¼‚å¸¸å€¼çš„å¤„ç†ç­‰é—®é¢˜ã€‚æ¢ç´¢æ€§åˆ†æå¯èƒ½æ˜¯ä¸€ç§è¯¦ç»†ç ”ç©¶æ•°æ®ç»†å¾®å·®åˆ«çš„æ–¹æ³•ï¼Œä»è€Œä½¿æ•°æ®çš„è´¨é‡è¿…é€Ÿæé«˜ã€‚
3. **ç»ƒæ¨¡å‹**ï¼šæ­¤æ­¥éª¤æ¶‰åŠä»¥æ¨¡å‹çš„å½¢å¼é€‰æ‹©é€‚å½“çš„ç®—æ³•å’Œæ•°æ®è¡¨ç¤ºã€‚æ¸…ç†åçš„æ•°æ®åˆ†ä¸ºä¸¤éƒ¨åˆ† - è®­ç»ƒå’Œæµ‹è¯•ï¼ˆæ¯”ä¾‹è§†å‰æç¡®å®šï¼‰; ç¬¬ä¸€éƒ¨åˆ†ï¼ˆè®­ç»ƒæ•°æ®ï¼‰ç”¨äºå¼€å‘æ¨¡å‹ã€‚ç¬¬äºŒéƒ¨åˆ†ï¼ˆæµ‹è¯•æ•°æ®ï¼‰ç”¨ä½œå‚è€ƒä¾æ®ã€‚
4. **è¯„ä¼°æ¨¡å‹**ï¼šä¸ºäº†æµ‹è¯•å‡†ç¡®æ€§ï¼Œä½¿ç”¨æ•°æ®çš„ç¬¬äºŒéƒ¨åˆ†ï¼ˆä¿æŒ/æµ‹è¯•æ•°æ®ï¼‰ã€‚æ­¤æ­¥éª¤æ ¹æ®ç»“æœç¡®å®šç®—æ³•é€‰æ‹©çš„ç²¾åº¦ã€‚æ£€æŸ¥æ¨¡å‹å‡†ç¡®æ€§çš„æ›´å¥½æµ‹è¯•æ˜¯æŸ¥çœ‹å…¶åœ¨æ¨¡å‹æ„å»ºæœŸé—´æ ¹æœ¬æœªä½¿ç”¨çš„æ•°æ®çš„æ€§èƒ½ã€‚
5. **æé«˜æ€§èƒ½**ï¼šæ­¤æ­¥éª¤å¯èƒ½æ¶‰åŠé€‰æ‹©å®Œå…¨ä¸åŒçš„æ¨¡å‹æˆ–å¼•å…¥æ›´å¤šå˜é‡æ¥æé«˜æ•ˆç‡ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´è¿›è¡Œæ•°æ®æ”¶é›†å’Œå‡†å¤‡çš„åŸå› ã€‚

æ— è®ºæ˜¯ä»»ä½•æ¨¡å‹ï¼Œè¿™5ä¸ªæ­¥éª¤éƒ½å¯ç”¨äºæ„å»ºæŠ€æœ¯ï¼Œå½“æˆ‘ä»¬è®¨è®ºç®—æ³•æ—¶ï¼Œæ‚¨å°†æ‰¾åˆ°è¿™äº”ä¸ªæ­¥éª¤å¦‚ä½•å‡ºç°åœ¨æ¯ä¸ªæ¨¡å‹ä¸­ï¼

## æ•°æ®é›†

### 1.sklearnç©å…·æ•°æ®é›†

- å®šä¹‰

  æ•°æ®é‡å°ï¼Œæ•°æ®åœ¨sklearnåº“çš„æœ¬åœ°ï¼Œåªè¦å®‰è£…äº†sklearnï¼Œä¸ç”¨ä¸Šç½‘å°±å¯ä»¥è·å–

- å›¾ç¤º

  ![real_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/real_1.png)

### 2.sklearnç°å®ä¸–ç•Œæ•°æ®é›†

- å®šä¹‰

  æ•°æ®é‡å¤§ï¼Œæ•°æ®åªèƒ½é€šè¿‡ç½‘ç»œè·å–

- å›¾ç¤º

  ![toal_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/toal_1.png)

### 3.sklearnåŠ è½½ç©å…·æ•°æ®é›†

#### 3.1 é¸¢å°¾èŠ±æ•°æ®

- è¯­æ³•

  ```python
  from sklearn.datasets import load_iris
  iris = load_iris()#é¸¢å°¾èŠ±æ•°æ®,è¿”å›ä¸€ä¸ªBunchå¯¹è±¡
  ```

- ç‰¹å¾

  - â€‹	èŠ±è¼é•¿ sepal length  
  - â€‹	èŠ±è¼å®½ sepal width 
  - â€‹	èŠ±ç“£é•¿ petal length  
  - â€‹	èŠ±ç“£å®½ petal width

- åˆ†ç±»

  - â€‹	0-Setosaå±±é¸¢å°¾    
  - â€‹	1-Versicolourå˜è‰²é¸¢å°¾   
  - â€‹	2-Virginicaç»´å‰å°¼äºšé¸¢å°¾

- å®ä¾‹

  ```python
  import pandas as pd
  import numpy as np
  from sklearn.datasets import load_iris
  iris=load_iris()
  data=iris.data
  target=iris.target
  target=target.reshape(len(target),1)
  iris_con=np.hstack([data,target])
  col=iris.feature_names
  col.append('target')
  iris_show=pd.DataFrame(iris_con,columns=col)
  ```

  ![bird](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/bird.png)

#### 3.2 ç³–å°¿ç—…æ•°æ®

- è¯­æ³•

  ```python
  from sklearn.datasets import  load_diabetes
  diabetes=load_diabetes()
  ```

- ç‰¹å¾

  - â€Œ**å¹´é¾„ï¼ˆâ€Œageï¼‰â€Œ**â€Œ
  - â€Œ**æ€§åˆ«ï¼ˆâ€Œsexï¼‰â€Œ**â€Œ
  - â€Œ**èº«ä½“è´¨é‡æŒ‡æ•°ï¼ˆâ€Œbmiï¼‰â€Œ**â€Œ
  - â€Œ**å¹³å‡è¡€å‹ï¼ˆâ€Œbpï¼‰â€Œ**â€Œ
  - â€Œ**å…­ç§è¡€æ¸…çš„åŒ–éªŒæ•°æ®ï¼ˆâ€Œs1~s6ï¼‰â€Œ**â€Œï¼Œâ€ŒåŒ…æ‹¬Tç»†èƒï¼ˆâ€Œtcï¼‰â€Œã€â€Œä½å¯†åº¦è„‚è›‹ç™½ï¼ˆâ€Œldlï¼‰â€Œã€â€Œé«˜å¯†åº¦è„‚è›‹ç™½ï¼ˆâ€Œhdlï¼‰â€Œã€â€Œä¿ƒç”²çŠ¶è…ºæ¿€ç´ ï¼ˆâ€Œtchï¼‰â€Œã€â€Œæ‹‰è«ä¸‰å—ªï¼ˆâ€Œltgï¼‰â€Œå’Œè¡€ç³–æ°´å¹³ï¼ˆâ€Œgluï¼‰â€Œã€‚â€Œ

- å®ä¾‹

  ```python
  from sklearn.datasets import  load_diabetes
  diabetes=load_diabetes()
  data=diabetes.data
  target=diabetes.target
  target=target.reshape(len(target),1)
  diabetes_con=np.hstack([data,target])
  col=diabetes.feature_names
  col.append('target')
  diabetes_show=pd.DataFrame(diabetes_con,columns=col)
  
  print(diabetes_show)
  ```

  ![tang](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tang.png)



#### 3.3 æ•°å­—æ•°æ®é›†

- è¯­æ³•

  ```python
  from sklearn.datasets import load_digits
  num=load_digits()
  ```

- ç‰¹å¾

  - æ¯ä¸ªæ ·æœ¬ç”±64ä¸ªç‰¹å¾ç»„æˆï¼Œâ€Œè¿™äº›ç‰¹å¾å®é™…ä¸Šæ˜¯8x8åƒç´ å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ çš„ç°åº¦å€¼ã€‚â€Œç°åº¦å€¼é€šå¸¸æ˜¯ä¸€ä¸ªä»‹äº0åˆ°æŸä¸ªæœ€å¤§å€¼ï¼ˆâ€Œåœ¨è¿™ä¸ªæ•°æ®é›†ä¸­æ˜¯16ï¼‰â€Œä¹‹é—´çš„æ•´æ•°ï¼Œâ€Œä½†åœ¨åŠ è½½åˆ°sklearnä¸­æ—¶ï¼Œâ€Œå®ƒä»¬è¢«è½¬æ¢ä¸ºfloat64ç±»å‹çš„å€¼ï¼Œâ€Œå¹¶ä¸”é€šå¸¸ä¼šè¢«å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´å†…ï¼Œã€‚
  - 64ä¸ªç‰¹å¾ä»¥çº¿æ€§æ–¹å¼æ’åˆ—ï¼Œâ€Œå¯¹åº”äº8x8å›¾åƒçŸ©é˜µçš„å±•å¹³ç‰ˆæœ¬ã€‚â€Œå…·ä½“æ¥è¯´ï¼Œâ€Œç¬¬ä¸€ä¸ªç‰¹å¾å¯¹åº”äºå›¾åƒå·¦ä¸Šè§’çš„åƒç´ ï¼Œâ€Œæ¥ä¸‹æ¥çš„8ä¸ªç‰¹å¾å¯¹åº”äºç¬¬ä¸€è¡Œçš„å…¶ä½™åƒç´ ï¼Œâ€Œä¾æ­¤ç±»æ¨ï¼Œâ€Œç›´åˆ°éå†æ•´ä¸ªå›¾åƒã€‚â€Œ

- åˆ†ç±»

  **å«0-9å…±10ç§æ ‡ç­¾ï¼Œâ€Œå„ç±»æ ·æœ¬å‡è¡¡ï¼Œâ€Œç‰¹å¾ä¸ºç¦»æ•£æ•°å€¼0-16ä¹‹é—´**â€Œã€‚â€Œ

- å®ä¾‹

  ```python
  from sklearn.datasets import load_digits
  num=load_digits()
  
  data=num.data
  target=num.target
  target=target.reshape(len(target),1)
  num_con=np.hstack([data,target])
  col=num.feature_names
  col.append('target')
  num_show=pd.DataFrame(num_con,columns=col)
  
  print(num_show)
  ```

  ![num](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/num.png)

#### 3.4 linnerudç‰©ç†é”»ç‚¼æ•°æ®

- è¯­æ³•

  ```python
  from sklearn.datasets import load_linnerud
  phy=load_linnerud()
  ```

- ç‰¹å¾

  Chinsï¼šå¼•ä½“å‘ä¸Š

  Situpsï¼šä»°å§èµ·å  

  Jumpsï¼šè·³é«˜  

- ç›®æ ‡å‚æ•°

  Weightï¼šä½“é‡  

  Waistï¼šè…°å›´  

  Pulseï¼šè„‰æ

- å®ä¾‹

  ```python
  from sklearn.datasets import load_linnerud
  phy=load_linnerud()
  
  data=phy.data
  target=phy.target
  phy_con=np.hstack([data,target])
  col=phy.feature_names
  col.append('Weight')
  col.append('Waist')
  col.append('Pulse')
  phy_show=pd.DataFrame(phy_con,columns=col)
  
  print(phy_show)
  ```

  ![phy](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/phy.png)

#### 3.5 è‘¡è„é…’æ•°æ®é›†

- è¯­æ³•

  ```python
  from sklearn.datasets import load_wine
  achlo=load_wine()
  ```

- ç‰¹å¾

  1.â€Œ**é…’ç²¾ï¼ˆâ€Œalcoholï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„é…’ç²¾å«é‡ã€‚â€Œ

  2.â€Œ**è‹¹æœé…¸ï¼ˆâ€Œmalic_acidï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„è‹¹æœé…¸å«é‡ï¼Œâ€Œè‹¹æœé…¸æ˜¯ä¸€ç§æœ‰æœºé…¸ï¼Œâ€Œå¯¹è‘¡è„é…’çš„é£å‘³å’Œå£æ„Ÿæœ‰é‡è¦å½±å“ã€‚â€Œ

  3.â€Œ**ç°åˆ†ï¼ˆâ€Œashï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„ç°åˆ†å«é‡ï¼Œâ€Œç°åˆ†æ˜¯è‘¡è„é…’ä¸­æ— æœºç‰©è´¨çš„æ€»å’Œï¼Œâ€ŒåŒ…æ‹¬çŸ¿ç‰©è´¨ç­‰ã€‚â€Œ

  4.â€Œ**ç°çš„ç¢±æ€§ï¼ˆâ€Œalcalinity_of_ashï¼‰â€Œ**â€Œï¼šâ€Œç°åˆ†çš„ç¢±æ€§ç¨‹åº¦ï¼Œâ€Œåæ˜ äº†ç°åˆ†ä¸­çŸ¿ç‰©è´¨çš„ç§ç±»å’Œå«é‡ã€‚â€Œ

  5.â€Œ**é•ï¼ˆâ€Œmagnesiumï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„é•å«é‡ï¼Œâ€Œé•æ˜¯ä¸€ç§é‡è¦çš„çŸ¿ç‰©è´¨ï¼Œâ€Œå¯¹è‘¡è„é…’çš„é£å‘³å’Œç¨³å®šæ€§æœ‰ä¸€å®šå½±å“ã€‚â€Œ

  6.â€Œ**æ€»é…šï¼ˆâ€Œtotal_phenolsï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„æ€»é…šå«é‡ï¼Œâ€Œé…šç±»ç‰©è´¨æ˜¯è‘¡è„é…’ä¸­çš„é‡è¦æˆåˆ†ï¼Œâ€Œå¯¹è‘¡è„é…’çš„é¢œè‰²ã€â€Œé£å‘³å’ŒæŠ—æ°§åŒ–æ€§æœ‰è´¡çŒ®ã€‚â€Œ

  7.â€Œ**ç±»é»„é…®ï¼ˆâ€Œflavanoidsï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„ç±»é»„é…®å«é‡ï¼Œâ€Œç±»é»„é…®æ˜¯ä¸€ç±»å…·æœ‰æŠ—æ°§åŒ–æ€§è´¨çš„æ¤ç‰©åŒ–å­¦ç‰©è´¨ã€‚â€Œ

  8.â€Œ**éé»„çƒ·ç±»é…šç±»ï¼ˆâ€Œnonflavanoid_phenolsï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„éé»„çƒ·ç±»é…šç±»ç‰©è´¨å«é‡ï¼Œâ€Œè¿™äº›ç‰©è´¨åŒæ ·å¯¹è‘¡è„é…’çš„é£å‘³å’ŒæŠ—æ°§åŒ–æ€§æœ‰è´¡çŒ®ã€‚â€Œ

  9.â€Œ**èŠ±é’ç´ ï¼ˆâ€Œproanthocyaninsï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„èŠ±é’ç´ å«é‡ï¼Œâ€ŒèŠ±é’ç´ æ˜¯ä¸€ç§å¼ºæ•ˆçš„å¤©ç„¶è‰²ç´ å’ŒæŠ—æ°§åŒ–å‰‚ï¼Œâ€Œå¯¹è‘¡è„é…’çš„é¢œè‰²å’Œç¨³å®šæ€§æœ‰é‡è¦ä½œç”¨ã€‚â€Œ

  10.â€Œ**é¢œè‰²å¼ºåº¦ï¼ˆâ€Œcolor_intensityï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’çš„é¢œè‰²å¼ºåº¦ï¼Œâ€Œåæ˜ äº†è‘¡è„é…’ä¸­è‰²ç´ çš„å«é‡å’Œåˆ†å¸ƒã€‚â€Œ

  11.â€Œ**è‰²è°ƒï¼ˆâ€Œhueï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’çš„è‰²è°ƒï¼Œâ€Œå³é¢œè‰²çš„å…·ä½“è¡¨ç°ï¼Œâ€Œå¦‚çº¢è‘¡è„é…’å¯èƒ½å‘ˆç°ä¸åŒçš„çº¢è‰²è°ƒã€‚â€Œ

  12.â€Œ**od280/od315ç¨€é‡Šè‘¡è„é…’ï¼ˆâ€Œod280/od315_of_diluted_winesï¼‰â€Œ**â€Œï¼šâ€Œè¿™æ˜¯ä¸€ä¸ªå…‰å­¦å¯†åº¦çš„æ¯”å€¼ï¼Œâ€Œç”¨äºæè¿°è‘¡è„é…’åœ¨ç‰¹å®šæ³¢é•¿ä¸‹çš„å¸æ”¶ç‰¹æ€§ï¼Œâ€Œä¸è‘¡è„é…’ä¸­çš„æŸäº›åŒ–å­¦æˆåˆ†å«é‡æœ‰å…³ã€‚â€Œ

  13.â€Œ**è„¯æ°¨é…¸ï¼ˆâ€Œprolineï¼‰â€Œ**â€Œï¼šâ€Œè‘¡è„é…’ä¸­çš„è„¯æ°¨é…¸å«é‡ï¼Œâ€Œè„¯æ°¨é…¸æ˜¯ä¸€ç§æ°¨åŸºé…¸ï¼Œâ€Œå¯¹è‘¡è„é…’çš„é£å‘³å’Œå£æ„Ÿæœ‰ä¸€å®šå½±å“ã€‚â€Œ

- åˆ†ç±»

  'class_0'

  'class_1'

  'class_2'

- å®ä¾‹

  ```python
  from sklearn.datasets import load_wine
  achlo=load_wine()
  
  data=achlo.data
  target=achlo.target
  target=target.reshape(len(target),1)
  achlo_con=np.hstack([data,target])
  col=achlo.feature_names
  col.append('target')
  achlo_show=pd.DataFrame(achlo_con,columns=col)
  
  
  print(achlo_show)
  ```

  ![achol](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/achol.png)

#### 3.6 ä¹³è…ºç™Œæ•°æ®é›†

- è¯­æ³•

  ```python
  from sklearn.datasets import load_breast_cancer
  cancer=load_breast_cancer()
  ```

- ç‰¹å¾

  1.â€Œ**è‚¿ç˜¤çš„å¤§å°ç›¸å…³ç‰¹å¾**â€Œï¼šâ€Œå¦‚åŠå¾„ã€â€Œå‘¨é•¿ã€â€Œé¢ç§¯ç­‰ï¼Œâ€Œè¿™äº›ç‰¹å¾ç›´æ¥åæ˜ äº†è‚¿ç˜¤çš„ç‰©ç†å°ºå¯¸1ã€‚â€Œ

  2.â€Œ**è‚¿ç˜¤çš„å½¢çŠ¶å’Œçº¹ç†ç‰¹å¾**â€Œï¼šâ€Œå¦‚çº¹ç†ï¼ˆâ€Œç°åº¦å€¼çš„æ ‡å‡†åå·®ï¼‰â€Œã€â€Œå¹³æ»‘åº¦ï¼ˆâ€ŒåŠå¾„çš„å˜åŒ–å¹…åº¦ï¼‰â€Œã€â€Œå¯†å®åº¦ï¼ˆâ€Œå‘¨é•¿çš„å¹³æ–¹é™¤ä»¥é¢ç§¯çš„å•†å†å‡1ï¼‰â€Œã€â€Œå‡¹åº¦ï¼ˆâ€Œå‡¹é™·éƒ¨åˆ†è½®å»“çš„ä¸¥é‡ç¨‹åº¦ï¼‰â€Œã€â€Œå‡¹ç‚¹ï¼ˆâ€Œå‡¹é™·è½®å»“çš„æ•°é‡ï¼‰â€Œç­‰ï¼Œâ€Œè¿™äº›ç‰¹å¾æè¿°äº†è‚¿ç˜¤çš„è¡¨é¢å½¢æ€å’Œå†…éƒ¨ç»“æ„12ã€‚â€Œ

  3.â€Œ**å…¶ä»–ç”Ÿç‰©å­¦ç‰¹å¾**â€Œï¼šâ€Œå¦‚å¯¹ç§°æ€§ã€â€Œåˆ†å½¢ç»´åº¦ç­‰ï¼Œâ€Œè¿™äº›ç‰¹å¾æä¾›äº†å…³äºè‚¿ç˜¤ç”Ÿé•¿æ–¹å¼å’Œå¤æ‚æ€§çš„é¢å¤–ä¿¡æ¯

- åˆ†ç±»

  0è¡¨ç¤ºè‰¯æ€§è‚¿ç˜¤

  1è¡¨ç¤ºæ¶æ€§è‚¿ç˜¤

- å®ä¾‹

  ```python
  from sklearn.datasets import load_breast_cancer
  cancer=load_breast_cancer()
  
  data=cancer.data
  target=cancer.target
  target=target.reshape(len(target),1)
  cancer_con=np.hstack([data,target])
  col=cancer.feature_names
  col=list(col)
  col.append('target')
  cancer_show=pd.DataFrame(cancer_con,columns=col)
  
  print(cancer_show)
  ```

  ![cancer](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/cancer.png)

### 4.sklearnè·å–ç°å®ä¸–ç•Œæ•°æ®é›†

#### 4.1è·å–20åˆ†ç±»æ–°é—»æ•°æ®

- æ–‡ä»¶ä¿å­˜ç›®å½•

  ```python
  """
  (1)æ‰€æœ‰ç°å®ä¸–ç•Œæ•°æ®ï¼Œé€šè¿‡ç½‘ç»œæ‰èƒ½ä¸‹è½½åï¼Œé»˜è®¤ä¿å­˜çš„ç›®å½•å¯ä»¥ä½¿ç”¨ä¸‹é¢apiè·å–ã€‚å®é™…ä¸Šå°±æ˜¯ä¿å­˜åˆ°å®¶ç›®å½•
  
  (2)ä¸‹è½½æ—¶ï¼Œæœ‰å¯èƒ½å›ä¸ºç½‘ç»œé—®é¢˜è€Œå‡ºé¢˜ï¼Œè¦â€œå°å¿ƒâ€çš„è§£å†³ç½‘ç»œé—®é¢˜ï¼Œä¸å¯è¨€â€¦..
  
  (3)ç¬¬ä¸€æ¬¡ä¸‹è½½ä¼šä¿å­˜çš„ç¡¬ç›˜ä¸­ï¼Œå¦‚æœç¬¬äºŒæ¬¡ä¸‹è½½ï¼Œå› ä¸ºç¡¬ç›˜ä¸­å·²ç»ä¿å­˜æœ‰äº†ï¼Œæ‰€ä»¥ä¸ä¼šå†æ¬¡ä¸‹è½½å°±ç›´æ¥åŠ è½½æˆåŠŸäº†ã€‚
  
  """
  from sklearn import datasets
  ret=datasets.get_data_home()
  print(ret)
  #C:\Users\13167\scikit_learn_data
  ```

- è¯­æ³•

  ```python
  sklearn.datasets.fetch_20newsgroups(data_home,subset)
  ```

- å‚æ•°

  data_home

  ```python
  =None
  è¿™æ˜¯é»˜è®¤å€¼ï¼Œä¸‹è½½çš„æ–‡ä»¶è·¯å¾„ä¸º â€œC:/Users/ADMIN/scikit_learn_data/20news-bydate_py3.pkzâ€
  
  =è‡ªå®šä¹‰è·¯å¾„
  	ä¾‹å¦‚ â€œ./srcâ€, ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„ä¸ºâ€œ./20news-bydate_py3.pkzâ€
  ```

  subset

  ```python
  â€œtrainâ€ï¼Œåªä¸‹è½½è®­ç»ƒé›†
  â€œtestâ€ï¼Œåªä¸‹è½½æµ‹è¯•é›†
  â€œallâ€ï¼Œ ä¸‹è½½çš„æ•°æ®åŒ…å«äº†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
  ```

   return_X_yï¼Œå†³å®šç€è¿”å›å€¼çš„æƒ…å†µ

  ```python
  å½“å‚æ•°return_X_yå€¼ä¸ºFalseæ—¶ï¼Œ å‡½æ•°è¿”å›Bunchå¯¹è±¡,Bunchå¯¹è±¡ä¸­æœ‰ä»¥ä¸‹å±æ€§
      *data:ç‰¹å¾æ•°æ®é›†, é•¿åº¦ä¸º18846çš„åˆ—è¡¨list, æ¯ä¸€ä¸ªå…ƒç´ å°±æ˜¯ä¸€ç¯‡æ–°é—»å†…å®¹ï¼Œ å…±æœ‰18846ç¯‡
      *target:ç›®æ ‡æ•°æ®é›†ï¼Œé•¿åº¦ä¸º18846çš„æ•°ç»„ndarray, ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œæ•´æ•°å€¼ä¸º[0,20]
      *target_names:ç›®æ ‡æè¿°ï¼Œé•¿åº¦ä¸º20çš„list
      *filenamesï¼šé•¿åº¦ä¸º18846çš„ndarray, å…ƒç´ ä¸ºå­—ç¬¦ä¸²,ä»£è¡¨æ–°é—»çš„æ•°æ®ä½ç½®çš„è·¯å¾„
      
  å½“å‚æ•°return_X_yå€¼ä¸ºTrueæ—¶ï¼Œå‡½æ•°è¿”å›å€¼ä¸ºå…ƒç»„ï¼Œå…ƒç»„é•¿åº¦ä¸º2ï¼Œ ç¬¬ä¸€ä¸ªå…ƒç´ å€¼ä¸ºç‰¹å¾æ•°æ®é›†ï¼Œç¬¬äºŒä¸ªå…ƒç´ å€¼ä¸ºç›®æ ‡æ•°æ®é›†
  ```

- å®ä¾‹

  ```python
  from sklearn import datasets
  ret=sklearn.datasets.fetch_20newsgroups(data_home='./',subset='all')
  print(len(ret.data))
  print(ret.target.shape) #(18846,)
  print(ret.target_names) #20
  print(ret.filenames) #18846
  ```

  ![news](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/news.png)

#### 4.2åŠ è½½åŠ åˆ©ç¦å°¼äºšä½æˆ¿æ•°æ®

- è¯­æ³•

  ```python
  fetch_california_housing( *, data_home=None, download_if_missing=True, return_X_y=False, as_frame=False
  )
  ```

- å®ä¾‹

  ```python
  from sklearn import datasets
  ret=datasets.fetch_california_housing(data_home='./',as_frame=True)
  print(ret.data)
  ```

  ![house](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/house.png)

### 5.æœ¬åœ°csvæ•°æ®åˆ›å»ºå’Œä½¿ç”¨

#### 1.åˆ›å»ºcsvæ–‡ä»¶

æ–¹å¼1ï¼šæ‰“å¼€è®¡äº‹æœ¬ï¼Œå†™å‡ºå¦‚ä¸‹æ•°æ®ï¼Œæ•°æ®ä¹‹é—´ä½¿ç”¨è‹±æ–‡ä¸‹çš„é€—å·, ä¿å­˜æ–‡ä»¶åæŠŠåç¼€åæ”¹ä¸ºcsv

```python
milage,Liters,Consumtime,target
40920,8.326976,0.953952,3
14488,7.153469,1.673904,2
26052,1.441871,0.805124,1
75136,13.147394,0.428964,1
```

æ–¹å¼2ï¼šåˆ›å»ºexcel æ–‡ä»¶,  å¡«å†™æ•°æ®ï¼Œä»¥csvä¸ºåç¼€ä¿å­˜æ–‡ä»¶

#### 2.pandasåŠ è½½csv

```py
test=pd.read_csv('test1.csv')
print(test)
```

![csv](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/csv.png)

### 6.æ•°æ®é›†åˆ’åˆ†

#### 1.å‡½æ•°

```python
sklearn.model_selection.train_test_split(*arraysï¼Œ**options)

"""
(1) *array 
	è¿™é‡Œç”¨äºæ¥æ”¶1åˆ°å¤šä¸ª"åˆ—è¡¨ã€numpyæ•°ç»„ã€ç¨€ç–çŸ©é˜µæˆ–padasä¸­çš„DataFrame"ã€‚	
	
(2) **optionsï¼Œ é‡è¦çš„å…³é”®å­—å‚æ•°æœ‰ï¼š
 	    test_size å€¼ä¸º0.0åˆ°1.0çš„å°æ•°ï¼Œè¡¨ç¤ºåˆ’åˆ†åæµ‹è¯•é›†å çš„æ¯”ä¾‹
        random_state å€¼ä¸ºä»»æ„æ•´æ•°ï¼Œè¡¨ç¤ºéšæœºç§å­ï¼Œä½¿ç”¨ç›¸åŒçš„éšæœºç§å­å¯¹ç›¸åŒçš„æ•°æ®é›†å¤šæ¬¡åˆ’åˆ†ç»“æœæ˜¯ç›¸åŒçš„ã€‚å¦åˆ™å¤šåŠä¸åŒ
        
(3)è¿”å›å€¼è¯´æ˜
	è¿”å›å€¼ä¸ºåˆ—è¡¨list, åˆ—è¡¨é•¿åº¦ä¸å½¢å‚arrayæ¥æ”¶åˆ°çš„å‚æ•°æ•°é‡ç›¸å…³è”, å½¢å‚arrayæ¥æ”¶åˆ°çš„æ˜¯ä»€ä¹ˆç±»å‹ï¼Œlistä¸­å¯¹åº”è¢«åˆ’åˆ†å‡ºæ¥çš„ä¸¤éƒ¨åˆ†å°±æ˜¯ä»€ä¹ˆç±»å‹
"""
```

#### 2.åˆ—è¡¨æ•°æ®é›†åˆ’åˆ†

```python
from sklearn.model_selection import train_test_split
data1 = [1,    2,    3,    4,    5]
data2 = ["1a", "2a","3a", "4a",  "5a"]
#éšæœºç§å­éƒ½ä½¿ç”¨äº†ç›¸åŒçš„æ•´æ•°(11)ï¼Œæ‰€ä»¥åˆ’åˆ†çš„åˆ’åˆ†çš„æƒ…å†µæ˜¯ç›¸åŒçš„ã€‚
a, b = train_test_split(data1, test_size=0.1, random_state=11)
print(a, b)
#[5, 1, 4, 2] [3]
a, b = train_test_split(data2, test_size=0.1, random_state=11)
print(a, b)
#['5a', '1a', '4a', '2a'] ['3a']
a, b, c, d  = train_test_split(data1, data2,  test_size=0.1, random_state=11)
print(a,b,c,d)
#[5, 1, 4, 2] [3] ['5a', '1a', '4a', '2a'] ['3a']
```

#### 3.ndarrayæ•°æ®é›†åˆ’åˆ†

```python
#åˆ’åˆ†å‰å’Œåˆ’åˆ†åçš„æ•°æ®ç±»å‹æ˜¯ç›¸åŒçš„
data1 = [1,    2,    3,    4,    5]
data2 = np.array(["1a", "2a","3a", "4a",  "5a"]) 
a, b, c, d  = train_test_split(data1, data2,  test_size=0.4, random_state=22)
print(a, b, c, d)  
#[4, 1, 5] [2, 3] ['4a' '1a' '5a'] ['2a' '3a']
print(type(a), type(b), type(c), type(d)) 
#<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>
```

#### 4.äºŒç»´æ•°ç»„æ•°æ®é›†åˆ’åˆ†

```python
from sklearn.model_selection import train_test_split
import numpy as np
data1 = np.arange(1, 16, 1)
data1.shape=(5,3)
print(data1)
a, b = train_test_split(data1,  test_size=0.4, random_state=22)
print(a)
print(b)

"""
[[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]
 [13 14 15]]
 
 [[10 11 12]
 [ 1  2  3]
 [13 14 15]]

 [[4 5 6]
 [7 8 9]]

"""
```

#### 5.DataFrameæ•°æ®é›†åˆ’åˆ†

```python
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
#å¯ä»¥åˆ’åˆ†DataFrame, åˆ’åˆ†åçš„ä¸¤éƒ¨åˆ†è¿˜æ˜¯DataFrame
data1 = np.arange(1, 16, 1)
data1.shape=(5,3)
data1 = pd.DataFrame(data1, index=[1,2,3,4,5], columns=["one","two","three"])
print(data1)

a, b = train_test_split(data1,  test_size=0.4, random_state=22)
print(a)
print(b)

"""
 one  two  three
1    1    2      3
2    4    5      6
3    7    8      9
4   10   11     12
5   13   14     15

    one  two  three
4   10   11     12
1    1    2      3
5   13   14     15

    one  two  three
2    4    5      6
3    7    8      9

"""
```

#### 6.å­—å…¸æ•°æ®é›†åˆ’åˆ†

```python
"""
å¯ä»¥åˆ’åˆ†éç¨€ç–çŸ©é˜µ

ç”¨äºå°†å­—å…¸åˆ—è¡¨è½¬æ¢ä¸ºç‰¹å¾å‘é‡ã€‚è¿™ä¸ªè½¬æ¢å™¨ä¸»è¦ç”¨äºå¤„ç†ç±»åˆ«æ•°æ®å’Œæ•°å€¼æ•°æ®çš„æ··åˆå‹æ•°æ®é›†

1.å¯¹äºç±»åˆ«ç‰¹å¾`DictVectorizer` ä¼šä¸ºæ¯ä¸ªä¸åŒçš„ç±»åˆ«åˆ›å»ºä¸€ä¸ªæ–°çš„äºŒè¿›åˆ¶ç‰¹å¾ï¼Œå¦‚æœåŸå§‹æ•°æ®ä¸­çš„æŸä¸ªæ ·æœ¬å…·æœ‰è¯¥ç±»åˆ«ï¼Œåˆ™å¯¹åº”çš„äºŒè¿›åˆ¶ç‰¹å¾å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚

2.å¯¹äºæ•°å€¼ç‰¹å¾ä¿æŒä¸å˜ï¼Œç›´æ¥ä½œä¸ºç‰¹å¾çš„ä¸€éƒ¨åˆ†
"""
```

```python
from sklearn.feature_extraction import DictVectorizer
data = [{'city':'æˆéƒ½', 'age':30, 'temperature':20}, 
        {'city':'é‡åº†','age':33, 'temperature':60}, 
        {'city':'åŒ—äº¬', 'age':42, 'temperature':80},
        {'city':'ä¸Šæµ·', 'age':22, 'temperature':70},
        {'city':'æˆéƒ½', 'age':72, 'temperature':40},
       ]
transfer = DictVectorizer(sparse=True)
data_new = transfer.fit_transform(data)
print("data_new:\n", data_new)
x = data_new.toarray()
print(type(x))
print(x)
"""
(0,0)æ˜¯çŸ©é˜µçš„è¡Œåˆ—ä¸‹æ ‡  30æ˜¯å€¼
data_new:
  (0, 0)	30.0
  (0, 3)	1.0
  (0, 5)	20.0
  (1, 0)	33.0
  (1, 4)	1.0
  (1, 5)	60.0
  (2, 0)	42.0
  (2, 2)	1.0
  (2, 5)	80.0
  (3, 0)	22.0
  (3, 1)	1.0
  (3, 5)	70.0
  (4, 0)	72.0
  (4, 3)	1.0
  (4, 5)	40.0
<class 'numpy.ndarray'>
# ç¬¬ä¸€è¡Œä¸­:30è¡¨ç¤ºageçš„å€¼  0è¡¨ç¤ºä¸Šæµ· 0è¡¨ç¤ºåŒ—äº¬ 1è¡¨ç¤ºæˆéƒ½ 0è¡¨ç¤ºé‡åº† 20è¡¨ç¤ºtemperature
[[30.  0.  0.  1.  0. 20.]
 [33.  0.  0.  0.  1. 60.]
 [42.  0.  1.  0.  0. 80.]
 [22.  1.  0.  0.  0. 70.]
 [72.  0.  0.  1.  0. 40.]]

"""
```

```python
a, b = train_test_split(data_new,  test_size=0.4, random_state=22)
print(a)
print("\n", b)

"""
  (0, 0)	22.0
  (0, 1)	1.0
  (0, 5)	70.0
  (1, 0)	30.0
  (1, 3)	1.0
  (1, 5)	20.0
  (2, 0)	72.0
  (2, 3)	1.0
  (2, 5)	40.0

  (0, 0)	33.0
  (0, 4)	1.0
  (0, 5)	60.0
  (1, 0)	42.0
  (1, 2)	1.0
  (1, 5)	80.0


"""
```

```python
#data_new.toarray()æ˜¯ndarray
a, b = train_test_split(data_new.toarray(),  test_size=0.4, random_state=22)
print(a)
print("\n", b)
"""
[[22.  1.  0.  0.  0. 70.]
 [30.  0.  0.  1.  0. 20.]
 [72.  0.  0.  1.  0. 40.]]

 [[33.  0.  0.  0.  1. 60.]
 [42.  0.  1.  0.  0. 80.]]

"""
```

#### 7.é¸¢å°¾èŠ±æ•°æ®é›†åˆ’åˆ†

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
iris = load_iris()
list = train_test_split(iris.data,iris.target,  test_size=0.2, random_state=22)
#x_trainè®­ç»ƒç‰¹å¾æ•°æ®é›†,x_testæµ‹è¯•ç‰¹å¾æ•°æ®é›†, y_trainè®­ç»ƒç›®æ ‡æ•°æ®é›†,y_testæµ‹è¯•ç›®æ ‡æ•°æ®é›†,
x_train, x_test, y_train, y_test = list   
#(120, 4) (30, 4) (120,) (30,)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
```

#### 8.ç°å®ä¸–ç•Œæ•°æ®é›†åˆ’åˆ†

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
import numpy as np
news = fetch_20newsgroups(data_home='./', subset='all')
list = train_test_split(news.data, news.target,test_size=0.2, random_state=22)
# """
# è¿”å›å€¼æ˜¯ä¸€ä¸ªlist:å…¶ä¸­æœ‰4ä¸ªå€¼ï¼Œåˆ†åˆ«ä¸ºè®­ç»ƒé›†ç‰¹å¾ã€æµ‹è¯•é›†ç‰¹å¾ã€è®­ç»ƒé›†ç›®æ ‡ã€æµ‹è¯•é›†ç›®æ ‡
x_train, x_test, y_train, y_test = list
# 15076 3770 (15076,) (3770,)
print(len(x_train), len(x_test), y_train.shape, y_test.shape) 
```

```python
#â€œåŠ è½½åŠ åˆ©ç¦å°¼äºšä½æˆ¿æ•°æ®é›†â€,å¹¶è¿›è¡Œæ•°æ®é›†åˆ’åˆ†
from sklearn import datasets
ret=datasets.fetch_california_housing(data_home='./',as_frame=True)
list=train_test_split(ret.data,ret.target,test_size=0.2,random_state=22)
x_train,x_target,y_train,y_target=list
print(x_train,x_target)
print(y_train,y_target)

```
## ç‰¹å¾å·¥ç¨‹

### 1.ç‰¹å¾å·¥ç¨‹æ¦‚å¿µ

- å®šä¹‰

  ç‰¹å¾å·¥ç¨‹æ˜¯å°†ä»»æ„æ•°æ®(å¦‚æ–‡æœ¬æˆ–å›¾åƒ)è½¬æ¢ä¸ºå¯ç”¨äºæœºå™¨å­¦ä¹ çš„**æ•°å­—ç‰¹å¾**,æ¯”å¦‚:å­—å…¸ç‰¹å¾æå–(ç‰¹å¾ç¦»æ•£åŒ–)ã€æ–‡æœ¬ç‰¹å¾æå–ã€å›¾åƒç‰¹å¾æå–ã€‚

- æ­¥éª¤

  - ç‰¹å¾æå–ï¼Œ å¦‚æœä¸æ˜¯è¡¨æ ¼æ•°æ®ï¼Œè¦è¿›è¡Œç‰¹å¾æå–ï¼Œæ¯”å¦‚å­—å…¸ç‰¹å¾æå–ï¼Œæ–‡æœ¬ç‰¹å¾æå–

  - æ— é‡åˆšåŒ–(é¢„å¤„ç†)

    - å½’ä¸€åŒ–
    - æ ‡å‡†åŒ–

  - é™ç»´

    - åº•æ–¹å·®è¿‡æ»¤ç‰¹å¾é€‰æ‹©

    - ä¸»æˆåˆ†åˆ†æ-PCAé™ç»´

### 2.ç‰¹å¾å·¥ç¨‹ç›¸å…³API

- å®ä¾‹åŒ–è½¬æ¢å™¨ç±»å¯¹è±¡

  ```python
  DictVectorizer  	å­—å…¸ç‰¹å¾æå–
  CountVectorizer 	æ–‡æœ¬ç‰¹å¾æå–
  TfidfVectorizer 	TF-IDFæ–‡æœ¬ç‰¹å¾è¯çš„é‡è¦ç¨‹åº¦ç‰¹å¾æå– 
  MinMaxScaler 		å½’ä¸€åŒ–
  StandardScaler 		æ ‡å‡†åŒ–
  VarianceThreshold 	åº•æ–¹å·®è¿‡æ»¤é™ç»´
  PCA  				ä¸»æˆåˆ†åˆ†æé™ç»´
  ```

- è½¬æ¢å™¨å¯¹è±¡è°ƒç”¨

  ```python
  """
  fit_transform()è¿›è¡Œè½¬æ¢, å…¶ä¸­fitç”¨äºè®¡ç®—æ•°æ®ï¼Œtransformè¿›è¡Œæœ€ç»ˆè½¬æ¢
  fit_transform()å¯ä»¥ä½¿ç”¨fit()å’Œtransform()ä»£æ›¿
  """
  data_new = transfer.fit_transform(data)
  å¯å†™æˆ
  transfer.fit(data)
  data_new = transfer.transform(data)
  ```



### 3.ç‰¹å¾å·¥ç¨‹å­˜å‚¨å½¢å¼

- ç¨€ç–çŸ©é˜µå®šä¹‰

  ç¨€ç–çŸ©é˜µæ˜¯æŒ‡ä¸€ä¸ªçŸ©é˜µä¸­å¤§éƒ¨åˆ†å…ƒç´ ä¸ºé›¶ï¼Œåªæœ‰å°‘æ•°å…ƒç´ æ˜¯éé›¶çš„çŸ©é˜µï¼Œä¸”éé›¶å…ƒç´ åˆ†å¸ƒæ²¡æœ‰æ˜æ˜¾çš„è§„å¾‹ã€‚

- ä¸‰å…ƒç»„è¡¨

  ä¸‰å…ƒç»„è¡¨å°±æ˜¯ä¸€ç§ç¨€ç–çŸ©é˜µç±»å‹æ•°æ®,å­˜å‚¨éé›¶å…ƒç´ çš„è¡Œç´¢å¼•ã€åˆ—ç´¢å¼•å’Œå€¼ã€‚é™¤äº†åˆ—å‡ºçš„æœ‰å€¼, å…¶ä½™å…¨æ˜¯0ã€‚

  ```python
  (0,0) 10
  
  (0,1) 20
  
  (2,0) 90
  
  (2,20) 8
  
  (8,0) 70
  ```

- ç¨ å¯†çŸ©é˜µ

  çŸ©é˜µä¸­éé›¶å…ƒç´ çš„æ•°é‡ä¸æ€»å…ƒç´ æ•°é‡ç›¸æ¯”æ¥è¿‘æˆ–ç›¸ç­‰ï¼Œä¹Ÿå°±æ˜¯è¯´çŸ©é˜µä¸­çš„å¤§éƒ¨åˆ†å…ƒç´ éƒ½æ˜¯éé›¶çš„ã€‚

- æ€»ç»“

  - **å­˜å‚¨**ï¼šç¨€ç–çŸ©é˜µä½¿ç”¨ç‰¹å®šçš„å­˜å‚¨æ ¼å¼æ¥èŠ‚çœç©ºé—´ï¼Œè€Œç¨ å¯†çŸ©é˜µä½¿ç”¨å¸¸è§„çš„æ•°ç»„å­˜å‚¨æ‰€æœ‰å…ƒç´ ï¼Œæ— è®ºå…¶æ˜¯å¦ä¸ºé›¶ã€‚
  - **è®¡ç®—**ï¼šç¨€ç–çŸ©é˜µåœ¨è¿›è¡Œè®¡ç®—æ—¶å¯ä»¥åˆ©ç”¨é›¶å…ƒç´ çš„ç‰¹æ€§è·³è¿‡ä¸å¿…è¦çš„è®¡ç®—ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚è€Œç¨ å¯†çŸ©é˜µåœ¨è®¡ç®—æ—¶éœ€è¦å¤„ç†æ‰€æœ‰å…ƒç´ ï¼ŒåŒ…æ‹¬é›¶å…ƒç´ ã€‚
  - **åº”ç”¨é¢†åŸŸ**ï¼šç¨€ç–çŸ©é˜µå¸¸è§äºå¤§è§„æ¨¡æ•°æ®åˆ†æã€å›¾å½¢å­¦ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨å­¦ä¹ ç­‰é¢†åŸŸï¼Œè€Œç¨ å¯†çŸ©é˜µåœ¨æ•°å­¦è®¡ç®—ã€çº¿æ€§ä»£æ•°ç­‰é€šç”¨è®¡ç®—é¢†åŸŸæ›´ä¸ºå¸¸è§ã€‚

### 4.DictVectorizer å­—å…¸åˆ—è¡¨ç‰¹å¾æå–

- è¯­æ³•

  ```python
  #åˆ›å»ºè½¬æ¢å™¨å¯¹è±¡
  sklearn.feature_extraction.DictVectorizer(sparse=True)
  """
  sparse=Trueè¿”å›ç±»å‹ä¸ºcsr_matrixçš„ç¨€ç–çŸ©é˜µ
  
  sparse=Falseè¡¨ç¤ºè¿”å›çš„æ˜¯æ•°ç»„,æ•°ç»„å¯ä»¥è°ƒç”¨.toarray()æ–¹æ³•å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºæ•°ç»„
  """
  ```

  ```python
  #è½¬æ¢å™¨å¯¹è±¡
  """
  è½¬æ¢å™¨å¯¹è±¡è°ƒç”¨fit_transform(data)å‡½æ•°ï¼Œå‚æ•°dataä¸ºä¸€ç»´å­—å…¸æ•°ç»„æˆ–ä¸€ç»´å­—å…¸åˆ—è¡¨,è¿”å›è½¬åŒ–åçš„çŸ©é˜µæˆ–æ•°ç»„
  
  è½¬æ¢å™¨å¯¹è±¡get_feature_names_out()æ–¹æ³•è·å–ç‰¹å¾å
  """
  ```

- å®ä¾‹:æå–ç¨€ç–çŸ©é˜µå¯¹åº”çš„æ•°ç»„

  ```python
  from sklearn.feature_extraction import DictVectorizer
  data=[
      {"name":"jack","age":23,"money":1000,"com":"å°ç±³"},
       {"name":"rose","age":33,"money":2000,"com":"è…¾è®¯"},
        {"name":"marry","age":83,"money":25,"com":"è‹¹æœ"},
         {"name":"joe","age":123,"money":899,"com":"å°ç±³"},
          {"name":"å°è’‹","age":33,"money":1,"com":"åæ¸…"},
  ]
  dict=DictVectorizer(sparse=False)
  """
  sparse=Trueè¿”å›ç±»å‹ä¸ºcsr_matrixçš„ç¨€ç–çŸ©é˜µ
  
  sparse=Falseè¡¨ç¤ºè¿”å›çš„æ˜¯æ•°ç»„,æ•°ç»„å¯ä»¥è°ƒç”¨.toarray()æ–¹æ³•å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºæ•°ç»„
  """
  data=dict.fit_transform(data)
  """
  fit_transform()è¿›è¡Œè½¬æ¢, å…¶ä¸­fitç”¨äºè®¡ç®—æ•°æ®ï¼Œtransformè¿›è¡Œæœ€ç»ˆè½¬æ¢
  fit_transform()å¯ä»¥ä½¿ç”¨fit()å’Œtransform()ä»£æ›¿
  """
  print(dict.get_feature_names_out())
  #è·å–ç‰¹å¾æ ‡ç­¾
  print(data)
  ```

  ![dict_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/dict_1.png)

- å®ä¾‹ï¼šæå–ç¨€ç–çŸ©é˜µ

  ```python
  from sklearn.feature_extraction import DictVectorizer
  data=[
      {"name":"jack","age":23,"money":1000,"com":"å°ç±³"},
       {"name":"rose","age":33,"money":2000,"com":"è…¾è®¯"},
        {"name":"marry","age":83,"money":25,"com":"è‹¹æœ"},
         {"name":"joe","age":123,"money":899,"com":"å°ç±³"},
          {"name":"å°è’‹","age":33,"money":1,"com":"åæ¸…"},
  ]
  dict=DictVectorizer(sparse=True)
  """
  sparse=Trueè¿”å›ç±»å‹ä¸ºcsr_matrixçš„ç¨€ç–çŸ©é˜µ
  
  sparse=Falseè¡¨ç¤ºè¿”å›çš„æ˜¯æ•°ç»„,æ•°ç»„å¯ä»¥è°ƒç”¨.toarray()æ–¹æ³•å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºæ•°ç»„
  """
  data=dict.fit_transform(data)
  """
  fit_transform()è¿›è¡Œè½¬æ¢, å…¶ä¸­fitç”¨äºè®¡ç®—æ•°æ®ï¼Œtransformè¿›è¡Œæœ€ç»ˆè½¬æ¢
  fit_transform()å¯ä»¥ä½¿ç”¨fit()å’Œtransform()ä»£æ›¿
  """
  print(dict.get_feature_names_out())
  #è·å–ç‰¹å¾æ ‡ç­¾
  print(data)
  ```

  ![dict_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/dict_2.png)

### 5.CountVectorizer æ–‡æœ¬ç‰¹å¾æå–

- è¯­æ³•

  ```python
  sklearn.feature_extraction.text.CountVectorizer
  """
  CountVectorizer()æ„é€ å‡½æ•°å…³é”®å­—å‚æ•°stop_wordsï¼Œå€¼ä¸ºlistï¼Œè¡¨ç¤ºè¯çš„é»‘åå•
  
  fit_transformå‡½æ•°çš„è¿”å›å€¼ä¸ºç¨€ç–çŸ©é˜µ
  
  """
  ```

- å®ä¾‹ï¼šè‹±æ–‡æ–‡æœ¬æå–

  ```python
  from sklearn.feature_extraction.text import CountVectorizer
  data=['man','what can i say','manba out','say man i am']
  count=CountVectorizer(stop_words=['man'])#è¿‡æ»¤æ‰manå•è¯
  data=count.fit_transform(data).toarray()#è½¬åŒ–ä¸ºæ•°ç»„
  data_show=pd.DataFrame(data,index=['ç¬¬ä¸€','ç¬¬äºŒ','ç¬¬ä¸‰','ç¬¬å››'],columns=count.get_feature_names_out())
  print(data_show)
  ```

  ![count_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/count_1.png)

- å®ä¾‹ï¼šä¸­æ–‡æ–‡æœ¬æå–

  ```python
  #a.ä¸­æ–‡æ–‡æœ¬ä¸åƒè‹±æ–‡æ–‡æœ¬ï¼Œä¸­æ–‡æ–‡æœ¬æ–‡å­—ä¹‹é—´æ²¡æœ‰ç©ºæ ¼ï¼Œæ‰€ä»¥è¦å…ˆåˆ†è¯ï¼Œä¸€èˆ¬ä½¿ç”¨jiebaåˆ†è¯.
  
  data = "åœ¨å¦‚ä»Šçš„äº’è”ç½‘ä¸–ç•Œï¼Œæ­£èƒ½é‡æ­£æˆä¸ºæ¾æ¹ƒæ—¶ä»£çš„å¤§æµé‡"
  data = jieba.cut(data)
  data = list(data)
  print(data) #['åœ¨', 'å¦‚ä»Š', 'çš„', 'äº’è”ç½‘', 'ä¸–ç•Œ', 'ï¼Œ', 'æ­£', 'èƒ½é‡', 'æ­£', 'æˆä¸º', 'æ¾æ¹ƒ', 'æ—¶ä»£', 'çš„', 'å¤§', 'æµé‡']
  data = "".join(data)
  print(data) #"åœ¨å¦‚ä»Šçš„äº’è”ç½‘ä¸–ç•Œ ï¼Œæ­£èƒ½é‡æ­£æˆä¸ºæ¾æ¹ƒæ—¶ä»£çš„å¤§æµé‡"
  ```

  ```python
  def china_txt(txt):
      return ' '.join(list(jieba.cut(txt)))
  
  data=['çº½çº¦è”å‚¨è°ƒæŸ¥æ˜¾ç¤ºå¤±ä¸šé¢„æœŸåˆ›å†å²æ–°é«˜','å¾·å…‹è¨æ–¯ç”µç½‘é¢ä¸´å¤å­£æœ€å¤§è€ƒéªŒ','å¾·å…‹è¨æ–¯å’Œçº½çº¦']
  data_new=[china_txt(x) for x in data]
  count=CountVectorizer()
  count_data=count.fit_transform(data_new).toarray()
  data_show=pd.DataFrame(count_data,columns=count.get_feature_names_out())
  
  print(data_show)
  ```

  ![count_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/count_2.png)

### 6.TfidfVectorizer TF-IDF æ–‡æœ¬ç‰¹å¾è¯çš„é‡è¦ç¨‹åº¦ç‰¹å¾æå– 

- ç®—æ³•

  ![compute](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/compute.png)

- è¯­æ³•

  ```python
  sklearn.feature_extraction.text.TfidfVectorizer()
  """
  æ„é€ å‡½æ•°å…³é”®å­—å‚æ•°stop_wordsï¼Œè¡¨ç¤ºè¯ç‰¹å¾é»‘åå•
  
  fit_transformå‡½æ•°çš„è¿”å›å€¼ä¸ºç¨€ç–çŸ©é˜µ
  """
  ```

- å®ä¾‹

  ```python
  from sklearn.feature_extraction.text import TfidfVectorizer
  
  def china_txt(txt):
      return ' '.join(list(jieba.cut(txt)))
  
  data=['çº½çº¦è”å‚¨è°ƒæŸ¥æ˜¾ç¤ºå¤±ä¸šé¢„æœŸåˆ›å†å²æ–°é«˜','å¾·å…‹è¨æ–¯ç”µç½‘é¢ä¸´å¤å­£æœ€å¤§è€ƒéªŒ','å¾·å…‹è¨æ–¯å’Œçº½çº¦']
  data_new=[china_txt(x) for x in data]
  count=TfidfVectorizer(stop_words=['é¢„æœŸ','è€ƒéªŒ'])
  count_data=count.fit_transform(data_new).toarray()
  data_show=pd.DataFrame(count_data,columns=count.get_feature_names_out())
  print(count.get_feature_names_out())
  print(count_data)
  ```

  ![tf](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/tf.png)

### 7.æ— é‡çº²åŒ–-é¢„å¤„ç†

- å®šä¹‰

  æ— é‡çº²ï¼Œå³æ²¡æœ‰å•ä½çš„æ•°æ®

- ä¸¾ä¾‹è¯´æ˜

  | ç¼–å·id | èº«é«˜ h   | æ”¶å…¥ s    | ä½“é‡ w  |
  | ------ | -------- | --------- | ------- |
  | 1      | 1.75(ç±³) | 15000(å…ƒ) | 120(æ–¤) |
  | 2      | 1.5(ç±³)  | 16000(å…ƒ) | 140(æ–¤) |
  | 3      | 1.6(ç±³)  | 20000(å…ƒ) | 100(æ–¤) |

  å‡è®¾ç®—æ³•ä¸­éœ€è¦æ±‚å®ƒä»¬ä¹‹é—´çš„æ¬§å¼è·ç¦», è¿™é‡Œä»¥ç¼–å·1å’Œç¼–å·2ä¸ºç¤ºä¾‹:

  $L = \sqrt{(1.75-1.5)^2+(15000-16000)^2+(120-140)^2}$

  ä»è®¡ç®—ä¸Šæ¥çœ‹, å‘ç°èº«é«˜å¯¹è®¡ç®—ç»“æœæ²¡æœ‰ä»€ä¹ˆå½±å“, åŸºæœ¬ä¸»è¦ç”±æ”¶å…¥æ¥å†³å®šäº†,ä½†æ˜¯ç°å®ç”Ÿæ´»ä¸­,èº«é«˜æ˜¯æ¯”è¾ƒé‡è¦çš„åˆ¤æ–­æ ‡å‡†.  æ‰€ä»¥éœ€è¦æ— é‡çº²åŒ–.

#### 1.MinMaxScaler å½’ä¸€åŒ–

- å®šä¹‰

  é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®æ˜ å°„åˆ°æŒ‡å®šåŒºé—´**(é»˜è®¤ä¸º0-1)**

- å…¬å¼

  ![G_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/G_1.png)

  è¿™é‡Œçš„ ğ‘¥min å’Œ ğ‘¥max åˆ†åˆ«æ˜¯æ¯ç§ç‰¹å¾ä¸­çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œè€Œ ğ‘¥æ˜¯å½“å‰ç‰¹å¾å€¼ï¼Œğ‘¥scaled æ˜¯å½’ä¸€åŒ–åçš„ç‰¹å¾å€¼ã€‚

  è‹¥è¦ç¼©æ”¾åˆ°å…¶ä»–åŒºé—´ï¼Œå¯ä»¥ä½¿ç”¨å…¬å¼ï¼š**x=x*(max-min)+min**

  æ¯”å¦‚ [-1, 1]çš„å…¬å¼ä¸º:

  ![G_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/G_2.png)

  æ‰‹ç®—è¿‡ç¨‹:

  ![G_3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/G_3.png)

- è¯­æ³•

  ```python
  sklearn.preprocessing.MinMaxScaler(feature_range)
  """
  å‚æ•°:feature_range=(0,1) å½’ä¸€åŒ–åçš„å€¼åŸŸ,å¯ä»¥è‡ªå·±è®¾å®š
  
  fit_transformå‡½æ•°å½’ä¸€åŒ–çš„åŸå§‹æ•°æ®ç±»å‹å¯ä»¥æ˜¯listã€DataFrameå’Œndarray, ä¸å¯ä»¥æ˜¯ç¨€ç–çŸ©é˜µ
  
  fit_transformå‡½æ•°çš„è¿”å›å€¼ä¸ºndarray
  
  """
  ```

- å®ä¾‹ï¼šlistæ•°æ®å½’ä¸€åŒ–

  ```python
  from sklearn.preprocessing import MinMaxScaler
  data=[[110,1243,23255,238423,42314],
        [103,1223,23535,234423,42335],
        [103,1223,23535,234423,42332],
        [103,1223,23535,234423,42339],
        [103,1223,23535,234423,42330],
        [103,1223,23535,234423,42331],
        [170,1253,23755,234223,10]]
  scale=MinMaxScaler(feature_range=(0,1))
  data=scale.fit_transform(data)
  print(data)
  ```

  ![min_max_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/min_max_1.png)

- å®ä¾‹ï¼šdataFrameæ•°æ®å½’ä¸€åŒ–

  ```python
  from sklearn.preprocessing import MinMaxScaler
  scale=MinMaxScaler(feature_range=(0,1))
  data=[[12,22,4],[22,23,1],[11,23,9]]
  data = pd.DataFrame(data=data, index=["ä¸€","äºŒ","ä¸‰"], columns=["ä¸€åˆ—","äºŒåˆ—","ä¸‰åˆ—"])
  data_show=scale.fit_transform(data)
  print(data_show)
  ```

  ![min_max_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/min_max_2.png)

- å®ä¾‹ï¼šndarrayæ•°æ®å½’ä¸€åŒ–

  ```python
  from sklearn.preprocessing import MinMaxScaler
  data=[
      {"name":"jack","age":23,"money":1000,"com":"å°ç±³"},
       {"name":"rose","age":33,"money":2000,"com":"è…¾è®¯"},
        {"name":"marry","age":83,"money":25,"com":"è‹¹æœ"},
         {"name":"joe","age":123,"money":899,"com":"å°ç±³"},
          {"name":"å°è’‹","age":33,"money":1,"com":"åæ¸…"},
  ]
  scale=MinMaxScaler()
  dict=DictVectorizer()
  data_dict=dict.fit_transform(data).toarray()
  data_scale=scale.fit_transform(data_dict)
  print(data_scale)
  ```

  ![min_max_3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/min_max_3.png)

- ç¼ºç‚¹

  æœ€å¤§å€¼å’Œæœ€å°å€¼å®¹æ˜“å—åˆ°å¼‚å¸¸ç‚¹å½±å“ï¼Œæ‰€ä»¥**é²æ§æ€§(å¥å£®æ€§)**è¾ƒå·®ã€‚æ‰€ä»¥å¸¸ä½¿ç”¨æ ‡å‡†åŒ–çš„æ— é‡é’¢åŒ–

#### 2.StandardScaler æ ‡å‡†åŒ–

- å®šä¹‰

  æ ‡å‡†åŒ–æ˜¯ä¸€ç§æ•°æ®é¢„å¤„ç†æŠ€æœ¯ï¼Œä¹Ÿç§°ä¸ºæ•°æ®å½’ä¸€åŒ–æˆ–ç‰¹å¾ç¼©æ”¾ã€‚å®ƒçš„ç›®çš„æ˜¯å°†ä¸åŒç‰¹å¾çš„æ•°å€¼èŒƒå›´ç¼©æ”¾åˆ°ç»Ÿä¸€çš„æ ‡å‡†èŒƒå›´ï¼Œä»¥ä¾¿æ›´å¥½åœ°é€‚åº”ä¸€äº›æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯é‚£äº›å¯¹è¾“å…¥æ•°æ®çš„å°ºåº¦æ•æ„Ÿçš„ç®—æ³•ã€‚

- å…¬å¼

  æœ€å¸¸è§çš„æ ‡å‡†åŒ–æ–¹æ³•æ˜¯Z-scoreæ ‡å‡†åŒ–ï¼Œä¹Ÿç§°ä¸ºé›¶å‡å€¼æ ‡å‡†åŒ–ã€‚å®ƒé€šè¿‡å¯¹æ¯ä¸ªç‰¹å¾çš„å€¼å‡å»å…¶å‡å€¼ï¼Œå†é™¤ä»¥å…¶æ ‡å‡†å·®ï¼Œå°†æ•°æ®è½¬æ¢ä¸ºå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„åˆ†å¸ƒã€‚

  ![æ ‡å‡†åŒ–](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/æ ‡å‡†åŒ–.png)

  å…¶ä¸­ï¼Œzæ˜¯è½¬æ¢åçš„æ•°å€¼ï¼Œxæ˜¯åŸå§‹æ•°æ®çš„å€¼ï¼ŒÎ¼æ˜¯è¯¥ç‰¹å¾çš„å‡å€¼ï¼ŒÏƒæ˜¯è¯¥ç‰¹å¾çš„æ ‡å‡†å·®

- è¯­æ³•

  ```python
  sklearn.preprocessing.StandardScale
  """
  ä¸MinMaxScalerä¸€æ ·ï¼ŒåŸå§‹æ•°æ®ç±»å‹å¯ä»¥æ˜¯listã€DataFrameå’Œndarray
  
  fit_transformå‡½æ•°çš„è¿”å›å€¼ä¸ºndarray,   å½’ä¸€åŒ–åå¾—åˆ°çš„æ•°æ®ç±»å‹éƒ½æ˜¯ndarray
  """
  ```

- å®ä¾‹

  ```python
  from sklearn.preprocessing import StandardScaler
  data=[
      {"name":"jack","age":23,"money":1000,"com":"å°ç±³"},
       {"name":"rose","age":33,"money":2000,"com":"è…¾è®¯"},
        {"name":"marry","age":83,"money":25,"com":"è‹¹æœ"},
         {"name":"joe","age":123,"money":899,"com":"å°ç±³"},
          {"name":"å°è’‹","age":33,"money":1,"com":"åæ¸…"},
  ]
  scale=StandardScaler()
  dict=DictVectorizer()
  data_dict=dict.fit_transform(data).toarray()
  data_scale=scale.fit_transform(data_dict)
  print('ndarryæ ‡å‡†åŒ–å½’ä¸€ï¼š\n',data_scale)
  
  
  scale=StandardScaler()
  data=[[12,22,4],[22,23,1],[11,23,9]]
  data = pd.DataFrame(data=data, index=["ä¸€","äºŒ","ä¸‰"], columns=["ä¸€åˆ—","äºŒåˆ—","ä¸‰åˆ—"])
  data_show=scale.fit_transform(data)
  print('DataFrameæ ‡å‡†åŒ–å½’ä¸€ï¼š\n',data_show)
  
  
  data=[[110,1243,23255,238423,42314],
        [103,1223,23535,234423,42335],
        [103,1223,23535,234423,42332],
        [103,1223,23535,234423,42339],
        [103,1223,23535,234423,42330],
        [103,1223,23535,234423,42331],
        [170,1253,23755,234223,10]]
  scale=StandardScaler()
  data=scale.fit_transform(data)
  print('listæ ‡å‡†åŒ–å½’ä¸€ï¼š\n',data)
  ```

  ![standard](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/standard.png)

- é‡ç‚¹

  ä½¿ç”¨å¦‚`StandardScaler`è¿™æ ·çš„æ•°æ®è½¬æ¢å™¨æ—¶ï¼Œ`fit`ã€`fit_transform`å’Œ`transform`è¿™ä¸‰ä¸ªæ–¹æ³•çš„ä½¿ç”¨æ˜¯è‡³å…³é‡è¦çš„ï¼Œå®ƒä»¬å„è‡ªæœ‰ä¸åŒçš„ä½œç”¨ï¼š

  1. **fit**:
     - è¿™ä¸ªæ–¹æ³•ç”¨æ¥è®¡ç®—æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œæ¯”å¦‚å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆåœ¨`StandardScaler`çš„æƒ…å†µä¸‹ï¼‰ã€‚è¿™äº›ç»Ÿè®¡ä¿¡æ¯éšåä¼šè¢«ç”¨äºæ•°æ®çš„æ ‡å‡†åŒ–ã€‚
     - ä½ åº”å½“ä»…åœ¨è®­ç»ƒé›†ä¸Šä½¿ç”¨`fit`æ–¹æ³•ã€‚
  2. **fit_transform**:
     - è¿™ä¸ªæ–¹æ³•ç›¸å½“äºå…ˆè°ƒç”¨`fit`å†è°ƒç”¨`transform`ï¼Œä½†æ˜¯å®ƒåœ¨å†…éƒ¨æ‰§è¡Œå¾—æ›´é«˜æ•ˆã€‚
     - å®ƒåŒæ ·åº”å½“ä»…åœ¨è®­ç»ƒé›†ä¸Šä½¿ç”¨ï¼Œå®ƒä¼šè®¡ç®—è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯å¹¶ç«‹å³åº”ç”¨åˆ°è¯¥è®­ç»ƒé›†ä¸Šã€‚
  3. **transform**:
     - è¿™ä¸ªæ–¹æ³•ä½¿ç”¨å·²ç»é€šè¿‡`fit`æ–¹æ³•è®¡ç®—å‡ºçš„ç»Ÿè®¡ä¿¡æ¯æ¥è½¬æ¢æ•°æ®ã€‚
     - å®ƒå¯ä»¥åº”ç”¨äºä»»ä½•æ•°æ®é›†ï¼ŒåŒ…æ‹¬è®­ç»ƒé›†ã€éªŒè¯é›†æˆ–æµ‹è¯•é›†ï¼Œä½†æ˜¯åº”ç”¨æ—¶ä½¿ç”¨çš„ç»Ÿè®¡ä¿¡æ¯å¿…é¡»æ¥è‡ªäºè®­ç»ƒé›†ã€‚

  å½“ä½ åœ¨é¢„å¤„ç†æ•°æ®æ—¶ï¼Œé¦–å…ˆéœ€è¦åœ¨è®­ç»ƒé›†`X_train`ä¸Šä½¿ç”¨`fit_transform`ï¼Œè¿™æ ·åšå¯ä»¥ä¸€æ¬¡æ€§å®Œæˆç»Ÿè®¡ä¿¡æ¯çš„è®¡ç®—å’Œæ•°æ®çš„æ ‡å‡†åŒ–ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦ç¡®ä¿æ¨¡å‹æ˜¯åŸºäºè®­ç»ƒæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œå­¦ä¹ çš„ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

  **ä¸€æ—¦`scaler`å¯¹è±¡åœ¨`X_train`ä¸Šè¢«`fit`ï¼Œå®ƒå°±å·²ç»çŸ¥é“äº†å¦‚ä½•å°†æ•°æ®æ ‡å‡†åŒ–ã€‚**è¿™æ—¶ï¼Œå¯¹äºæµ‹è¯•é›†`X_test`ï¼Œæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨`transform`æ–¹æ³•ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›åœ¨æµ‹è¯•é›†ä¸Šé‡æ–°è®¡ç®—ä»»ä½•ç»Ÿè®¡ä¿¡æ¯ï¼Œä¹Ÿä¸å¸Œæœ›æµ‹è¯•é›†çš„ä¿¡æ¯å½±å“åˆ°è®­ç»ƒè¿‡ç¨‹ã€‚å¦‚æœæˆ‘ä»¬å¯¹`X_test`ä¹Ÿä½¿ç”¨`fit_transform`ï¼Œæµ‹è¯•é›†çš„ä¿¡æ¯å°±å¯èƒ½ä¼šå½±å“åˆ°è®­ç»ƒè¿‡ç¨‹ã€‚

  **æ€»ç»“æ¥è¯´:æˆ‘ä»¬å¸¸å¸¸æ˜¯å…ˆfit_transform(x_train)ç„¶åå†transform(x_text)**

### 8.ç‰¹å¾é™ç»´

- å®šä¹‰

  é™ç»´å°±æ˜¯å»æ‰ä¸€äº›ç‰¹å¾,æˆ–è€…è½¬åŒ–å¤šä¸ªç‰¹å¾ä¸ºå°‘é‡ä¸ªç‰¹å¾

- ä½œç”¨

  **ç‰¹å¾é™ç»´å…¶ç›®çš„**:æ˜¯å‡å°‘æ•°æ®é›†çš„ç»´åº¦ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿ç•™æ•°æ®çš„é‡è¦ä¿¡æ¯ã€‚

  **ç‰¹å¾é™ç»´çš„å¥½å¤„**:

  å‡å°‘è®¡ç®—æˆæœ¬ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­å¤„ç†æ•°æ®å¯èƒ½éå¸¸è€—æ—¶ä¸”è®¡ç®—å¯†é›†ã€‚é™ç»´å¯ä»¥ç®€åŒ–æ¨¡å‹ï¼Œé™ä½è®­ç»ƒæ—¶é—´å’Œèµ„æºéœ€æ±‚ã€‚

  å»é™¤å™ªå£°ï¼šé«˜ç»´æ•°æ®å¯èƒ½åŒ…å«è®¸å¤šæ— å…³æˆ–å†—ä½™ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯èƒ½å¼•å…¥å™ªå£°å¹¶å¯¼è‡´è¿‡æ‹Ÿåˆã€‚é™ç»´å¯ä»¥å¸®åŠ©å»é™¤è¿™äº›ä¸å¿…è¦çš„ç‰¹å¾ã€‚

- æ–¹æ³•

  - ç‰¹å¾é€‰æ‹©
    - ä»åŸå§‹ç‰¹å¾é›†ä¸­æŒ‘é€‰å‡ºæœ€ç›¸å…³çš„ç‰¹å¾
  - ä¸»æˆä»½åˆ†æ(PCA)
    - ä¸»æˆåˆ†åˆ†æå°±æ˜¯æŠŠä¹‹å‰çš„ç‰¹å¾é€šè¿‡ä¸€ç³»åˆ—æ•°å­¦è®¡ç®—ï¼Œå½¢æˆæ–°çš„ç‰¹å¾ï¼Œæ–°çš„ç‰¹å¾æ•°é‡ä¼šå°äºä¹‹å‰ç‰¹å¾æ•°é‡

#### 1.ç‰¹å¾é€‰æ‹©

##### 1.1 VarianceThreshold ä½æ–¹å·®è¿‡æ»¤ç‰¹å¾é€‰æ‹©

- å®šä¹‰

  **Filter(è¿‡æ»¤å¼):** ä¸»è¦æ¢ç©¶ç‰¹å¾æœ¬èº«ç‰¹ç‚¹ï¼Œ ç‰¹å¾ä¸ç‰¹å¾ã€ç‰¹å¾ä¸ç›®æ ‡ å€¼ä¹‹é—´å…³è”

  - æ–¹å·®é€‰æ‹©æ³•: ä½æ–¹å·®ç‰¹å¾è¿‡æ»¤

    å¦‚æœ**ä¸€ä¸ªç‰¹å¾çš„æ–¹å·®å¾ˆå°ï¼Œè¯´æ˜è¿™ä¸ªç‰¹å¾çš„å€¼åœ¨æ ·æœ¬ä¸­å‡ ä¹ç›¸åŒæˆ–å˜åŒ–ä¸å¤§**ï¼ŒåŒ…å«çš„ä¿¡æ¯é‡å¾ˆå°‘ï¼Œæ¨¡å‹å¾ˆéš¾é€šè¿‡è¯¥ç‰¹å¾åŒºåˆ†ä¸åŒçš„å¯¹è±¡,**æ¯”å¦‚åŒºåˆ†ç”œç“œå­å’Œå’¸ç“œå­è¿˜æ˜¯è’œé¦™ç“œå­,å¦‚æœæœ‰ä¸€ä¸ªç‰¹å¾æ˜¯é•¿åº¦,è¿™ä¸ªç‰¹å¾ç›¸å·®ä¸å¤§å¯ä»¥å»æ‰ã€‚**

    1. **è®¡ç®—æ–¹å·®**ï¼šå¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œè®¡ç®—å…¶åœ¨è®­ç»ƒé›†ä¸­çš„æ–¹å·®(**æ¯ä¸ªæ ·æœ¬å€¼ä¸å‡å€¼ä¹‹å·®çš„å¹³æ–¹,åœ¨æ±‚å¹³å‡**)ã€‚
    2. **è®¾å®šé˜ˆå€¼**ï¼šé€‰æ‹©ä¸€ä¸ªæ–¹å·®é˜ˆå€¼ï¼Œä»»ä½•ä½äºè¿™ä¸ªé˜ˆå€¼çš„ç‰¹å¾éƒ½å°†è¢«è§†ä¸ºä½æ–¹å·®ç‰¹å¾ã€‚
    3. **è¿‡æ»¤ç‰¹å¾**ï¼šç§»é™¤æ‰€æœ‰æ–¹å·®ä½äºè®¾å®šé˜ˆå€¼çš„ç‰¹å¾

- è¯­æ³•

  ```python
  #åˆ›å»ºå¯¹è±¡ï¼Œå‡†å¤‡æŠŠæ–¹å·®ä¸ºç­‰äºå°äº2çš„å»æ‰ï¼Œthresholdçš„ç¼ºçœå€¼ä¸º2.0
  sklearn.feature_selection.VarianceThreshold(threshold=2.0)
  
  #æŠŠxä¸­ä½æ–¹å·®ç‰¹å¾å»æ‰, xçš„ç±»å‹å¯ä»¥æ˜¯DataFrameã€ndarrayå’Œlist
  VananceThreshold.fit_transform(x)
  
  fit_transformå‡½æ•°çš„è¿”å›å€¼ä¸ºndarray
  ```

- å®ä¾‹

  ```python
  from sklearn.feature_selection import VarianceThreshold
  
  data=[[110,1243,23255,238423,42314],
        [103,1223,23535,234423,42335],
        [103,1223,23535,234423,42332],
        [103,1223,23535,234423,42339],
        [103,1223,23535,234423,42330],
        [103,1223,23535,234423,42331],
        [170,1253,23755,234223,10]]
  data=pd.DataFrame(data)
  threshold=VarianceThreshold(threshold=1000)
  data_hold=threshold.fit_transform(data)
  print('åŸå§‹æ•°æ®ï¼š\n',data)
  print('è®¾å®šé˜ˆå€¼åçš„æ•°æ®ï¼š\n',data_hold)
  ```

  ![hold_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/hold_1.png)

##### 1.2 æ ¹æ®ç›¸å…³ç³»æ•°çš„ç‰¹å¾é€‰æ‹©

- å®šä¹‰

  **æ­£ç›¸å…³æ€§**ï¼ˆPositive Correlationï¼‰æ˜¯æŒ‡ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ä¸€ç§ç»Ÿè®¡å…³ç³»ï¼Œå…¶ä¸­ä¸€ä¸ªå˜é‡çš„å¢åŠ é€šå¸¸ä¼´éšç€å¦ä¸€ä¸ªå˜é‡çš„å¢åŠ ï¼Œåä¹‹äº¦ç„¶ã€‚æ­£ç›¸å…³æ€§é€šå¸¸ç”¨æ­£å€¼çš„ç›¸å…³ç³»æ•°æ¥è¡¨ç¤ºï¼Œ**è¿™ä¸ªå€¼ä»‹äº0å’Œ1ä¹‹é—´ã€‚å½“ç›¸å…³ç³»æ•°ç­‰äº1æ—¶ï¼Œè¡¨ç¤ºä¸¤ä¸ªå˜é‡ä¹‹é—´å­˜åœ¨å®Œç¾çš„æ­£ç›¸å…³å…³ç³»ï¼Œ**å³ä¸€ä¸ªå˜é‡çš„å€¼å¯ä»¥å®Œå…¨ç”±å¦ä¸€ä¸ªå˜é‡çš„å€¼é¢„æµ‹ã€‚

  - å¦‚æœç¬¬ä¸€ä¸ªå˜é‡å¢åŠ ï¼Œç¬¬äºŒä¸ªå˜é‡ä¹Ÿæœ‰å¾ˆå¤§çš„æ¦‚ç‡ä¼šå¢åŠ ã€‚
  - åŒæ ·ï¼Œå¦‚æœç¬¬ä¸€ä¸ªå˜é‡å‡å°‘ï¼Œç¬¬äºŒä¸ªå˜é‡ä¹Ÿå¾ˆå¯èƒ½ä¼šå‡å°‘ã€‚

  **è´Ÿç›¸å…³æ€§**ï¼ˆNegative Correlationï¼‰ä¸æ­£ç›¸å…³æ€§åˆšå¥½ç›¸å,ä½†æ˜¯ä¹Ÿè¯´æ˜ç›¸å…³,æ¯”å¦‚è¿åŠ¨é¢‘ç‡å’ŒBIMä½“é‡æŒ‡æ•°ç¨‹è´Ÿç›¸å…³ã€‚

  **ä¸ç›¸å…³**ï¼šæŒ‡ä¸¤è€…çš„ç›¸å…³æ€§å¾ˆå°,ä¸€ä¸ªå˜é‡å˜åŒ–ä¸ä¼šå¼•èµ·å¦å¤–çš„å˜é‡å˜åŒ–,åªæ˜¯æ²¡æœ‰çº¿æ€§å…³ç³». æ¯”å¦‚é¥­é‡å’Œæ™ºå•†ã€‚

  **çš®å°”é€Šç›¸å…³ç³»æ•°**ï¼ˆPearson correlation coefficient)æ˜¯ä¸€ç§åº¦é‡ä¸¤ä¸ªå˜é‡ä¹‹é—´çº¿æ€§ç›¸å…³æ€§çš„ç»Ÿè®¡é‡ã€‚å®ƒæä¾›äº†ä¸¤ä¸ªå˜é‡é—´å…³ç³»çš„æ–¹å‘ï¼ˆæ­£ç›¸å…³æˆ–è´Ÿç›¸å…³ï¼‰å’Œå¼ºåº¦çš„ä¿¡æ¯ã€‚çš®å°”é€Šç›¸å…³ç³»æ•°çš„å–å€¼èŒƒå›´æ˜¯ \[âˆ’1,1]ã€‚

  - $\rho=1$ è¡¨ç¤ºå®Œå…¨æ­£ç›¸å…³ï¼Œå³éšç€ä¸€ä¸ªå˜é‡çš„å¢åŠ ï¼Œå¦ä¸€ä¸ªå˜é‡ä¹Ÿçº¿æ€§å¢åŠ ã€‚
  - $\rho=-1$  è¡¨ç¤ºå®Œå…¨è´Ÿç›¸å…³ï¼Œå³éšç€ä¸€ä¸ªå˜é‡çš„å¢åŠ ï¼Œå¦ä¸€ä¸ªå˜é‡çº¿æ€§å‡å°‘ã€‚
  - $\rho=0$ è¡¨ç¤ºä¸¤ä¸ªå˜é‡ä¹‹é—´ä¸å­˜åœ¨çº¿æ€§å…³ç³»ã€‚

  ç›¸å…³ç³»æ•°$\rho$çš„ç»å¯¹å€¼ä¸º0-1ä¹‹é—´ï¼Œç»å¯¹å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºè¶Šç›¸å…³ï¼Œå½“ä¸¤ç‰¹å¾å®Œå…¨ç›¸å…³æ—¶ï¼Œä¸¤ç‰¹å¾çš„å€¼è¡¨ç¤ºçš„å‘é‡æ˜¯åœ¨åŒä¸€æ¡ç›´çº¿ä¸Šï¼Œå½“ä¸¤ç‰¹å¾çš„ç›¸å…³ç³»æ•°ç»å¯¹å€¼å¾ˆå°æ—¶ï¼Œä¸¤ç‰¹å¾å€¼è¡¨ç¤ºçš„å‘é‡æ¥è¿‘åœ¨åŒä¸€æ¡ç›´çº¿ä¸Šã€‚

- å…¬å¼

  å¯¹äºä¸¤ç»„æ•°æ® ğ‘‹={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘›} å’Œ ğ‘Œ={ğ‘¦1,ğ‘¦2,...,ğ‘¦ğ‘›}ï¼Œçš®å°”é€Šç›¸å…³ç³»æ•°å¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š

	$\rho=\frac{\text{Cos}(x, y)}{\sqrt{D x} \cdot \sqrt{D y}}=\frac{E[(x-E x)(y-E y)]}{\sqrt{D x} \cdot \sqrt{D y}}=\frac{\sum_{i=1}^{n}(x-\tilde{x})(y-\bar{y}) /(n-1)}{\sqrt{\sum_{i=1}^{n}(x-\bar{x})^{2} /(n-1)} \cdot \sqrt{\sum_{i=1}^{n}(y-\bar{y})^{2} /(n-1)}}$

  $\bar{x}$å’Œ $\bar{y}$ åˆ†åˆ«æ˜¯ğ‘‹å’Œğ‘Œçš„å¹³å‡å€¼

  |Ï|<0.4ä¸ºä½åº¦ç›¸å…³;    0.4<=|Ï|<0.7ä¸ºæ˜¾è‘—ç›¸å…³ï¼›  0.7<=|Ï|<1ä¸ºé«˜åº¦ç›¸å…³

- è¯­æ³•

  ```python
  scipy.stats.personr(x, y) #è®¡ç®—ä¸¤ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§
  """
  è¿”å›å¯¹è±¡æœ‰ä¸¤ä¸ªå±æ€§:
  
     statisticçš®å°”é€Šç›¸å…³ç³»æ•°[-1,1]   
  
     pvalueé›¶å‡è®¾(äº†è§£),ç»Ÿè®¡ä¸Šè¯„ä¼°ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§,è¶Šå°è¶Šç›¸å…³
     
  """
  ```

- å®ä¾‹

  ```python
  from scipy.stats import pearsonr
  x=np.array([1,2,3,4, 5, 6, 7, 8,9])
  y_1=np.array([10,20,30,40,50,60,70,80,90])
  y_2=np.array([100,200,300,10, 20, 0, 70, 1000,200])
  print("x:\n",x)
  print("y_1:\n",y_1)
  print("y_2:\n",y_2)
  ret_1=pearsonr(x,y_1)
  ret_2=pearsonr(x,y_2)
  print(ret_1)
  print(ret_2)
  ```

  ![pearsonr](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pearsonr.png)

#### 2.ä¸»æˆåˆ†åˆ†æ(PCA)

- å®šä¹‰

  PCAçš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»åŸå§‹ç‰¹å¾ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªæ–°çš„åæ ‡ç³»ç»Ÿï¼Œä½¿å¾—æ•°æ®åœ¨æ–°åæ ‡è½´ä¸Šçš„æŠ•å½±èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦åœ°ä¿ç•™æ•°æ®çš„æ–¹å·®ï¼ŒåŒæ—¶å‡å°‘æ•°æ®çš„ç»´åº¦ã€‚

- åŸç†

  ![pca_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_1.png)

  $x_0$æŠ•å½±åˆ°Lçš„å¤§å°ä¸º$x_0*cos  \alpha$

  $y_0$æŠ•å½±åˆ°Lçš„å¤§å°ä¸º$y_0*sin\alpha$ 

  ä½¿ç”¨$(x_0,y_0)$è¡¨ç¤ºä¸€ä¸ªç‚¹ï¼Œ  è¡¨æ˜è¯¥ç‚¹æœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œ è€Œæ˜ å°„åˆ°Lä¸Šæœ‰ä¸€ä¸ªç‰¹å¾å°±å¯ä»¥è¡¨ç¤ºè¿™ä¸ªç‚¹äº†ã€‚è¿™å°±è¾¾åˆ°äº†é™ç»´çš„åŠŸèƒ½ ã€‚

  æŠ•å½±åˆ°Lä¸Šçš„å€¼å°±æ˜¯é™ç»´åä¿ç•™çš„ä¿¡æ¯ï¼ŒæŠ•å½±åˆ°ä¸Lå‚ç›´çš„è½´ä¸Šçš„å€¼å°±æ˜¯ä¸¢å¤±çš„ä¿¡æ¯ã€‚ä¿ç•™ä¿¡æ¯/ä¸¢å¤±ä¿¡æ¯=ä¿¡æ¯ä¿ç•™çš„æ¯”ä¾‹

  ä¸‹å›¾ä¸­çº¢çº¿ä¸Šç‚¹ä¸ç‚¹çš„è·ç¦»æ˜¯æœ€å¤§çš„ï¼Œæ‰€ä»¥åœ¨çº¢è‰²çº¿ä¸Šç‚¹çš„æ–¹å·®æœ€å¤§ï¼Œç²‰çº¢çº¿ä¸Šçš„åˆšå¥½ç›¸å. 

  æ‰€ä»¥çº¢è‰²çº¿ä¸Šç‚¹æ¥è¡¨ç¤ºä¹‹å‰ç‚¹çš„ä¿¡æ¯æŸå¤±æ˜¯æœ€å°çš„ã€‚

  ![pca_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_2.png)

- æ­¥éª¤

  - å¾—åˆ°çŸ©é˜µ

  - ç”¨çŸ©é˜µPå¯¹åŸå§‹æ•°æ®è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå¾—åˆ°æ–°çš„æ•°æ®çŸ©é˜µZ,æ¯ä¸€åˆ—å°±æ˜¯ä¸€ä¸ªä¸»æˆåˆ†, å¦‚ä¸‹å›¾å°±æ˜¯æŠŠ10ç»´é™æˆäº†2ç»´,å¾—åˆ°äº†ä¸¤ä¸ªä¸»æˆåˆ†

    ![pca_4](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_4.png)

  - æ ¹æ®ä¸»æˆåˆ†çš„æ–¹å·®ç­‰ï¼Œç¡®å®šæœ€ç»ˆä¿ç•™çš„ä¸»æˆåˆ†ä¸ªæ•°ï¼Œ æ–¹å·®å¤§çš„è¦ç•™ä¸‹ã€‚**ä¸€ä¸ªç‰¹å¾çš„å¤šä¸ªæ ·æœ¬çš„å€¼å¦‚æœéƒ½ç›¸åŒï¼Œåˆ™æ–¹å·®ä¸º0ï¼Œ åˆ™è¯´æ˜è¯¥ç‰¹å¾å€¼ä¸èƒ½åŒºåˆ«æ ·æœ¬ï¼Œæ‰€ä»¥è¯¥ç‰¹å¾æ²¡æœ‰ç”¨ã€‚**

  æ¯”å¦‚ä¸‹å›¾çš„äºŒç»´æ•°æ®è¦é™ä¸ºä¸€ç»´æ•°æ®ï¼Œå›¾å½¢æ³•æ˜¯æŠŠæ‰€åœ¨æ•°æ®åœ¨äºŒç»´åæ ‡ä¸­ä»¥ç‚¹çš„å½¢å¼æ ‡å‡ºï¼Œç„¶åç»™å‡ºä¸€æ¡ç›´çº¿ï¼Œè®©æ‰€æœ‰ç‚¹å‚ç›´æ˜ å°„åˆ°ç›´çº¿ä¸Šï¼Œè¯¥ç›´çº¿æœ‰å¾ˆå¤šï¼Œåªæœ‰ç‚¹åˆ°çº¿çš„è·ç¦»ä¹‹å’Œæœ€å°çš„çº¿æ‰èƒ½è®©ä¹‹å‰ä¿¡æ¯æŸå¤±æœ€å°ã€‚

  è¿™æ ·ä¹‹å‰æ‰€æœ‰çš„äºŒç»´è¡¨ç¤ºçš„ç‚¹å°±å…¨éƒ¨å˜æˆä¸€æ¡ç›´çº¿ä¸Šçš„ç‚¹ï¼Œä»äºŒç»´é™æˆäº†ä¸€ç»´ã€‚

  ![pca_3](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_3.png)

  ä¸Šå›¾æ˜¯ä¸€ä¸ªä»äºŒç»´é™åˆ°ä¸€ç»´çš„ç¤ºä¾‹ï¼šçš„åŸå§‹æ•°æ®ä¸º

  | ç‰¹å¾1-X1 | ç‰¹å¾2-X2 |
  | -------- | -------- |
  | -1       | -2       |
  | -1       | 0        |
  | 0        | 0        |
  | 2        | 1        |
  | 0        | 1        |

  é™ç»´åæ–°çš„æ•°æ®ä¸º

  | ç‰¹å¾3-X0 |
  | -------- |
  | -3/âˆš2    |
  | -1/âˆš2    |
  | 0        |
  | 3/âˆš2     |
  | -1/âˆš2    |

- è¯­æ³•

  ```python
  from sklearn.decomposition import PCA
  PCA(n_components=None)
  """
  - ä¸»æˆåˆ†åˆ†æ
  - n_components:
    - å®å‚ä¸ºå°æ•°æ—¶ï¼šè¡¨ç¤ºé™ç»´åä¿ç•™ç™¾åˆ†ä¹‹å¤šå°‘çš„ä¿¡æ¯
    - å®å‚ä¸ºæ•´æ•°æ—¶ï¼šè¡¨ç¤ºå‡å°‘åˆ°å¤šå°‘ç‰¹å¾
  """
  ```

- å®ä¾‹

  ```python
  from sklearn.decomposition import PCA
  data=[[110,1243,23255,238423,42314],
        [103,1223,23535,234423,42335],
        [103,1223,23535,234423,42332],
        [103,1223,23535,234423,42339],
        [103,1223,23535,234423,42330],
        [103,1223,23535,234423,42331],
        [170,1253,23755,234223,10]]
  
  pca_intger=PCA(n_components=3)#é™ç»´è‡³ä¸‰ç»´åº¦
  pca_float=PCA(n_components=0.4)#ä¿ç•™ç™¾åˆ†ä¹‹40çš„æ•°æ®
  data_integer=pca_intger.fit_transform(data)
  data_float=pca_float.fit_transform(data)
  print('é™ç»´æ•´æ•°æ•°æ®ï¼š\n',data_integer)
  print('é™ç»´å°æ•°æ•°æ®ï¼š\n',data_float)
  ```

  ![pca_end](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/pca_end.png)

## Sklearnæœºå™¨å­¦ä¹ æ¦‚è¿°

```python
1.å®ä¾‹åŒ–é¢„ä¼°å™¨(ä¼°è®¡å™¨)å¯¹è±¡(estimator)ï¼Œ é¢„ä¼°å™¨å¯¹è±¡å¾ˆå¤š,éƒ½æ˜¯estimatorçš„å­ç±»
	ï¼ˆ1ï¼‰ç”¨äºåˆ†ç±»çš„é¢„ä¼°å™¨
		sklearn.neighbors.KNeighborsClassifier k-è¿‘é‚»
		sklearn.naive_bayes.MultinomialNB è´å¶æ–¯
		sklearn.linear_model.LogisticRegressioon é€»è¾‘å›å½’
		sklearn.tree.DecisionTreeClassifier å†³ç­–æ ‘
		sklearn.ensemble.RandomForestClassifier éšæœºæ£®æ—
	(2)ç”¨äºå›å½’çš„é¢„ä¼°å™¨
		sklearn.linear_model.LinearRegressionçº¿æ€§å›å½’
		sklearn.linear_model.Ridgeå²­å›å½’
	(3)ç”¨äºæ— ç›‘ç£å­¦ä¹ çš„é¢„ä¼°å™¨
		sklearn.cluster.KMeans èšç±»
2.è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒç»“æŸåç”Ÿæˆæ¨¡å‹
	estimator.fit(x_train, y_train)
3.æ¨¡å‹è¯„ä¼°
	(1)æ–¹å¼1ï¼Œç›´æ¥å¯¹æ¯”
		y_predict = estimator.predict(x_test)
		y_test == y_predict
  	(2)æ–¹å¼2, è®¡ç®—å‡†ç¡®ç‡
  		accuracy = estimator.score(x_test, y_test)
```

## KNNåˆ†ç±»ç®—æ³•

### 1.ä¸¤ç§å¸¸ç”¨æ ·æœ¬è·ç¦»æµ‹ç®—

![ou_ha](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ou_ha.png)

#### 1.1æ¬§å¼è·ç¦»

- å®šä¹‰ï¼šæ¬§å¼è·ç¦»æ˜¯æœ€å¸¸è§çš„è·ç¦»åº¦é‡æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºç›´çº¿è·ç¦»ã€‚åœ¨äºŒç»´ç©ºé—´ä¸­ï¼Œæ¬§å¼è·ç¦»è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„ç›´çº¿è·ç¦»ã€‚

- ç¼ºç‚¹ï¼šåœ¨ä½¿ç”¨æ­¤è·ç¦»åº¦é‡ä¹‹å‰ï¼Œéœ€è¦å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚éšç€æ•°æ®ç»´æ•°çš„å¢åŠ ï¼Œæ¬§æ°è·ç¦»çš„ç”¨å¤„ä¹Ÿå°±è¶Šå°ã€‚

  ![ou_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ou_1.png)

  ![ou_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ou_2.png)

#### 1.2æ›¼å“ˆé¡¿è·ç¦»

- å®šä¹‰ï¼šæ›¼å“ˆé¡¿è·ç¦»æ˜¯è®¡ç®—ä¸¤ç‚¹ä¹‹é—´æ°´å¹³æˆ–å‚ç›´çº¿æ®µçš„è·ç¦»ä¹‹å’Œï¼Œä¹Ÿç§°ä¸ºåŸå¸‚è¡—åŒºè·ç¦»æˆ–L1è·ç¦»ã€‚

- ç¼ºç‚¹:ç”±äºå®ƒä¸æ˜¯å¯èƒ½çš„æœ€çŸ­è·¯å¾„ï¼Œå®ƒæ¯”æ¬§å‡ é‡Œå¾—è·ç¦»æ›´æœ‰å¯èƒ½ç»™å‡ºä¸€ä¸ªæ›´é«˜çš„è·ç¦»å€¼ã€‚éšç€æ•°æ®ç»´æ•°çš„å¢åŠ ï¼Œæ›¼å“ˆé¡¿è·ç¦»çš„ç”¨å¤„ä¹Ÿå°±è¶Šå°ã€‚

  ![ha_1](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ha_1.png)

  ![ha_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/ha_2.png)

### 2.KNNç®—æ³•åŸç†

- å®šä¹‰

  K-è¿‘é‚»ç®—æ³•ï¼ˆK-Nearest Neighborsï¼Œç®€ç§°KNNï¼‰,æ ¹æ®Kä¸ªé‚»å±…æ ·æœ¬çš„ç±»åˆ«æ¥åˆ¤æ–­å½“å‰æ ·æœ¬çš„ç±»åˆ«ã€‚

  å¦‚æœä¸€ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„kä¸ªæœ€ç›¸ä¼¼(æœ€é‚»è¿‘)æ ·æœ¬ä¸­çš„å¤§å¤šæ•°å±äºæŸä¸ªç±»åˆ«ï¼Œåˆ™è¯¥ç±»æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«

- ä¾‹å­è¯´æ˜

  ![exam](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/exam.png)

  æ ¹æ®KNNç®—æ³•å”äººè¡—æ¢æ¡ˆã€‹ç”µå½±å±äº**å–œå‰§ç±»å‹**ï¼š

  ![exam_2](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/exam_2.png)

### 3.KNNç¼ºç‚¹

- å¯¹äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œè®¡ç®—é‡å¤§ï¼Œå› ä¸ºéœ€è¦è®¡ç®—æµ‹è¯•æ ·æœ¬ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»ã€‚

- å¯¹äºé«˜ç»´æ•°æ®ï¼Œè·ç¦»åº¦é‡å¯èƒ½å˜å¾—ä¸é‚£ä¹ˆæœ‰æ„ä¹‰ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„â€œç»´åº¦ç¾éš¾â€

- éœ€è¦é€‰æ‹©åˆé€‚çš„kå€¼å’Œè·ç¦»åº¦é‡ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›å®éªŒå’Œè°ƒæ•´

### 4.KNN-API

- è¯­æ³•

  ```python
  from sklearn.neighbors import KNeighborsClassifier
  
  knn=KNeighborsClassifier(n_neighbors=5, algorithm='auto')
  ```

- å‚æ•°

  **n_neighbors: int, default=5,** é»˜è®¤æƒ…å†µä¸‹ç”¨äºkneighborsæŸ¥è¯¢çš„è¿‘é‚»æ•°ï¼Œå°±æ˜¯K
  **algorithm:{â€˜autoâ€™, â€˜ball_treeâ€™, â€˜kd_treeâ€™, â€˜bruteâ€™}, default=â€™autoâ€™ã€‚**æ‰¾åˆ°è¿‘é‚»çš„æ–¹å¼ï¼Œæ³¨æ„ä¸æ˜¯è®¡ç®—è·ç¦»çš„æ–¹å¼ï¼Œå¼€å‘ä¸­é»˜è®¤å€¼'auto'

- è®­ç»ƒä¸é¢„æµ‹

  (1) fit(x,y) ä½¿ç”¨Xä½œä¸ºè®­ç»ƒæ•°æ®å’Œyä½œä¸ºç›®æ ‡æ•°æ®  
  (2) predict(X) é¢„æµ‹æä¾›çš„æ•°æ®ï¼Œå¾—åˆ°é¢„æµ‹æ•°æ®

### 5.KNNå®ä¾‹

- KNNç®—æ³•â€”é¸¢å°¾èŠ±

  ```python
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  #1.è·å–æ•°æ®
  iris=load_iris()
  data=np.array(iris.data)
  target=np.array(iris.target)
  #2.åˆ†å‰²æ•°æ®ï¼ˆåˆ’åˆ†è®­ç»ƒæ•°æ®ï¼Œæµ‹è¯•æ•°æ®ï¼‰
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,random_state=44,shuffle=True)
  #3.ç‰¹å¾å·¥ç¨‹ï¼ˆæ ‡å‡†åŒ–ï¼‰
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)#ä¿æŒæµ‹è¯•é›†æ‹¥æœ‰ä¸è®­ç»ƒé›†ç›¸åŒçš„æ ‡å‡†åŒ–
  #4.æ¨¡å‹è®­ç»ƒ(ä¼ å…¥è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾)
  knn=KNeighborsClassifier(n_neighbors=5)
  knn.fit(x_train,y_train)
  #5.æ¨¡å‹è¯„ä¼°ï¼ˆé¢„æµ‹æ•°æ®ï¼‰
  y_predict=knn.predict(x_test)
  #æ–¹æ³•1:å¯¹æ¯”é¢„æµ‹å€¼å’ŒçœŸå®å€¼
  print('æ¨¡å‹é¢„æµ‹ç»“æœï¼š',y_predict==y_test)
  #æ–¹æ³•2ï¼šè®¡ç®—å‡†ç¡®ç‡
  score=knn.score(x_test,y_test)
  print('æ¨¡å‹é¢„æµ‹ç‡ï¼š',score)
  print('æ¨¡å‹é¢„æµ‹ç‡ï¼š',score)
  ```

  ![hua](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/hua.png)

- KNNç®—æ³•â€”æ•°å­—æ•°æ®

  ```python
  from sklearn.datasets import load_digits
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  #1.åŠ è½½æ•°æ®
  digit=load_digits()
  data=np.array(digit.data)
  target=np.array(digit.target)
  #2.åˆ’åˆ†æ•°æ®
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,random_state=44,shuffle=True)
  #3.æ ‡å‡†åŒ–
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)#ä¿æŒæµ‹è¯•é›†æ‹¥æœ‰ä¸è®­ç»ƒé›†ç›¸åŒçš„æ ‡å‡†åŒ–
  #4.è®­ç»ƒæ¨¡å‹(ä¼ å…¥è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾)
  knn=KNeighborsClassifier(n_neighbors=5)
  knn.fit(x_train,y_train)
  #5.æ¨¡å‹è¯„ä¼°(é¢„æµ‹æµ‹è¯•é›†)
  y_predict=knn.predict(x_test)
  #æ–¹æ³•1:å¯¹æ¯”é¢„æµ‹å€¼å’ŒçœŸå®å€¼
  print('æ¨¡å‹é¢„æµ‹ç»“æœï¼š',y_predict==y_test)
  #æ–¹æ³•2ï¼šè®¡ç®—å‡†ç¡®ç‡
  score=knn.score(x_test,y_test)
  print('æ¨¡å‹é¢„æµ‹ç‡ï¼š',score)
  ```

  ![digit](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/digit.png)

- KNNç®—æ³•â€”è‘¡è„é…’

  ```python
  from sklearn.datasets import load_wine
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  
  #1.åŠ è½½æ•°æ®
  acho=load_wine()
  data=acho.data
  target=acho.target
  #2.åˆ’åˆ†æ•°æ®
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.2,random_state=44,shuffle=True)
  #3.æ ‡å‡†åŒ–
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)
  #4.è®­ç»ƒæ¨¡å‹(ä¼ å…¥è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾)
  knn=KNeighborsClassifier(n_neighbors=7)
  knn.fit(x_train,y_train)
  #5.æ¨¡å‹è¯„ä¼°(é¢„æµ‹æµ‹è¯•é›†)
  y_predict=knn.predict(x_test)
  #æ–¹æ³•1:å¯¹æ¯”é¢„æµ‹å€¼å’ŒçœŸå®å€¼
  print('æ¨¡å‹é¢„æµ‹ç»“æœï¼š',y_predict==y_test)
  #æ–¹æ³•2ï¼šè®¡ç®—å‡†ç¡®ç‡
  score=knn.score(x_test,y_test)
  print('æ¨¡å‹é¢„æµ‹ç‡ï¼š',score)
  ```

  ![acho](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/acho.png)

- KNNç®—æ³•â€”ä¹³è…ºç™Œ

  ```python
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  
  # 1.åŠ è½½æ•°æ®
  cancer = load_breast_cancer()
  data = cancer.data
  target = cancer.target
  # 2.åˆ’åˆ†æ•°æ®
  x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.3, shuffle=True, random_state=44)
  # 3.æ ‡å‡†åŒ–
  stand = StandardScaler()
  x_train = stand.fit_transform(x_train)
  x_test = stand.transform(x_test)
  # 4.è®­ç»ƒæ¨¡å‹(ä¼ å…¥è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾)
  knn = KNeighborsClassifier(n_neighbors=7)
  knn.fit(x_train, y_train)
  # 5.æ¨¡å‹è¯„ä¼°(é¢„æµ‹æµ‹è¯•é›†)
  y_predict = knn.predict(x_test)
  # æ–¹æ³•1:å¯¹æ¯”é¢„æµ‹å€¼å’ŒçœŸå®å€¼
  print('æ¨¡å‹é¢„æµ‹ç»“æœï¼š', y_predict == y_test)
  # æ–¹æ³•2ï¼šè®¡ç®—å‡†ç¡®ç‡
  score = knn.score(x_test, y_test)
  print('æ¨¡å‹é¢„æµ‹ç‡ï¼š', score)
  ```

  ![cancer_knn](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/cancer_knn.png)

- KNNç®—æ³•â€”æ£®æ—è¦†ç›–ç±»å‹æ•°æ®

  ```python
  from sklearn.datasets import fetch_covtype
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  from sklearn.neighbors import KNeighborsClassifier
  
  # 1.åŠ è½½æ•°æ®
  """"
  æ•°æ®é›†ä¸­çš„ç‰¹å¾å¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼š
  åœŸå£¤ç±»å‹ç‰¹å¾ (10 ä¸ªç‰¹å¾)ï¼š
      æ¯ä¸ªåœŸå£¤ç±»å‹æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶ç‰¹å¾ï¼ŒæŒ‡ç¤ºæ ·æœ¬æ˜¯å¦å±äºè¯¥åœŸå£¤ç±»å‹ã€‚
      ç‰¹å¾ç¼–å·ä¸º Soil_Type1 åˆ° Soil_Type40ï¼Œä½†å¹¶éæ‰€æœ‰åœŸå£¤ç±»å‹éƒ½å‡ºç°ã€‚
  é‡ç”ŸåŠ¨æ¤ç‰©æµ‹é‡å€¼ (44 ä¸ªç‰¹å¾)ï¼š
      è¿™äº›ç‰¹å¾åŒ…æ‹¬æµ·æ‹”ã€å¡åº¦ã€æ°´å¹³è·ç¦»åˆ°æ°´ä½“ã€å‚ç›´è·ç¦»åˆ°æ°´ä½“ç­‰ã€‚
      ç‰¹å¾ç¼–å·ä¸º Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology,
                Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, 
               Horizontal_Distance_To_Fire_Points ç­‰ã€‚
  ç±»åˆ«æ•°é‡: 7 ç±»ä¸åŒçš„æ£®æ—è¦†ç›–ç±»å‹ã€‚
  """
  ret=datasets.fetch_covtype(data_home='./')
  data=ret.data
  target=ret.target
  #2.åˆ’åˆ†æ•°æ®
  x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3,shuffle=True,random_state=44)
  # 3.æ ‡å‡†åŒ–
  stand=StandardScaler()
  x_train=stand.fit_transform(x_train)
  x_test=stand.transform(x_test)
  # 4.è®­ç»ƒæ¨¡å‹
  knn=KNeighborsClassifier(n_neighbors=7)
  knn.fit(x_train,y_train)
  # 5.æ¨¡å‹è¯„ä¼°
  y_predict=knn.predict(x_test)
  # æ–¹æ³•1:å¯¹æ¯”é¢„æµ‹å€¼å’ŒçœŸå®å€¼
  print('æ¨¡å‹é¢„æµ‹ç»“æœï¼š', y_predict == y_test)
  # æ–¹æ³•2ï¼šè®¡ç®—å‡†ç¡®ç‡
  score = knn.score(x_test, y_test)
  print('æ¨¡å‹é¢„æµ‹ç‡ï¼š', score)
  ```

  ![forest](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/forest.png)

### 6.æ¨¡å‹ä¿å­˜ä¸åŠ è½½

```python
import joblib
# ä¿å­˜æ¨¡å‹
joblib.dump(estimator, "my_ridge.pkl")
# åŠ è½½æ¨¡å‹
estimator = joblib.load("my_ridge.pkl")
#ä½¿ç”¨æ¨¡å‹é¢„æµ‹
y_test=estimator.predict([[0.4,0.2,0.4,0.7]])
print(y_test)
```

## æ¨¡å‹é€‰æ‹©ä¸è°ƒä¼˜

### 1.äº¤å‰éªŒè¯

#### 1.1 ä¿ç•™äº¤å‰éªŒè¯(HoldOutï¼‰

- å®šä¹‰

  **HoldOut Cross-validationï¼ˆTrain-Test Splitï¼‰**

- ä½œç”¨

  æ•´ä¸ªæ•°æ®é›†è¢«**éšæœº**åœ°åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚æ ¹æ®ç»éªŒæ³•åˆ™ï¼Œæ•´ä¸ªæ•°æ®é›†çš„è¿‘70%è¢«ç”¨ä½œè®­ç»ƒé›†ï¼Œå…¶ä½™30%è¢«ç”¨ä½œéªŒè¯é›†ã€‚**ä¹Ÿå°±æ˜¯æˆ‘ä»¬æœ€å¸¸ä½¿ç”¨çš„ï¼Œç›´æ¥åˆ’åˆ†æ•°æ®é›†çš„æ–¹æ³•ã€‚**

- ä¼˜ç‚¹

  å¾ˆç®€å•å¾ˆå®¹æ˜“æ‰§è¡Œã€‚

- ç¼ºç‚¹

  1.ä¸é€‚ç”¨äºä¸å¹³è¡¡çš„æ•°æ®é›†ã€‚

  â€‹		å‡è®¾80%çš„æ•°æ®å±äº â€œ0 â€œç±»ï¼Œå…¶ä½™20%çš„æ•°æ®å±äº â€œ1 â€œç±»ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒé›†çš„å¤§å°ä¸º80%ï¼Œæµ‹è¯•æ•°æ®çš„å¤§å°ä¸ºæ•°æ®é›†çš„20%ã€‚å¯èƒ½å‘ç”Ÿçš„æƒ…å†µæ˜¯ï¼Œæ‰€æœ‰80%çš„ â€œ0 â€œç±»æ•°æ®éƒ½åœ¨è®­ç»ƒé›†ä¸­ï¼Œè€Œæ‰€æœ‰ â€œ1 â€œç±»æ•°æ®éƒ½åœ¨æµ‹è¯•é›†ä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†ä¸èƒ½å¾ˆå¥½åœ°æ¦‚æ‹¬æˆ‘ä»¬çš„æµ‹è¯•æ•°æ®ï¼Œå› ä¸ºå®ƒä¹‹å‰æ²¡æœ‰è§è¿‡ â€œ1 â€œç±»çš„æ•°æ®ã€‚

  2.ä¸€å¤§å—æ•°æ®è¢«å‰¥å¤ºäº†è®­ç»ƒæ¨¡å‹çš„æœºä¼šã€‚

  â€‹		åœ¨å°æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œæœ‰ä¸€éƒ¨åˆ†æ•°æ®å°†è¢«ä¿ç•™ä¸‹æ¥ç”¨äºæµ‹è¯•æ¨¡å‹ï¼Œè¿™äº›æ•°æ®å¯èƒ½å…·æœ‰é‡è¦çš„ç‰¹å¾ï¼Œè€Œæˆ‘ä»¬çš„æ¨¡å‹å¯èƒ½ä¼šå› ä¸ºæ²¡æœ‰åœ¨è¿™äº›æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒè€Œé”™è¿‡ã€‚

#### 1.2 K-æŠ˜äº¤å‰éªŒè¯(K-fold)

- å®šä¹‰

  **ï¼ˆK-fold Cross Validationï¼Œè®°ä¸ºK-CVæˆ–K-foldï¼‰**

- ä½œç”¨

  K-Foldäº¤å‰éªŒè¯æŠ€æœ¯ä¸­ï¼Œæ•´ä¸ªæ•°æ®é›†è¢«åˆ’åˆ†ä¸º**Kä¸ªå¤§å°ç›¸åŒçš„éƒ¨åˆ†**ã€‚æ¯ä¸ªåˆ†åŒºè¢«ç§°ä¸º ä¸€ä¸ªâ€Foldâ€ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰Kä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºK-Foldã€‚**ä¸€ä¸ªFoldè¢«ç”¨ä½œéªŒè¯é›†ï¼Œå…¶ä½™çš„K-1ä¸ªFoldè¢«ç”¨ä½œè®­ç»ƒé›†ã€‚**

  è¯¥æŠ€æœ¯é‡å¤Kæ¬¡ï¼Œ**ç›´åˆ°æ¯ä¸ªFoldéƒ½è¢«ç”¨ä½œéªŒè¯é›†ï¼Œå…¶ä½™çš„ä½œä¸ºè®­ç»ƒé›†ã€‚**

  **æ¨¡å‹çš„æœ€ç»ˆå‡†ç¡®åº¦æ˜¯é€šè¿‡å–kä¸ªæ¨¡å‹éªŒè¯æ•°æ®çš„å¹³å‡å‡†ç¡®åº¦æ¥è®¡ç®—çš„ã€‚**

- å›¾ç¤º

  ![k_z](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/k_z.png)

#### 1.3åˆ†å±‚k-æŠ˜äº¤å‰éªŒè¯(Stratified k-fold)

- å®šä¹‰

  **Stratified k-fold cross validation**

- ä½œç”¨

  K-æŠ˜äº¤å‰éªŒè¯çš„å˜ç§ï¼Œ åˆ†å±‚çš„æ„æ€æ˜¯è¯´åœ¨**æ¯ä¸€æŠ˜ä¸­éƒ½ä¿æŒç€åŸå§‹æ•°æ®ä¸­å„ä¸ªç±»åˆ«çš„æ¯”ä¾‹å…³ç³»**ï¼Œæ¯”å¦‚è¯´ï¼šåŸå§‹æ•°æ®æœ‰3ç±»ï¼Œæ¯”ä¾‹ä¸º1:2:1ï¼Œé‡‡ç”¨3æŠ˜åˆ†å±‚äº¤å‰éªŒè¯ï¼Œé‚£ä¹ˆåˆ’åˆ†çš„3æŠ˜ä¸­ï¼Œæ¯ä¸€æŠ˜ä¸­çš„æ•°æ®ç±»åˆ«ä¿æŒç€1:2:1çš„æ¯”ä¾‹ï¼Œè¿™æ ·çš„éªŒè¯ç»“æœæ›´åŠ å¯ä¿¡ã€‚

- å›¾ç¤º

  ![k_z_z](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/k_z_z.png)

#### 1.4å…¶ä»–éªŒè¯

- å»é™¤päº¤å‰éªŒè¯)
- ç•™ä¸€äº¤å‰éªŒè¯ï¼‰
- è’™ç‰¹å¡ç½—äº¤å‰éªŒè¯
- æ—¶é—´åºåˆ—äº¤å‰éªŒè¯

#### 1.5API

```python
from sklearn.model_selection import StratifiedKFold
kf=StratifiedKFold(n_splits=5,shuffle=True,random_state=44)
"""
    n_splitsåˆ’åˆ†ä¸ºå‡ ä¸ªæŠ˜å  
    shuffleæ˜¯å¦åœ¨æ‹†åˆ†ä¹‹å‰è¢«æ‰“ä¹±(éšæœºåŒ–),Falseåˆ™æŒ‰ç…§é¡ºåºæ‹†åˆ†
    random_stateéšæœºå› å­

"""
indexs=kf.split(X,y) 
"""
	è¿”å›ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡,ä¸€å…±æœ‰5ä¸ªæŠ˜å ,æ¯ä¸ªæŠ˜å å¯¹åº”çš„æ˜¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ä¸‹æ ‡

	ç„¶åå¯ä»¥ç”¨forå¾ªç¯å–å‡ºæ¯ä¸€ä¸ªæŠ˜å å¯¹åº”çš„Xå’Œyä¸‹æ ‡æ¥è®¿é—®åˆ°å¯¹åº”çš„æµ‹è¯•æ•°æ®é›†å’Œè®­ç»ƒæ•°æ®é›† ä»¥åŠæµ‹è¯•ç›®æ ‡é›†å’Œè®­ç»ƒç›®æ ‡é›†
"""
for train_index, test_index in indexs:
X[train_index],y[train_index],X[test_index ],y[test_index]


#æ™®é€šKæŠ˜äº¤å‰éªŒè¯å’Œåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯çš„ä½¿ç”¨æ˜¯ä¸€æ ·çš„  åªæ˜¯å¼•å…¥çš„ç±»ä¸åŒ
#ä½¿ç”¨æ—¶åªæ˜¯KFoldè¿™ä¸ªç±»åä¸ä¸€æ ·å…¶ä»–ä»£ç å®Œå…¨ä¸€æ ·
from sklearn.model_selection import KFold
```

#### 1.6å®ä¾‹

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

#1.åŠ è½½æ•°æ®
iris=load_iris()
data=iris.data
target=iris.target
#2.åˆå§‹åŒ–åˆ†å±‚K-æŠ˜äº¤å‰éªŒè¯å™¨
kf=StratifiedKFold(n_splits=5,shuffle=True,random_state=44)
#3.åˆå§‹åŒ–KNNå®¹å™¨
knn=KNeighborsClassifier(n_neighbors=7)
#4.äº¤å‰éªŒè¯
score=[]
for train_index,test_index in kf.split(data,target):
    x_train,x_test=data[train_index],data[test_index]
    y_train,y_test=target[train_index],target[test_index]
    #æ ‡å‡†åŒ–
    stand=StandardScaler()
    x_train=stand.fit_transform(x_train)
    x_test=stand.transform(x_test)
    #æ¨¡å‹è®­ç»ƒ
    knn.fit(x_train,y_train)
    #å¾—åˆ°æ¯æ¬¡æŠ˜å çš„å‡†ç¡®ç‡
    score_num=knn.score(x_test,y_test)
    score.append(score_num)
print('å¹³å‡åŒ–å¾—åˆ†ï¼š',sum(score)/len(score))
"""
å¹³å‡åŒ–å¾—åˆ†ï¼š 0.9600000000000002
"""
```

### 2.è¶…å‚æ•°æœç´¢

- å®šä¹‰

  è¶…å‚æ•°æœç´¢ä¹Ÿå«ç½‘æ ¼æœç´¢(Grid Search)

  æ¯”å¦‚åœ¨KNNç®—æ³•ä¸­ï¼Œkæ˜¯ä¸€ä¸ªå¯ä»¥äººä¸ºè®¾ç½®çš„å‚æ•°ï¼Œæ‰€ä»¥å°±æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚ç½‘æ ¼æœç´¢èƒ½è‡ªåŠ¨çš„å¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°æœ€å¥½çš„è¶…å‚æ•°å€¼ã€‚

- å›¾ç¤º

  ![super](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/super.png)

### 3.è¶…å‚æ•°æœç´¢API

```python
from sklearn.model_selection import GridSearchCV
gc=GridSearchCV(estimatorï¼Œparam_gridï¼Œcv)

è¯´æ˜ï¼š
åŒæ—¶è¿›è¡Œäº¤å‰éªŒè¯(CV)ã€å’Œç½‘æ ¼æœç´¢(GridSearch)ï¼ŒGridSearchCVå®è®¡ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªä¼°è®¡å™¨(estimator)ï¼ŒåŒæ—¶å®ƒæœ‰å‡ ä¸ªé‡è¦å±æ€§ï¼š
      best_params_  æœ€ä½³å‚æ•°
      best_score_ åœ¨è®­ç»ƒé›†ä¸­çš„å‡†ç¡®ç‡
      best_estimator_ æœ€ä½³ä¼°è®¡å™¨
      cv_results_ äº¤å‰éªŒè¯è¿‡ç¨‹æè¿°
      best_index_æœ€ä½³kåœ¨åˆ—è¡¨ä¸­çš„ä¸‹æ ‡
å‚æ•°ï¼š
	estimatorï¼š scikit-learnä¼°è®¡å™¨å®ä¾‹
	param_grid:ä»¥å‚æ•°åç§°ï¼ˆstrï¼‰ä½œä¸ºé”®ï¼Œå°†å‚æ•°è®¾ç½®åˆ—è¡¨å°è¯•ä½œä¸ºå€¼çš„å­—å…¸
		ç¤ºä¾‹ï¼š {"n_neighbors": [1, 3, 5, 7, 9, 11]}
    cv: ç¡®å®šäº¤å‰éªŒè¯åˆ‡åˆ†ç­–ç•¥,å€¼ä¸º:
        (1)None  é»˜è®¤5æŠ˜
        (2)integer  è®¾ç½®å¤šå°‘æŠ˜
        å¦‚æœä¼°è®¡å™¨æ˜¯åˆ†ç±»å™¨ï¼Œä½¿ç”¨"åˆ†å±‚k-æŠ˜äº¤å‰éªŒè¯(StratifiedKFold)"ã€‚åœ¨æ‰€æœ‰å…¶ä»–æƒ…å†µä¸‹ï¼Œä½¿ç”¨KFoldã€‚
```

### 4.è¶…å‚æ•°æœç´¢å®ä¾‹

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
#1.åŠ è½½æ•°æ®
iris=load_iris()
data=iris.data
target=iris.target
#2.åˆ’åˆ†æ•°æ®
x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3,random_state=44,shuffle=True)
#3.æ ‡å‡†åŒ–ï¼ˆæ•°æ®é¢„å¤„ç†ï¼‰
stand=StandardScaler()
x_train=stand.fit_transform(x_train)
x_test=stand.transform(x_test)
#4.åˆå§‹åŒ–KNNå®¹å™¨
knn=KNeighborsClassifier()
#5.è¶…å‚æ•°æœç´¢
gc=GridSearchCV(knn,{'n_neighbors':[1,2,3,4,5,6,7,8,9,10]},cv=10)
gc.fit(x_train,y_train)
#æ¨¡å‹è¯„ä¼°
y_predict=gc.predict(x_test)
# æ–¹æ³•1:å¯¹æ¯”é¢„æµ‹å€¼å’ŒçœŸå®å€¼
print('æ¨¡å‹é¢„æµ‹ç»“æœï¼š', y_predict == y_test)
# æ–¹æ³•2ï¼šè®¡ç®—å‡†ç¡®ç‡
score = gc.score(x_test, y_test)
print('æ¨¡å‹é¢„æµ‹ç‡ï¼š', score)
# æœ€ä½³å‚æ•°ï¼šbest_params_
print("æœ€ä½³å‚æ•°ï¼š\n", gc.best_params_)
# æœ€ä½³ç»“æœï¼šbest_score_
print("åœ¨è®­ç»ƒé›†ä¸­çš„å‡†ç¡®ç‡ï¼š\n", gc.best_score_)
# æœ€ä½³ä¼°è®¡å™¨ï¼šbest_estimator_
print("æœ€ä½³ä¼°è®¡å™¨:\n", gc.best_estimator_)
# äº¤å‰éªŒè¯ç»“æœï¼šcv_results_
print("äº¤å‰éªŒè¯è¿‡ç¨‹æè¿°:\n", gc.cv_results_)
# æœ€ä½³å‚æ•°ç»„åˆçš„ç´¢å¼•:æœ€ä½³kåœ¨åˆ—è¡¨ä¸­çš„ä¸‹æ ‡
print("æœ€ä½³å‚æ•°ç»„åˆçš„ç´¢å¼•:\n", gc.best_index_)
```

![spuer_serach](https://github.com/ljgit1316/Picture_resource/blob/main/Machine_Pic/spuer_serach.png)


